{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_awgn_noise(signal,SNR_dB):\n",
    "    \"\"\"  Adds AWGN noise vector to signal \n",
    "         to generate a resulting signal vector y of specified SNR in dB\n",
    "    \"\"\"\n",
    "    L=len(signal)\n",
    "    SNR = 10**(SNR_dB/10.0) #SNR to linear scale\n",
    "    Esym=np.sum(np.square(np.abs(signal)))/L #Calculate actual symbol energy\n",
    "    N0=Esym/SNR; #Find the noise spectral density\n",
    "    if(isinstance(signal[0], complex)):\n",
    "        noiseSigma=np.sqrt(N0/2.0)#Standard deviation for AWGN Noise when x is complex\n",
    "        n = noiseSigma*(np.random.randn(1,L)+1j*np.random.randn(1,L))#computed noise \n",
    "    else:\n",
    "        noiseSigma = np.sqrt(N0);#Standard deviation for AWGN Noise when x is real\n",
    "        n = noiseSigma*np.random.randn(1,L)#computed noise\n",
    "    y = signal + n #received signal\n",
    "    \n",
    "    return y.flatten()\n",
    "\n",
    "def generate_data_set(dataset_size, preamble_length, data_length, channel_length, SNR):\n",
    "    input_data_bits = np.random.randint(0,2,(dataset_size, data_length)) \n",
    "    input_data_constellations = input_data_bits * 2 - 1\n",
    "        \n",
    "    preambles = np.random.randint(0, 2, (dataset_size, preamble_length))\n",
    "    convolved_preambles = []\n",
    "    convolved_data_constellations = []\n",
    "    \n",
    "    for i in range(dataset_size):\n",
    "        channel_taps = np.random.uniform(0,1,channel_length)\n",
    "        if sum(channel_taps)>=1:\n",
    "            channel_taps = channel_taps / sum(channel_taps)\n",
    "        preamble_conv = add_awgn_noise(sig.convolve(preambles[i], channel_taps, mode='same'), SNR)\n",
    "        convolved_preambles.append(preamble_conv)\n",
    "        constellation_convolved = add_awgn_noise(sig.convolve(input_data_constellations[i], channel_taps, mode='same'), SNR)\n",
    "        convolved_data_constellations.append(constellation_convolved)\n",
    "    X = np.hstack([preambles, np.array(convolved_preambles), np.array(convolved_data_constellations)])\n",
    "    Y = input_data_constellations\n",
    "    return X, Y\n",
    "\n",
    "X, Y = generate_data_set(10000, 20, 40, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 80) (10000, 40)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4936 - mean_squared_error: 0.4936     \n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3449 - mean_squared_error: 0.3449     \n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3305 - mean_squared_error: 0.3305     \n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3219 - mean_squared_error: 0.3219     \n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3123 - mean_squared_error: 0.3123     \n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3055 - mean_squared_error: 0.3055     \n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2978 - mean_squared_error: 0.2978     \n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2912 - mean_squared_error: 0.2912     \n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2846 - mean_squared_error: 0.2846     \n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2759 - mean_squared_error: 0.2759     \n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2704 - mean_squared_error: 0.2704     \n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2642 - mean_squared_error: 0.2642     \n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2553 - mean_squared_error: 0.2553     \n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2506 - mean_squared_error: 0.2506     \n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2429 - mean_squared_error: 0.2429     \n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2368 - mean_squared_error: 0.2368     \n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2304 - mean_squared_error: 0.2304     \n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2246 - mean_squared_error: 0.2246     \n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2183 - mean_squared_error: 0.2183     \n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2126 - mean_squared_error: 0.2126     \n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2064 - mean_squared_error: 0.2064     \n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2016 - mean_squared_error: 0.2016     \n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1952 - mean_squared_error: 0.1952     \n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1893 - mean_squared_error: 0.1893     \n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1850 - mean_squared_error: 0.1850     \n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1799 - mean_squared_error: 0.1799     \n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1746 - mean_squared_error: 0.1746     \n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1703 - mean_squared_error: 0.1703     \n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1656 - mean_squared_error: 0.1656     \n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1610 - mean_squared_error: 0.1610     \n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1565 - mean_squared_error: 0.1565     \n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1515 - mean_squared_error: 0.1515     \n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1493 - mean_squared_error: 0.1493     \n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1444 - mean_squared_error: 0.1444     \n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1402 - mean_squared_error: 0.1402     \n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1371 - mean_squared_error: 0.1371     \n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1329 - mean_squared_error: 0.1329     \n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1306 - mean_squared_error: 0.1306     \n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1282 - mean_squared_error: 0.1282     \n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1227 - mean_squared_error: 0.1227     \n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1201 - mean_squared_error: 0.1201     \n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1184 - mean_squared_error: 0.1184     \n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1134 - mean_squared_error: 0.1134      ETA: 0s - loss: 0.1070 - mean\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1119 - mean_squared_error: 0.1119     \n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1089 - mean_squared_error: 0.1089     \n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1063 - mean_squared_error: 0.1063     \n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1050 - mean_squared_error: 0.1050     \n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1018 - mean_squared_error: 0.1018     \n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1000 - mean_squared_error: 0.1000     \n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0968 - mean_squared_error: 0.0968     \n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0947 - mean_squared_error: 0.0947     \n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0938 - mean_squared_error: 0.0938     \n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0911 - mean_squared_error: 0.0911     \n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0908 - mean_squared_error: 0.0908     \n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0878 - mean_squared_error: 0.0878     \n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0863 - mean_squared_error: 0.0863     \n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0831 - mean_squared_error: 0.0831     \n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0812 - mean_squared_error: 0.0812     \n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0810 - mean_squared_error: 0.0810     \n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0819 - mean_squared_error: 0.0819     \n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0788 - mean_squared_error: 0.0788     \n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0753 - mean_squared_error: 0.0753     \n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0769 - mean_squared_error: 0.0769     \n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0741 - mean_squared_error: 0.0741     \n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0728 - mean_squared_error: 0.0728     \n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0703 - mean_squared_error: 0.0703     \n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0698 - mean_squared_error: 0.0698     \n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0703 - mean_squared_error: 0.0703     \n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0701 - mean_squared_error: 0.0701     \n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0694 - mean_squared_error: 0.0694     \n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0691 - mean_squared_error: 0.0691     \n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0648 - mean_squared_error: 0.0648     \n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0636 - mean_squared_error: 0.0636     \n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0638 - mean_squared_error: 0.0638     \n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s - loss: 0.0648 - mean_squared_error: 0.0648     \n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0626 - mean_squared_error: 0.0626     \n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0623 - mean_squared_error: 0.0623     \n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0610 - mean_squared_error: 0.0610     \n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0626 - mean_squared_error: 0.0626     \n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0622 - mean_squared_error: 0.0622     \n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0591 - mean_squared_error: 0.0591     \n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0600 - mean_squared_error: 0.0600     \n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0586 - mean_squared_error: 0.0586     \n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0578 - mean_squared_error: 0.0578     \n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0570 - mean_squared_error: 0.0570     \n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0579 - mean_squared_error: 0.0579     \n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0584 - mean_squared_error: 0.0584     \n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0563 - mean_squared_error: 0.0563     \n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0557 - mean_squared_error: 0.0557     \n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0570 - mean_squared_error: 0.0570     \n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0569 - mean_squared_error: 0.0569     \n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0582 - mean_squared_error: 0.0582     \n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0552 - mean_squared_error: 0.0552     \n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0538 - mean_squared_error: 0.0538     \n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0532 - mean_squared_error: 0.0532     \n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0559 - mean_squared_error: 0.0559     \n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0539 - mean_squared_error: 0.0539     \n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0533 - mean_squared_error: 0.0533     \n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0534 - mean_squared_error: 0.0534     \n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0543 - mean_squared_error: 0.0543     \n",
      "Mean Squared Error: 0.4468851149584626\n",
      "Mean Absolute Error: 0.25533151349089467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "def create_model(input_layer_dim, hidden_layer_dim, input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_layer_dim, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(hidden_layer_dim, activation='relu'))\n",
    "    model.add(Dense(output_dim, activation='tanh'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "    return model\n",
    "\n",
    "model = create_model(300, 300, X_train.shape[1], Y_train.shape[1])\n",
    "model.fit(X_train, Y_train, epochs=100)\n",
    "predictions = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mse = mean_squared_error(predictions, Y_test)\n",
    "mabse = mean_absolute_error(predictions, Y_test)\n",
    "print('Mean Squared Error: {0}'.format(mse))\n",
    "print('Mean Absolute Error: {0}'.format(mabse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -0.964104    0.99977    ... -1.         -1.\n",
      "  -0.99993986]\n",
      " [-1.          1.          1.         ... -1.         -1.\n",
      "  -0.999523  ]\n",
      " [-0.9999333   1.          1.         ...  1.          0.9999468\n",
      "  -1.        ]\n",
      " ...\n",
      " [-1.         -0.45595777  1.         ...  0.9998585   1.\n",
      "  -0.99995536]\n",
      " [ 1.          1.         -0.9999999  ...  0.9999931   1.\n",
      "   0.31632844]\n",
      " [ 0.9999934  -0.9886104  -1.         ... -0.35461572  1.\n",
      "   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -1  1 ... -1 -1 -1]\n",
      " [-1  1  1 ... -1 -1 -1]\n",
      " [-1  1  1 ...  1  1 -1]\n",
      " ...\n",
      " [-1  1  1 ...  1  1 -1]\n",
      " [ 1  1 -1 ...  1  1  1]\n",
      " [ 1 -1 -1 ...  1  1  1]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1, -1,  1, ..., -1, -1, -1],\n",
       "       [-1,  1,  1, ..., -1, -1, -1],\n",
       "       [-1,  1,  1, ...,  1,  1, -1],\n",
       "       ...,\n",
       "       [-1, -1,  1, ...,  1,  1, -1],\n",
       "       [ 1,  1, -1, ...,  1,  1,  1],\n",
       "       [ 1, -1, -1, ..., -1,  1,  1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rounded_predictions = np.where(predictions > 0, 1, -1)\n",
    "rounded_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8743625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_accuracy(original, predictions):\n",
    "    return np.sum(original == predictions) / (original.shape[0] * original.shape[1])\n",
    "calc_accuracy(Y_test, rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 42) (10000, 40)\n",
      "[[ 0.02480375  0.87786724  0.42363619 ...  0.69840526  1.00192467\n",
      "   0.81512052]\n",
      " [ 0.28461326  0.41015693 -0.48605192 ...  0.33533935 -0.51485916\n",
      "  -0.66148329]\n",
      " [ 0.50815963  0.3191266   0.68153568 ...  0.31422932  0.38569871\n",
      "   0.9785387 ]\n",
      " ...\n",
      " [ 0.40532169  0.19955261 -0.52453031 ... -0.84053881  0.20772992\n",
      "  -0.20105906]\n",
      " [ 0.61171852  0.22170766  0.82467143 ... -0.31189178 -0.66584868\n",
      "   0.37340235]\n",
      " [ 0.23507199  0.38222523 -0.23465891 ...  0.29236833  0.30861638\n",
      "  -0.43417995]]\n",
      "[[-1 -1  1 ...  1  1 -1]\n",
      " [-1 -1  1 ... -1 -1 -1]\n",
      " [ 1  1  1 ...  1  1  1]\n",
      " ...\n",
      " [-1  1 -1 ... -1  1 -1]\n",
      " [ 1 -1  1 ... -1 -1  1]\n",
      " [-1 -1 -1 ...  1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "def generate_channel_data_set(dataset_size, data_length, channel_length, SNR):\n",
    "    input_data_bits = np.random.randint(0,2,(dataset_size, data_length)) \n",
    "    input_data_constellations = input_data_bits * 2 - 1\n",
    "    \n",
    "    convolved_data_constellations = []\n",
    "    channels= []\n",
    "    \n",
    "    for i in range(dataset_size):\n",
    "        channel_taps = np.random.uniform(0,1,channel_length)\n",
    "        if sum(channel_taps)>=1:\n",
    "            channel_taps = channel_taps / sum(channel_taps)\n",
    "        channels.append(channel_taps)\n",
    "        constellation_convolved = add_awgn_noise(sig.convolve(input_data_constellations[i], channel_taps, mode='same'), SNR)\n",
    "        convolved_data_constellations.append(constellation_convolved)\n",
    "    X = np.hstack([np.array(channels), np.array(convolved_data_constellations)])\n",
    "    Y = input_data_constellations\n",
    "    return X, Y\n",
    "\n",
    "X, Y = generate_channel_data_set(10000, 40, 2, 10)\n",
    "print(X.shape, Y.shape)\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4722 - mean_squared_error: 0.4722     \n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3289 - mean_squared_error: 0.3289     \n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3110 - mean_squared_error: 0.3110     \n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2931 - mean_squared_error: 0.2931     \n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2634 - mean_squared_error: 0.2634     \n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2242 - mean_squared_error: 0.2242     \n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1825 - mean_squared_error: 0.1825     \n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1451 - mean_squared_error: 0.1451     \n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1187 - mean_squared_error: 0.1187     \n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1023 - mean_squared_error: 0.1023     \n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0907 - mean_squared_error: 0.0907     \n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0813 - mean_squared_error: 0.0813     \n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0748 - mean_squared_error: 0.0748     \n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0698 - mean_squared_error: 0.0698     \n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0654 - mean_squared_error: 0.0654     \n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0607 - mean_squared_error: 0.0607     \n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0563 - mean_squared_error: 0.0563     \n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0530 - mean_squared_error: 0.0530     \n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0511 - mean_squared_error: 0.0511     \n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0474 - mean_squared_error: 0.0474     \n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0459 - mean_squared_error: 0.0459     \n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0425 - mean_squared_error: 0.0425     \n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0420 - mean_squared_error: 0.0420     \n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0390 - mean_squared_error: 0.0390     \n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0369 - mean_squared_error: 0.0369     \n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0354 - mean_squared_error: 0.0354     \n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0336 - mean_squared_error: 0.0336     \n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0326 - mean_squared_error: 0.0326     \n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0313 - mean_squared_error: 0.0313     \n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0294 - mean_squared_error: 0.0294     \n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0295 - mean_squared_error: 0.0295     \n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0286 - mean_squared_error: 0.0286     \n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0265 - mean_squared_error: 0.0265     \n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0267 - mean_squared_error: 0.0267     \n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0251 - mean_squared_error: 0.0251     \n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0241 - mean_squared_error: 0.0241     \n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0245 - mean_squared_error: 0.0245     \n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0239 - mean_squared_error: 0.0239     \n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0226 - mean_squared_error: 0.0226     \n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0225 - mean_squared_error: 0.0225     \n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0193 - mean_squared_error: 0.0193     \n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0202 - mean_squared_error: 0.0202     \n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0202 - mean_squared_error: 0.0202     \n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0193 - mean_squared_error: 0.0193     \n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0208 - mean_squared_error: 0.0208     \n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0184 - mean_squared_error: 0.0184     \n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0181 - mean_squared_error: 0.0181     \n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0181 - mean_squared_error: 0.0181     \n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0169 - mean_squared_error: 0.0169     \n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0157 - mean_squared_error: 0.0157     \n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0173 - mean_squared_error: 0.0173     \n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0174 - mean_squared_error: 0.0174     \n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0166 - mean_squared_error: 0.0166     \n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0154 - mean_squared_error: 0.0154     \n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0150 - mean_squared_error: 0.0150     \n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0158 - mean_squared_error: 0.0158     \n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0156 - mean_squared_error: 0.0156     \n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0163 - mean_squared_error: 0.0163     \n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0140 - mean_squared_error: 0.0140     \n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0138 - mean_squared_error: 0.0138     \n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0137 - mean_squared_error: 0.0137     \n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0128 - mean_squared_error: 0.0128     \n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0124 - mean_squared_error: 0.0124     \n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0142 - mean_squared_error: 0.0142     \n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0132 - mean_squared_error: 0.0132     \n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0142 - mean_squared_error: 0.0142     \n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0138 - mean_squared_error: 0.0138     \n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0135 - mean_squared_error: 0.0135     \n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0125 - mean_squared_error: 0.0125     \n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0123 - mean_squared_error: 0.0123     \n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0121 - mean_squared_error: 0.0121     \n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0118 - mean_squared_error: 0.0118     \n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0110 - mean_squared_error: 0.0110     \n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0115 - mean_squared_error: 0.0115     \n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s - loss: 0.0116 - mean_squared_error: 0.0116     \n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0134 - mean_squared_error: 0.0134     \n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0125 - mean_squared_error: 0.0125     \n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0128 - mean_squared_error: 0.0128     \n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0130 - mean_squared_error: 0.0130     \n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0119 - mean_squared_error: 0.0119     \n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0099 - mean_squared_error: 0.0099     \n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0107 - mean_squared_error: 0.0107     \n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0112 - mean_squared_error: 0.0112     \n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0112 - mean_squared_error: 0.0112     \n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0124 - mean_squared_error: 0.0124     \n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0118 - mean_squared_error: 0.0118     \n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0099 - mean_squared_error: 0.0099     \n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0100 - mean_squared_error: 0.0100     \n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0106 - mean_squared_error: 0.0106     \n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0117 - mean_squared_error: 0.0117     \n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0112 - mean_squared_error: 0.0112     \n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0089 - mean_squared_error: 0.0089     \n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0111 - mean_squared_error: 0.0111     \n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0115 - mean_squared_error: 0.0115     \n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0109 - mean_squared_error: 0.0109     \n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0100 - mean_squared_error: 0.0100     \n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0099 - mean_squared_error: 0.0099     \n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0106 - mean_squared_error: 0.0106     \n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0099 - mean_squared_error: 0.0099     \n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0121 - mean_squared_error: 0.0121     \n",
      "Mean Squared Error: 0.1258839583981294\n",
      "Mean Absolute Error: 0.07678654679361657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.963425"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "model = create_model(300, 300, X_train.shape[1], Y_train.shape[1])\n",
    "model.fit(X_train, Y_train, epochs=100)\n",
    "predictions = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mse = mean_squared_error(predictions, Y_test)\n",
    "mabse = mean_absolute_error(predictions, Y_test)\n",
    "print('Mean Squared Error: {0}'.format(mse))\n",
    "print('Mean Absolute Error: {0}'.format(mabse))\n",
    "rounded_predictions = np.where(predictions > 0, 1, -1)\n",
    "calc_accuracy(Y_test, rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 1s - loss: 0.0231 - mean_squared_error: 0.0231     \n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0079 - mean_squared_error: 0.0079     \n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0061 - mean_squared_error: 0.0061     \n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0052 - mean_squared_error: 0.0052     \n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0046 - mean_squared_error: 0.0046     \n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0040 - mean_squared_error: 0.0040     \n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0035 - mean_squared_error: 0.0035     \n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0033 - mean_squared_error: 0.0033     \n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0030 - mean_squared_error: 0.0030     \n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0029 - mean_squared_error: 0.0029     \n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0025 - mean_squared_error: 0.0025     \n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0024 - mean_squared_error: 0.0024     \n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0023 - mean_squared_error: 0.0023     \n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0021 - mean_squared_error: 0.0021      ETA: 0s - loss: 0.0020 - mean_squared_error\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0019 - mean_squared_error: 0.0019     \n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0017 - mean_squared_error: 0.0017     \n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0017 - mean_squared_error: 0.0017     \n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016     \n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015     \n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0014 - mean_squared_error: 0.0014     \n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0014 - mean_squared_error: 0.0014     \n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0013 - mean_squared_error: 0.0013     \n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0012 - mean_squared_error: 0.0012     \n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015     \n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0013 - mean_squared_error: 0.0013     \n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0012 - mean_squared_error: 0.0012     \n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0011 - mean_squared_error: 0.0011     \n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0011 - mean_squared_error: 0.0011       \n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0010 - mean_squared_error: 0.0010     \n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0010 - mean_squared_error: 0.0010             \n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s - loss: 9.1609e-04 - mean_squared_error: 9.1609e-04     \n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 0s - loss: 9.2507e-04 - mean_squared_error: 9.2507e-04     \n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s - loss: 8.9555e-04 - mean_squared_error: 8.9555e-04     \n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s - loss: 8.9525e-04 - mean_squared_error: 8.9525e-04     \n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s - loss: 8.7257e-04 - mean_squared_error: 8.7257e-04     \n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s - loss: 8.8977e-04 - mean_squared_error: 8.8977e-04     \n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s - loss: 9.4402e-04 - mean_squared_error: 9.4402e-04     \n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s - loss: 8.9988e-04 - mean_squared_error: 8.9988e-04     \n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0011 - mean_squared_error: 0.0011     \n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.7380e-04 - mean_squared_error: 7.7380e-04     \n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.9740e-04 - mean_squared_error: 7.9740e-04     \n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.4833e-04 - mean_squared_error: 7.4833e-04     \n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.8963e-04 - mean_squared_error: 6.8963e-04     \n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.7108e-04 - mean_squared_error: 6.7108e-04     \n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.4509e-04 - mean_squared_error: 7.4509e-04     \n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.8620e-04 - mean_squared_error: 7.8620e-04     \n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.6079e-04 - mean_squared_error: 7.6079e-04     \n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.8096e-04 - mean_squared_error: 6.8096e-04     \n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.7422e-04 - mean_squared_error: 6.7422e-04     \n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.1712e-04 - mean_squared_error: 6.1712e-04     \n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.1547e-04 - mean_squared_error: 6.1547e-04     \n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.8371e-04 - mean_squared_error: 5.8371e-04     \n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.5251e-04 - mean_squared_error: 6.5251e-04     \n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.7298e-04 - mean_squared_error: 6.7298e-04     \n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.1673e-04 - mean_squared_error: 7.1673e-04     \n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.0083e-04 - mean_squared_error: 7.0083e-04     \n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.1299e-04 - mean_squared_error: 6.1299e-04     \n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.7817e-04 - mean_squared_error: 5.7817e-04     \n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.4587e-04 - mean_squared_error: 5.4587e-04     \n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.3526e-04 - mean_squared_error: 5.3526e-04     \n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.7193e-04 - mean_squared_error: 5.7193e-04     \n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.4012e-04 - mean_squared_error: 5.4012e-04     \n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.9892e-04 - mean_squared_error: 4.9892e-04     \n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.9601e-04 - mean_squared_error: 5.9601e-04     \n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.0577e-04 - mean_squared_error: 6.0577e-04     \n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.6426e-04 - mean_squared_error: 5.6426e-04     \n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.0979e-04 - mean_squared_error: 5.0979e-04     \n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.0751e-04 - mean_squared_error: 5.0751e-04     \n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.5348e-04 - mean_squared_error: 4.5348e-04     \n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.1428e-04 - mean_squared_error: 5.1428e-04     \n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.0134e-04 - mean_squared_error: 5.0134e-04     \n",
      "Epoch 72/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s - loss: 4.8633e-04 - mean_squared_error: 4.8633e-04     \n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.5724e-04 - mean_squared_error: 4.5724e-04     \n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.9316e-04 - mean_squared_error: 4.9316e-04     \n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.6844e-04 - mean_squared_error: 4.6844e-04     \n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.6410e-04 - mean_squared_error: 4.6410e-04     \n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.7005e-04 - mean_squared_error: 4.7005e-04     \n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.7612e-04 - mean_squared_error: 4.7612e-04     \n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.7417e-04 - mean_squared_error: 4.7417e-04     \n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.6037e-04 - mean_squared_error: 4.6037e-04     \n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.4635e-04 - mean_squared_error: 4.4635e-04     \n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.2276e-04 - mean_squared_error: 4.2276e-04     \n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.4100e-04 - mean_squared_error: 4.4100e-04     \n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.1675e-04 - mean_squared_error: 4.1675e-04     \n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.5230e-04 - mean_squared_error: 4.5230e-04     \n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.4703e-04 - mean_squared_error: 4.4703e-04     \n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.5900e-04 - mean_squared_error: 4.5900e-04     \n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.3874e-04 - mean_squared_error: 4.3874e-04     \n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.8779e-04 - mean_squared_error: 3.8779e-04     \n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.1728e-04 - mean_squared_error: 4.1728e-04     \n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.0587e-04 - mean_squared_error: 4.0587e-04     \n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.1228e-04 - mean_squared_error: 4.1228e-04     \n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.2162e-04 - mean_squared_error: 4.2162e-04     \n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.0246e-04 - mean_squared_error: 4.0246e-04     \n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.6677e-04 - mean_squared_error: 3.6677e-04     \n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.6579e-04 - mean_squared_error: 3.6579e-04     \n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.7606e-04 - mean_squared_error: 3.7606e-04     \n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.5923e-04 - mean_squared_error: 3.5923e-04     \n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.4807e-04 - mean_squared_error: 3.4807e-04     \n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.3754e-04 - mean_squared_error: 4.3754e-04     \n",
      "Mean Squared Error: 0.006299316999302837\n",
      "Mean Absolute Error: 0.06200604512354965\n"
     ]
    }
   ],
   "source": [
    "def generate_preamble_datasets(dataset_size, preamble_length, channel_length, SNR):\n",
    "        \n",
    "    preambles = np.random.randint(0, 2, (dataset_size, preamble_length))\n",
    "    convolved_preambles = []\n",
    "    channels = []\n",
    "    for i in range(dataset_size):\n",
    "        channel_taps = np.random.uniform(0,1,channel_length)\n",
    "        if sum(channel_taps)>=1:\n",
    "            channel_taps = channel_taps / sum(channel_taps)\n",
    "            \n",
    "        channels.append(channel_taps)\n",
    "        preamble_conv = add_awgn_noise(sig.convolve(preambles[i], channel_taps, mode='same'), SNR)\n",
    "        convolved_preambles.append(preamble_conv)\n",
    "\n",
    "    X = np.hstack([preambles, np.array(convolved_preambles)])\n",
    "    Y = np.array(channels)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = generate_preamble_datasets(10000, 20, 2, 10)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "model = create_model(300, 300, X_train.shape[1], Y_train.shape[1])\n",
    "model.fit(X_train, Y_train, epochs=100)\n",
    "predictions = model.predict(X_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "mse = mean_squared_error(predictions, Y_test)\n",
    "mabse = mean_absolute_error(predictions, Y_test)\n",
    "print('Mean Squared Error: {0}'.format(mse))\n",
    "print('Mean Absolute Error: {0}'.format(mabse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 81) (10000, 42)\n",
      "(8000, 40) (2000, 40)\n",
      "(8000, 2) (2000, 2)\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0296 - mean_squared_error: 0.0296     \n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0088 - mean_squared_error: 0.0088     \n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0066 - mean_squared_error: 0.0066     \n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0055 - mean_squared_error: 0.0055     \n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0049 - mean_squared_error: 0.0049     \n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0044 - mean_squared_error: 0.0044     \n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0039 - mean_squared_error: 0.0039     \n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0036 - mean_squared_error: 0.0036     \n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0031 - mean_squared_error: 0.0031     \n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0032 - mean_squared_error: 0.0032     \n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0028 - mean_squared_error: 0.0028     \n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0025 - mean_squared_error: 0.0025     \n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0023 - mean_squared_error: 0.0023     \n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0021 - mean_squared_error: 0.0021     \n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0020 - mean_squared_error: 0.0020     \n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0019 - mean_squared_error: 0.0019     \n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0019 - mean_squared_error: 0.0019     \n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0018 - mean_squared_error: 0.0018     \n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016     \n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0016 - mean_squared_error: 0.0016     \n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0015 - mean_squared_error: 0.0015     \n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0013 - mean_squared_error: 0.0013     \n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0012 - mean_squared_error: 0.0012     \n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0012 - mean_squared_error: 0.0012     \n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0013 - mean_squared_error: 0.0013     \n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0013 - mean_squared_error: 0.0013     \n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0013 - mean_squared_error: 0.0013     \n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0012 - mean_squared_error: 0.0012     \n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0012 - mean_squared_error: 0.0012     \n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0011 - mean_squared_error: 0.0011     \n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0010 - mean_squared_error: 0.0010     \n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 0s - loss: 9.8452e-04 - mean_squared_error: 9.8452e-04     \n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s - loss: 8.8936e-04 - mean_squared_error: 8.8936e-04     \n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s - loss: 9.3127e-04 - mean_squared_error: 9.3127e-04     \n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0010 - mean_squared_error: 0.0010     \n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s - loss: 9.8414e-04 - mean_squared_error: 9.8414e-04      ETA: 0s - loss: 9.4855e-04 - mean_squared_error: 9.48\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s - loss: 8.8278e-04 - mean_squared_error: 8.8278e-04     \n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s - loss: 8.0080e-04 - mean_squared_error: 8.0080e-04     \n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.7500e-04 - mean_squared_error: 7.7500e-04     \n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.9534e-04 - mean_squared_error: 7.9534e-04     \n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.8103e-04 - mean_squared_error: 7.8103e-04     \n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 0s - loss: 8.8528e-04 - mean_squared_error: 8.8528e-04     \n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 0s - loss: 8.6494e-04 - mean_squared_error: 8.6494e-04     \n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s - loss: 9.3126e-04 - mean_squared_error: 9.3126e-04     \n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.7384e-04 - mean_squared_error: 7.7384e-04     \n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.7495e-04 - mean_squared_error: 6.7495e-04     \n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s - loss: 7.1205e-04 - mean_squared_error: 7.1205e-04     \n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.6514e-04 - mean_squared_error: 6.6514e-04     \n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.9026e-04 - mean_squared_error: 6.9026e-04     \n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.9756e-04 - mean_squared_error: 6.9756e-04     \n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.9994e-04 - mean_squared_error: 6.9994e-04     \n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.6210e-04 - mean_squared_error: 6.6210e-04     \n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.3030e-04 - mean_squared_error: 6.3030e-04     \n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.6643e-04 - mean_squared_error: 6.6643e-04     \n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.5994e-04 - mean_squared_error: 6.5994e-04     \n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.6173e-04 - mean_squared_error: 6.6173e-04     \n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.0271e-04 - mean_squared_error: 6.0271e-04     \n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.0980e-04 - mean_squared_error: 6.0980e-04     \n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.5425e-04 - mean_squared_error: 5.5425e-04      ETA: 0s - loss: 5.4946e-04 - mean_squared_error: 5.4946e-\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.5674e-04 - mean_squared_error: 5.5674e-04     \n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.9384e-04 - mean_squared_error: 5.9384e-04     \n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s - loss: 6.4564e-04 - mean_squared_error: 6.4564e-04     \n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.7522e-04 - mean_squared_error: 5.7522e-04     \n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.4149e-04 - mean_squared_error: 5.4149e-04     \n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.9122e-04 - mean_squared_error: 5.9122e-04     \n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.8314e-04 - mean_squared_error: 5.8314e-04     \n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.3095e-04 - mean_squared_error: 5.3095e-04     \n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.1199e-04 - mean_squared_error: 5.1199e-04     \n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.9413e-04 - mean_squared_error: 4.9413e-04     \n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.7964e-04 - mean_squared_error: 5.7964e-04     \n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s - loss: 4.9578e-04 - mean_squared_error: 4.9578e-04     \n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.7470e-04 - mean_squared_error: 4.7470e-04     \n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.6300e-04 - mean_squared_error: 4.6300e-04     \n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.2944e-04 - mean_squared_error: 4.2944e-04     \n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.1853e-04 - mean_squared_error: 4.1853e-04     \n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.0429e-04 - mean_squared_error: 5.0429e-04     \n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.6286e-04 - mean_squared_error: 5.6286e-04     \n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.2368e-04 - mean_squared_error: 5.2368e-04     \n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.6425e-04 - mean_squared_error: 4.6425e-04     \n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.3980e-04 - mean_squared_error: 4.3980e-04     \n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.1344e-04 - mean_squared_error: 4.1344e-04     \n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.4109e-04 - mean_squared_error: 4.4109e-04     \n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.1859e-04 - mean_squared_error: 4.1859e-04     \n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.1592e-04 - mean_squared_error: 4.1592e-04     \n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.7220e-04 - mean_squared_error: 4.7220e-04     \n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.4404e-04 - mean_squared_error: 4.4404e-04     \n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.5720e-04 - mean_squared_error: 4.5720e-04     \n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.5439e-04 - mean_squared_error: 4.5439e-04     \n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s - loss: 5.0822e-04 - mean_squared_error: 5.0822e-04     \n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.6321e-04 - mean_squared_error: 4.6321e-04     \n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.2717e-04 - mean_squared_error: 4.2717e-04     \n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.8504e-04 - mean_squared_error: 3.8504e-04     \n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.8772e-04 - mean_squared_error: 3.8772e-04     \n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.7467e-04 - mean_squared_error: 3.7467e-04     \n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.5968e-04 - mean_squared_error: 3.5968e-04     \n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s - loss: 4.1911e-04 - mean_squared_error: 4.1911e-04     \n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.7912e-04 - mean_squared_error: 3.7912e-04     \n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.9468e-04 - mean_squared_error: 3.9468e-04     \n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.6610e-04 - mean_squared_error: 3.6610e-04     \n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s - loss: 3.8330e-04 - mean_squared_error: 3.8330e-04     \n",
      "(8000, 43) (2000, 43) (8000, 40) (8000, 40)\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.4609 - mean_squared_error: 0.4609     \n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.3161 - mean_squared_error: 0.3161     \n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2934 - mean_squared_error: 0.2934     \n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2704 - mean_squared_error: 0.2704     \n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.2352 - mean_squared_error: 0.2352     \n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1906 - mean_squared_error: 0.1906     \n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1525 - mean_squared_error: 0.1525     \n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.1203 - mean_squared_error: 0.1203     \n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0983 - mean_squared_error: 0.0983     \n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0843 - mean_squared_error: 0.0843     \n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0744 - mean_squared_error: 0.0744     \n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0667 - mean_squared_error: 0.0667     \n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0613 - mean_squared_error: 0.0613     \n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0568 - mean_squared_error: 0.0568     \n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0533 - mean_squared_error: 0.0533     \n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0494 - mean_squared_error: 0.0494     \n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0458 - mean_squared_error: 0.0458     \n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0440 - mean_squared_error: 0.0440     \n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0403 - mean_squared_error: 0.0403     \n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0385 - mean_squared_error: 0.0385     \n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0364 - mean_squared_error: 0.0364     \n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0345 - mean_squared_error: 0.0345     \n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0330 - mean_squared_error: 0.0330     \n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0313 - mean_squared_error: 0.0313     \n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0299 - mean_squared_error: 0.0299     \n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0283 - mean_squared_error: 0.0283     \n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0259 - mean_squared_error: 0.0259     \n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0254 - mean_squared_error: 0.0254     \n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0236 - mean_squared_error: 0.0236     \n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0227 - mean_squared_error: 0.0227     \n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0228 - mean_squared_error: 0.0228     \n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0219 - mean_squared_error: 0.0219     \n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0207 - mean_squared_error: 0.0207     \n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0198 - mean_squared_error: 0.0198     \n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0199 - mean_squared_error: 0.0199     \n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0184 - mean_squared_error: 0.0184     \n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0183 - mean_squared_error: 0.0183     \n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0186 - mean_squared_error: 0.0186     \n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0177 - mean_squared_error: 0.0177     \n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0166 - mean_squared_error: 0.0166     \n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0152 - mean_squared_error: 0.0152     \n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0153 - mean_squared_error: 0.0153     \n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 0s - loss: 0.0168 - mean_squared_error: 0.0168     \n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0158 - mean_squared_error: 0.0158     \n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0143 - mean_squared_error: 0.0143     \n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0140 - mean_squared_error: 0.0140     \n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0134 - mean_squared_error: 0.0134     \n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0133 - mean_squared_error: 0.0133     \n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0132 - mean_squared_error: 0.0132     \n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0145 - mean_squared_error: 0.0145     \n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0147 - mean_squared_error: 0.0147     \n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0137 - mean_squared_error: 0.0137     \n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0124 - mean_squared_error: 0.0124     \n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0126 - mean_squared_error: 0.0126     \n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0129 - mean_squared_error: 0.0129     \n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0136 - mean_squared_error: 0.0136     \n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0141 - mean_squared_error: 0.0141     \n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0115 - mean_squared_error: 0.0115     \n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0112 - mean_squared_error: 0.0112     \n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0118 - mean_squared_error: 0.0118     \n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0122 - mean_squared_error: 0.0122     \n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0113 - mean_squared_error: 0.0113     \n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0116 - mean_squared_error: 0.0116     \n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0112 - mean_squared_error: 0.0112     \n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0126 - mean_squared_error: 0.0126     \n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0111 - mean_squared_error: 0.0111     \n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0106 - mean_squared_error: 0.0106     \n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0122 - mean_squared_error: 0.0122     \n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0112 - mean_squared_error: 0.0112     \n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0109 - mean_squared_error: 0.0109     \n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0107 - mean_squared_error: 0.0107     \n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0103 - mean_squared_error: 0.0103     \n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0101 - mean_squared_error: 0.0101     \n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0114 - mean_squared_error: 0.0114     \n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0111 - mean_squared_error: 0.0111     \n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0088 - mean_squared_error: 0.0088     \n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0085 - mean_squared_error: 0.0085     \n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0120 - mean_squared_error: 0.0120     \n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0094 - mean_squared_error: 0.0094     \n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0100 - mean_squared_error: 0.0100     \n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0098 - mean_squared_error: 0.0098     \n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0100 - mean_squared_error: 0.0100     \n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0122 - mean_squared_error: 0.0122     \n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0097 - mean_squared_error: 0.0097     \n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0093 - mean_squared_error: 0.0093     \n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0090 - mean_squared_error: 0.0090     \n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0093 - mean_squared_error: 0.0093     \n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0103 - mean_squared_error: 0.0103     \n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0102 - mean_squared_error: 0.0102     \n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0098 - mean_squared_error: 0.0098     \n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0086 - mean_squared_error: 0.0086     \n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0098 - mean_squared_error: 0.0098     \n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0111 - mean_squared_error: 0.0111     \n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0098 - mean_squared_error: 0.0098     \n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0082 - mean_squared_error: 0.0082     \n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0077 - mean_squared_error: 0.0077     \n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0091 - mean_squared_error: 0.0091     \n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0086 - mean_squared_error: 0.0086     \n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0096 - mean_squared_error: 0.0096     \n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 0s - loss: 0.0113 - mean_squared_error: 0.0113     \n",
      "Mean Squared Error: 0.1383385401146661\n",
      "Mean Absolute Error: 0.08303870677636487\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9598625"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine previous two neural networks\n",
    "def generate_combined_dataset(dataset_size, preamble_length, data_length, channel_length, SNR):\n",
    "    input_data_bits = np.random.randint(0,2,(dataset_size, data_length)) \n",
    "    input_data_constellations = input_data_bits * 2 - 1\n",
    "        \n",
    "    preambles = np.random.randint(0, 2, (dataset_size, preamble_length))\n",
    "    convolved_preambles = []\n",
    "    convolved_data_constellations = []\n",
    "    channels = []\n",
    "    for i in range(dataset_size):\n",
    "        channel_taps = np.random.uniform(0,1,channel_length)\n",
    "        if sum(channel_taps)>=1:\n",
    "            channel_taps = channel_taps / sum(channel_taps)\n",
    "            \n",
    "        channels.append(channel_taps)\n",
    "        preamble_conv = add_awgn_noise(sig.convolve(preambles[i], channel_taps, mode='same'), SNR)\n",
    "        convolved_preambles.append(preamble_conv)\n",
    "        constellation_convolved = add_awgn_noise(sig.convolve(input_data_constellations[i], channel_taps, mode='full'), SNR)\n",
    "        convolved_data_constellations.append(constellation_convolved)\n",
    "    X = np.hstack([preambles, np.array(convolved_preambles), np.array(convolved_data_constellations)])\n",
    "    Y = np.hstack([np.array(channels), input_data_constellations])\n",
    "    return X, Y\n",
    "\n",
    "preamble_length = 20\n",
    "dataset_size = 10000\n",
    "data_length = 40\n",
    "channel_length = 2\n",
    "SNR= 10\n",
    "X, Y = generate_combined_dataset(dataset_size, preamble_length, data_length, channel_length, SNR)\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "# print(X)\n",
    "# print(Y)\n",
    "preamble_X_train, preamble_X_test = X_train[:, :preamble_length * 2],  X_test[:, :preamble_length * 2]\n",
    "channel_Y_train, channel_Y_test = Y_train[:, :channel_length], Y_test[:, :channel_length]\n",
    "print(preamble_X_train.shape, preamble_X_test.shape)\n",
    "print(channel_Y_train.shape, channel_Y_test.shape)\n",
    "model_1 = create_model(300, 300, preamble_X_train.shape[1], channel_Y_train.shape[1])\n",
    "model_1.fit(preamble_X_train, channel_Y_train, epochs=100)\n",
    "\n",
    "# predictions = model.predict(preamble_X_test)\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# mse = mean_squared_error(predictions, channel_Y_test)\n",
    "# mabse = mean_absolute_error(predictions, channel_Y_test)\n",
    "# print('Mean Squared Error: {0}'.format(mse))\n",
    "# print('Mean Absolute Error: {0}'.format(mabse))\n",
    "channel_X_train = model_1.predict(preamble_X_train)\n",
    "channel_X_test = model_1.predict(preamble_X_test)\n",
    "data_convolved_X_train, data_convolved_X_test = X_train[:, preamble_length * 2:],  X_test[:, preamble_length * 2:]\n",
    "new_X_train = np.hstack([channel_X_train, data_convolved_X_train])\n",
    "new_X_test = np.hstack([channel_X_test, data_convolved_X_test])\n",
    "new_Y_train, new_Y_test = Y_train[:, channel_length:], Y_test[:, channel_length:]\n",
    "print(new_X_train.shape, new_X_test.shape, new_Y_train.shape, new_Y_train.shape)\n",
    "\n",
    "model_2 = create_model(300, 300, new_X_train.shape[1], new_Y_train.shape[1])\n",
    "model_2.fit(new_X_train, new_Y_train, epochs=100)\n",
    "predictions = model_2.predict(new_X_test)\n",
    "mse = mean_squared_error(predictions, new_Y_test)\n",
    "mabse = mean_absolute_error(predictions, new_Y_test)\n",
    "print('Mean Squared Error: {0}'.format(mse))\n",
    "print('Mean Absolute Error: {0}'.format(mabse))\n",
    "rounded_predictions = np.where(predictions > 0, 1, -1)\n",
    "calc_accuracy(new_Y_test, rounded_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 41)\n",
      "0.9587125\n"
     ]
    }
   ],
   "source": [
    "def zfEqualize(channel_output, channel):\n",
    "    # make sure have same fft length \n",
    "    # we get the correct number of terms as this is like a circular convolution so the last packet is garbage\n",
    "    freq_domain = np.fft.fft(channel_output, len(channel_output))/np.fft.fft(channel, len(channel_output))\n",
    "    return np.fft.ifft(freq_domain)[0:len(channel_output) - len(channel) + 1] # check \n",
    "\n",
    "# Count number of symbols that are off\n",
    "def calc_accuracy(original, predictions):\n",
    "    return np.sum(original == predictions) / (original.shape[0] * original.shape[1])\n",
    "\n",
    "# Adds complex gaussian noise to inputted signal \n",
    "def add_awgn_noise(signal,SNR_dB):\n",
    "    \"\"\"  Adds AWGN noise vector to signal \n",
    "         to generate a resulting signal vector y of specified SNR in dB\n",
    "    \"\"\"\n",
    "    L=len(signal)\n",
    "    SNR = 10**(SNR_dB/10.0) #SNR to linear scale\n",
    "    Esym=np.sum(np.square(np.abs(signal)))/L #Calculate actual symbol energy\n",
    "    N0=Esym/SNR; #Find the noise spectral density\n",
    "    if(isinstance(signal[0], complex)):\n",
    "        noiseSigma=np.sqrt(N0/2.0)#Standard deviation for AWGN Noise when x is complex\n",
    "        n = noiseSigma*(np.random.randn(1,L)+1j*np.random.randn(1,L))#computed noise \n",
    "    else:\n",
    "        noiseSigma = np.sqrt(N0);#Standard deviation for AWGN Noise when x is real\n",
    "        n = noiseSigma*np.random.randn(1,L)#computed noise\n",
    "    y = signal + n #received signal\n",
    "    \n",
    "    return y.flatten()\n",
    "\n",
    "def test_zf_accuracy(channel_outputs, true_data, channels, data_length, channel_length):\n",
    "    # make modulator and message size\n",
    "    predictions = []\n",
    "    print(channel_outputs.shape)\n",
    "    for i in range(channel_outputs.shape[0]):\n",
    "        equalized_result = zfEqualize(channel_outputs[i], channels[i])\n",
    "        real_component = np.real(equalized_result)\n",
    "        rounded_predictions = np.where(real_component > 0, 1, -1)\n",
    "        predictions.append(rounded_predictions)\n",
    "    return calc_accuracy(np.array(predictions), true_data)\n",
    "        \n",
    "accuracy = test_zf_accuracy(data_convolved_X_test, new_Y_test, channel_Y_test, 40, 2)\n",
    "\n",
    "print(accuracy)\n",
    "#     constellation = 4\n",
    "#     QAMModem = cp.modulation.QAMModem(constellation)\n",
    "#     n = constellation * num_bits\n",
    "#     sym_errors = 0 \n",
    "\n",
    "#     for i in range(runs):\n",
    "#         # generate input bits and constellation\n",
    "#         input_bits = np.random.randint(0,2,n) \n",
    "#         input_constellation = QAMModem.modulate(input_bits)\n",
    "#         # pass signal through random channel\n",
    "#         channel_function= np.random.randn(channel_len)\n",
    "#         channel_output = sig.convolve(input_constellation, channel_function, mode=\"full\")\n",
    "#         channel_output = addNoise(channel_output, noise_amp)\n",
    "#         # equalize, demodulate and calculate the symbol error rate\n",
    "#         eqd = zfEqualize(channel_output, channel_function)\n",
    "#         falses = numFalses(input_bits, eqd, QAMModem)\n",
    "#         sym_errors += falses\n",
    "#     sym_error_rate = sym_errors/(n*runs)\n",
    "#     return sym_error_rate\n",
    "\n",
    "# Generate data samples run them through a channel then equalize using zero force equalizer and return symbol error rate\n",
    "# def find_SER(channel_len, noise_amp, runs = 10, num_bits = 4 ):\n",
    "#     # make modulator and message size\n",
    "#     constellation = 4\n",
    "#     QAMModem = cp.modulation.QAMModem(constellation)\n",
    "#     n = constellation * num_bits\n",
    "#     sym_errors = 0 \n",
    "\n",
    "#     for i in range(runs):\n",
    "#         # generate input bits and constellation\n",
    "#         input_bits = np.random.randint(0,2,n) \n",
    "#         input_constellation = QAMModem.modulate(input_bits)\n",
    "#         # pass signal through random channel\n",
    "#         channel_function= np.random.randn(channel_len)\n",
    "#         channel_output = sig.convolve(input_constellation, channel_function, mode=\"full\")\n",
    "#         channel_output = addNoise(channel_output, noise_amp)\n",
    "#         # equalize, demodulate and calculate the symbol error rate\n",
    "#         eqd = zfEqualize(channel_output, channel_function)\n",
    "#         falses = numFalses(input_bits, eqd, QAMModem)\n",
    "#         sym_errors += falses\n",
    "#     sym_error_rate = sym_errors/(n*runs)\n",
    "#     return sym_error_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size 10000\n",
      "Trial 0\n",
      "(8000, 40) (2000, 40)\n",
      "(8000, 2) (2000, 2)\n",
      "Fitting first model\n",
      "(8000, 43) (2000, 43) (8000, 40) (8000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96025\n",
      "(2000, 41)\n",
      "Zero Force accuracy 0.961\n",
      "Dataset size 10000\n",
      "Trial 1\n",
      "(8000, 40) (2000, 40)\n",
      "(8000, 2) (2000, 2)\n",
      "Fitting first model\n",
      "(8000, 43) (2000, 43) (8000, 40) (8000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.957675\n",
      "(2000, 41)\n",
      "Zero Force accuracy 0.9598125\n",
      "Dataset size 10000\n",
      "Trial 2\n",
      "(8000, 40) (2000, 40)\n",
      "(8000, 2) (2000, 2)\n",
      "Fitting first model\n",
      "(8000, 43) (2000, 43) (8000, 40) (8000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9602875\n",
      "(2000, 41)\n",
      "Zero Force accuracy 0.9645\n",
      "Dataset size 10000\n",
      "Trial 3\n",
      "(8000, 40) (2000, 40)\n",
      "(8000, 2) (2000, 2)\n",
      "Fitting first model\n",
      "(8000, 43) (2000, 43) (8000, 40) (8000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9616625\n",
      "(2000, 41)\n",
      "Zero Force accuracy 0.9626375\n",
      "Dataset size 10000\n",
      "Trial 4\n",
      "(8000, 40) (2000, 40)\n",
      "(8000, 2) (2000, 2)\n",
      "Fitting first model\n",
      "(8000, 43) (2000, 43) (8000, 40) (8000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9605\n",
      "(2000, 41)\n",
      "Zero Force accuracy 0.9631625\n",
      "Dataset size 10000\n",
      "Trial 5\n",
      "(8000, 40) (2000, 40)\n",
      "(8000, 2) (2000, 2)\n",
      "Fitting first model\n",
      "(8000, 43) (2000, 43) (8000, 40) (8000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.958\n",
      "(2000, 41)\n",
      "Zero Force accuracy 0.9601375\n",
      "Dataset size 10000\n",
      "Trial 6\n",
      "(8000, 40) (2000, 40)\n",
      "(8000, 2) (2000, 2)\n",
      "Fitting first model\n",
      "(8000, 43) (2000, 43) (8000, 40) (8000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9567625\n",
      "(2000, 41)\n",
      "Zero Force accuracy 0.963325\n",
      "Dataset size 10000\n",
      "Trial 7\n",
      "(8000, 40) (2000, 40)\n",
      "(8000, 2) (2000, 2)\n",
      "Fitting first model\n",
      "(8000, 43) (2000, 43) (8000, 40) (8000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96165\n",
      "(2000, 41)\n",
      "Zero Force accuracy 0.9622625\n",
      "Dataset size 10000\n",
      "Trial 8\n",
      "(8000, 40) (2000, 40)\n",
      "(8000, 2) (2000, 2)\n",
      "Fitting first model\n",
      "(8000, 43) (2000, 43) (8000, 40) (8000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9585\n",
      "(2000, 41)\n",
      "Zero Force accuracy 0.962875\n",
      "Dataset size 10000\n",
      "Trial 9\n",
      "(8000, 40) (2000, 40)\n",
      "(8000, 2) (2000, 2)\n",
      "Fitting first model\n",
      "(8000, 43) (2000, 43) (8000, 40) (8000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9591875\n",
      "(2000, 41)\n",
      "Zero Force accuracy 0.9599375\n",
      "Dataset size 20000\n",
      "Trial 0\n",
      "(16000, 40) (4000, 40)\n",
      "(16000, 2) (4000, 2)\n",
      "Fitting first model\n",
      "(16000, 43) (4000, 43) (16000, 40) (16000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96551875\n",
      "(4000, 41)\n",
      "Zero Force accuracy 0.962375\n",
      "Dataset size 20000\n",
      "Trial 1\n",
      "(16000, 40) (4000, 40)\n",
      "(16000, 2) (4000, 2)\n",
      "Fitting first model\n",
      "(16000, 43) (4000, 43) (16000, 40) (16000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.964625\n",
      "(4000, 41)\n",
      "Zero Force accuracy 0.9611875\n",
      "Dataset size 20000\n",
      "Trial 2\n",
      "(16000, 40) (4000, 40)\n",
      "(16000, 2) (4000, 2)\n",
      "Fitting first model\n",
      "(16000, 43) (4000, 43) (16000, 40) (16000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96509375\n",
      "(4000, 41)\n",
      "Zero Force accuracy 0.9612\n",
      "Dataset size 20000\n",
      "Trial 3\n",
      "(16000, 40) (4000, 40)\n",
      "(16000, 2) (4000, 2)\n",
      "Fitting first model\n",
      "(16000, 43) (4000, 43) (16000, 40) (16000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9649125\n",
      "(4000, 41)\n",
      "Zero Force accuracy 0.95975\n",
      "Dataset size 20000\n",
      "Trial 4\n",
      "(16000, 40) (4000, 40)\n",
      "(16000, 2) (4000, 2)\n",
      "Fitting first model\n",
      "(16000, 43) (4000, 43) (16000, 40) (16000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9645125\n",
      "(4000, 41)\n",
      "Zero Force accuracy 0.960075\n",
      "Dataset size 20000\n",
      "Trial 5\n",
      "(16000, 40) (4000, 40)\n",
      "(16000, 2) (4000, 2)\n",
      "Fitting first model\n",
      "(16000, 43) (4000, 43) (16000, 40) (16000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9653875\n",
      "(4000, 41)\n",
      "Zero Force accuracy 0.9614125\n",
      "Dataset size 20000\n",
      "Trial 6\n",
      "(16000, 40) (4000, 40)\n",
      "(16000, 2) (4000, 2)\n",
      "Fitting first model\n",
      "(16000, 43) (4000, 43) (16000, 40) (16000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96411875\n",
      "(4000, 41)\n",
      "Zero Force accuracy 0.96075625\n",
      "Dataset size 20000\n",
      "Trial 7\n",
      "(16000, 40) (4000, 40)\n",
      "(16000, 2) (4000, 2)\n",
      "Fitting first model\n",
      "(16000, 43) (4000, 43) (16000, 40) (16000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96456875\n",
      "(4000, 41)\n",
      "Zero Force accuracy 0.96064375\n",
      "Dataset size 20000\n",
      "Trial 8\n",
      "(16000, 40) (4000, 40)\n",
      "(16000, 2) (4000, 2)\n",
      "Fitting first model\n",
      "(16000, 43) (4000, 43) (16000, 40) (16000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9651\n",
      "(4000, 41)\n",
      "Zero Force accuracy 0.95875625\n",
      "Dataset size 20000\n",
      "Trial 9\n",
      "(16000, 40) (4000, 40)\n",
      "(16000, 2) (4000, 2)\n",
      "Fitting first model\n",
      "(16000, 43) (4000, 43) (16000, 40) (16000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.964625\n",
      "(4000, 41)\n",
      "Zero Force accuracy 0.96283125\n",
      "Dataset size 30000\n",
      "Trial 0\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96685\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9598083333333334\n",
      "Dataset size 30000\n",
      "Trial 1\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9673541666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9599083333333334\n",
      "Dataset size 30000\n",
      "Trial 2\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9674708333333333\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9621291666666667\n",
      "Dataset size 30000\n",
      "Trial 3\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9664166666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9617916666666667\n",
      "Dataset size 30000\n",
      "Trial 4\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9674625\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9598916666666667\n",
      "Dataset size 30000\n",
      "Trial 5\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9677791666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9606541666666667\n",
      "Dataset size 30000\n",
      "Trial 6\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9659583333333334\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9612916666666667\n",
      "Dataset size 30000\n",
      "Trial 7\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9662875\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9598208333333333\n",
      "Dataset size 30000\n",
      "Trial 8\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9675791666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9597916666666667\n",
      "Dataset size 30000\n",
      "Trial 9\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9667625\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9618791666666666\n",
      "Dataset size 40000\n",
      "Trial 0\n",
      "(32000, 40) (8000, 40)\n",
      "(32000, 2) (8000, 2)\n",
      "Fitting first model\n",
      "(32000, 43) (8000, 43) (32000, 40) (32000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.967096875\n",
      "(8000, 41)\n",
      "Zero Force accuracy 0.959859375\n",
      "Dataset size 40000\n",
      "Trial 1\n",
      "(32000, 40) (8000, 40)\n",
      "(32000, 2) (8000, 2)\n",
      "Fitting first model\n",
      "(32000, 43) (8000, 43) (32000, 40) (32000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9677875\n",
      "(8000, 41)\n",
      "Zero Force accuracy 0.96233125\n",
      "Dataset size 40000\n",
      "Trial 2\n",
      "(32000, 40) (8000, 40)\n",
      "(32000, 2) (8000, 2)\n",
      "Fitting first model\n",
      "(32000, 43) (8000, 43) (32000, 40) (32000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.968296875\n",
      "(8000, 41)\n",
      "Zero Force accuracy 0.960171875\n",
      "Dataset size 40000\n",
      "Trial 3\n",
      "(32000, 40) (8000, 40)\n",
      "(32000, 2) (8000, 2)\n",
      "Fitting first model\n",
      "(32000, 43) (8000, 43) (32000, 40) (32000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96776875\n",
      "(8000, 41)\n",
      "Zero Force accuracy 0.9617125\n",
      "Dataset size 40000\n",
      "Trial 4\n",
      "(32000, 40) (8000, 40)\n",
      "(32000, 2) (8000, 2)\n",
      "Fitting first model\n",
      "(32000, 43) (8000, 43) (32000, 40) (32000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.967840625\n",
      "(8000, 41)\n",
      "Zero Force accuracy 0.961246875\n",
      "Dataset size 40000\n",
      "Trial 5\n",
      "(32000, 40) (8000, 40)\n",
      "(32000, 2) (8000, 2)\n",
      "Fitting first model\n",
      "(32000, 43) (8000, 43) (32000, 40) (32000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96804375\n",
      "(8000, 41)\n",
      "Zero Force accuracy 0.961190625\n",
      "Dataset size 40000\n",
      "Trial 6\n",
      "(32000, 40) (8000, 40)\n",
      "(32000, 2) (8000, 2)\n",
      "Fitting first model\n",
      "(32000, 43) (8000, 43) (32000, 40) (32000, 40)\n",
      "Fitting Second Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN accuracy 0.967665625\n",
      "(8000, 41)\n",
      "Zero Force accuracy 0.960915625\n",
      "Dataset size 40000\n",
      "Trial 7\n",
      "(32000, 40) (8000, 40)\n",
      "(32000, 2) (8000, 2)\n",
      "Fitting first model\n",
      "(32000, 43) (8000, 43) (32000, 40) (32000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96806875\n",
      "(8000, 41)\n",
      "Zero Force accuracy 0.96223125\n",
      "Dataset size 40000\n",
      "Trial 8\n",
      "(32000, 40) (8000, 40)\n",
      "(32000, 2) (8000, 2)\n",
      "Fitting first model\n",
      "(32000, 43) (8000, 43) (32000, 40) (32000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96758125\n",
      "(8000, 41)\n",
      "Zero Force accuracy 0.9608625\n",
      "Dataset size 40000\n",
      "Trial 9\n",
      "(32000, 40) (8000, 40)\n",
      "(32000, 2) (8000, 2)\n",
      "Fitting first model\n",
      "(32000, 43) (8000, 43) (32000, 40) (32000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9679375\n",
      "(8000, 41)\n",
      "Zero Force accuracy 0.961353125\n",
      "Dataset size 50000\n",
      "Trial 0\n",
      "(40000, 40) (10000, 40)\n",
      "(40000, 2) (10000, 2)\n",
      "Fitting first model\n",
      "(40000, 43) (10000, 43) (40000, 40) (40000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9692025\n",
      "(10000, 41)\n",
      "Zero Force accuracy 0.9611825\n",
      "Dataset size 50000\n",
      "Trial 1\n",
      "(40000, 40) (10000, 40)\n",
      "(40000, 2) (10000, 2)\n",
      "Fitting first model\n",
      "(40000, 43) (10000, 43) (40000, 40) (40000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.968775\n",
      "(10000, 41)\n",
      "Zero Force accuracy 0.960815\n",
      "Dataset size 50000\n",
      "Trial 2\n",
      "(40000, 40) (10000, 40)\n",
      "(40000, 2) (10000, 2)\n",
      "Fitting first model\n",
      "(40000, 43) (10000, 43) (40000, 40) (40000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9686575\n",
      "(10000, 41)\n",
      "Zero Force accuracy 0.9623375\n",
      "Dataset size 50000\n",
      "Trial 3\n",
      "(40000, 40) (10000, 40)\n",
      "(40000, 2) (10000, 2)\n",
      "Fitting first model\n",
      "(40000, 43) (10000, 43) (40000, 40) (40000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9687775\n",
      "(10000, 41)\n",
      "Zero Force accuracy 0.962085\n",
      "Dataset size 50000\n",
      "Trial 4\n",
      "(40000, 40) (10000, 40)\n",
      "(40000, 2) (10000, 2)\n",
      "Fitting first model\n",
      "(40000, 43) (10000, 43) (40000, 40) (40000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9682325\n",
      "(10000, 41)\n",
      "Zero Force accuracy 0.9604375\n",
      "Dataset size 50000\n",
      "Trial 5\n",
      "(40000, 40) (10000, 40)\n",
      "(40000, 2) (10000, 2)\n",
      "Fitting first model\n",
      "(40000, 43) (10000, 43) (40000, 40) (40000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9681575\n",
      "(10000, 41)\n",
      "Zero Force accuracy 0.96281\n",
      "Dataset size 50000\n",
      "Trial 6\n",
      "(40000, 40) (10000, 40)\n",
      "(40000, 2) (10000, 2)\n",
      "Fitting first model\n",
      "(40000, 43) (10000, 43) (40000, 40) (40000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96804\n",
      "(10000, 41)\n",
      "Zero Force accuracy 0.96166\n",
      "Dataset size 50000\n",
      "Trial 7\n",
      "(40000, 40) (10000, 40)\n",
      "(40000, 2) (10000, 2)\n",
      "Fitting first model\n",
      "(40000, 43) (10000, 43) (40000, 40) (40000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.966885\n",
      "(10000, 41)\n",
      "Zero Force accuracy 0.959935\n",
      "Dataset size 50000\n",
      "Trial 8\n",
      "(40000, 40) (10000, 40)\n",
      "(40000, 2) (10000, 2)\n",
      "Fitting first model\n",
      "(40000, 43) (10000, 43) (40000, 40) (40000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96783\n",
      "(10000, 41)\n",
      "Zero Force accuracy 0.9607475\n",
      "Dataset size 50000\n",
      "Trial 9\n",
      "(40000, 40) (10000, 40)\n",
      "(40000, 2) (10000, 2)\n",
      "Fitting first model\n",
      "(40000, 43) (10000, 43) (40000, 40) (40000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9680725\n",
      "(10000, 41)\n",
      "Zero Force accuracy 0.96012\n",
      "Dataset size 60000\n",
      "Trial 0\n",
      "(48000, 40) (12000, 40)\n",
      "(48000, 2) (12000, 2)\n",
      "Fitting first model\n",
      "(48000, 43) (12000, 43) (48000, 40) (48000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9701583333333333\n",
      "(12000, 41)\n",
      "Zero Force accuracy 0.9596354166666666\n",
      "Dataset size 60000\n",
      "Trial 1\n",
      "(48000, 40) (12000, 40)\n",
      "(48000, 2) (12000, 2)\n",
      "Fitting first model\n",
      "(48000, 43) (12000, 43) (48000, 40) (48000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9700833333333333\n",
      "(12000, 41)\n",
      "Zero Force accuracy 0.96178125\n",
      "Dataset size 60000\n",
      "Trial 2\n",
      "(48000, 40) (12000, 40)\n",
      "(48000, 2) (12000, 2)\n",
      "Fitting first model\n",
      "(48000, 43) (12000, 43) (48000, 40) (48000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96921875\n",
      "(12000, 41)\n",
      "Zero Force accuracy 0.960825\n",
      "Dataset size 60000\n",
      "Trial 3\n",
      "(48000, 40) (12000, 40)\n",
      "(48000, 2) (12000, 2)\n",
      "Fitting first model\n",
      "(48000, 43) (12000, 43) (48000, 40) (48000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9698520833333333\n",
      "(12000, 41)\n",
      "Zero Force accuracy 0.9607708333333334\n",
      "Dataset size 60000\n",
      "Trial 4\n",
      "(48000, 40) (12000, 40)\n",
      "(48000, 2) (12000, 2)\n",
      "Fitting first model\n",
      "(48000, 43) (12000, 43) (48000, 40) (48000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96995\n",
      "(12000, 41)\n",
      "Zero Force accuracy 0.96141875\n",
      "Dataset size 60000\n",
      "Trial 5\n",
      "(48000, 40) (12000, 40)\n",
      "(48000, 2) (12000, 2)\n",
      "Fitting first model\n",
      "(48000, 43) (12000, 43) (48000, 40) (48000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.96775625\n",
      "(12000, 41)\n",
      "Zero Force accuracy 0.9599520833333334\n",
      "Dataset size 60000\n",
      "Trial 6\n",
      "(48000, 40) (12000, 40)\n",
      "(48000, 2) (12000, 2)\n",
      "Fitting first model\n",
      "(48000, 43) (12000, 43) (48000, 40) (48000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9686125\n",
      "(12000, 41)\n",
      "Zero Force accuracy 0.9618125\n",
      "Dataset size 60000\n",
      "Trial 7\n",
      "(48000, 40) (12000, 40)\n",
      "(48000, 2) (12000, 2)\n",
      "Fitting first model\n",
      "(48000, 43) (12000, 43) (48000, 40) (48000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9695770833333334\n",
      "(12000, 41)\n",
      "Zero Force accuracy 0.9607854166666666\n",
      "Dataset size 60000\n",
      "Trial 8\n",
      "(48000, 40) (12000, 40)\n",
      "(48000, 2) (12000, 2)\n",
      "Fitting first model\n",
      "(48000, 43) (12000, 43) (48000, 40) (48000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9686708333333334\n",
      "(12000, 41)\n",
      "Zero Force accuracy 0.9611166666666666\n",
      "Dataset size 60000\n",
      "Trial 9\n",
      "(48000, 40) (12000, 40)\n",
      "(48000, 2) (12000, 2)\n",
      "Fitting first model\n",
      "(48000, 43) (12000, 43) (48000, 40) (48000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9694833333333334\n",
      "(12000, 41)\n",
      "Zero Force accuracy 0.9612416666666667\n",
      "Dataset size 70000\n",
      "Trial 0\n",
      "(56000, 40) (14000, 40)\n",
      "(56000, 2) (14000, 2)\n",
      "Fitting first model\n",
      "(56000, 43) (14000, 43) (56000, 40) (56000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9702910714285714\n",
      "(14000, 41)\n",
      "Zero Force accuracy 0.96155\n",
      "Dataset size 70000\n",
      "Trial 1\n",
      "(56000, 40) (14000, 40)\n",
      "(56000, 2) (14000, 2)\n",
      "Fitting first model\n",
      "(56000, 43) (14000, 43) (56000, 40) (56000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9690071428571428\n",
      "(14000, 41)\n",
      "Zero Force accuracy 0.9613285714285714\n",
      "Dataset size 70000\n",
      "Trial 2\n",
      "(56000, 40) (14000, 40)\n",
      "(56000, 2) (14000, 2)\n",
      "Fitting first model\n",
      "(56000, 43) (14000, 43) (56000, 40) (56000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9699196428571428\n",
      "(14000, 41)\n",
      "Zero Force accuracy 0.9606892857142857\n",
      "Dataset size 70000\n",
      "Trial 3\n",
      "(56000, 40) (14000, 40)\n",
      "(56000, 2) (14000, 2)\n",
      "Fitting first model\n",
      "(56000, 43) (14000, 43) (56000, 40) (56000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.968675\n",
      "(14000, 41)\n",
      "Zero Force accuracy 0.9603142857142857\n",
      "Dataset size 70000\n",
      "Trial 4\n",
      "(56000, 40) (14000, 40)\n",
      "(56000, 2) (14000, 2)\n",
      "Fitting first model\n",
      "(56000, 43) (14000, 43) (56000, 40) (56000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9708142857142857\n",
      "(14000, 41)\n",
      "Zero Force accuracy 0.9612267857142858\n",
      "Dataset size 70000\n",
      "Trial 5\n",
      "(56000, 40) (14000, 40)\n",
      "(56000, 2) (14000, 2)\n",
      "Fitting first model\n",
      "(56000, 43) (14000, 43) (56000, 40) (56000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9697625\n",
      "(14000, 41)\n",
      "Zero Force accuracy 0.9613303571428572\n",
      "Dataset size 70000\n",
      "Trial 6\n",
      "(56000, 40) (14000, 40)\n",
      "(56000, 2) (14000, 2)\n",
      "Fitting first model\n",
      "(56000, 43) (14000, 43) (56000, 40) (56000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9685071428571429\n",
      "(14000, 41)\n",
      "Zero Force accuracy 0.9611178571428571\n",
      "Dataset size 70000\n",
      "Trial 7\n",
      "(56000, 40) (14000, 40)\n",
      "(56000, 2) (14000, 2)\n",
      "Fitting first model\n",
      "(56000, 43) (14000, 43) (56000, 40) (56000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9683464285714286\n",
      "(14000, 41)\n",
      "Zero Force accuracy 0.9610446428571429\n",
      "Dataset size 70000\n",
      "Trial 8\n",
      "(56000, 40) (14000, 40)\n",
      "(56000, 2) (14000, 2)\n",
      "Fitting first model\n",
      "(56000, 43) (14000, 43) (56000, 40) (56000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9693321428571429\n",
      "(14000, 41)\n",
      "Zero Force accuracy 0.9611446428571429\n",
      "Dataset size 70000\n",
      "Trial 9\n",
      "(56000, 40) (14000, 40)\n",
      "(56000, 2) (14000, 2)\n",
      "Fitting first model\n",
      "(56000, 43) (14000, 43) (56000, 40) (56000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9700571428571428\n",
      "(14000, 41)\n",
      "Zero Force accuracy 0.960075\n",
      "Dataset size 80000\n",
      "Trial 0\n",
      "(64000, 40) (16000, 40)\n",
      "(64000, 2) (16000, 2)\n",
      "Fitting first model\n",
      "(64000, 43) (16000, 43) (64000, 40) (64000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.969975\n",
      "(16000, 41)\n",
      "Zero Force accuracy 0.960490625\n",
      "Dataset size 80000\n",
      "Trial 1\n",
      "(64000, 40) (16000, 40)\n",
      "(64000, 2) (16000, 2)\n",
      "Fitting first model\n",
      "(64000, 43) (16000, 43) (64000, 40) (64000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.969625\n",
      "(16000, 41)\n",
      "Zero Force accuracy 0.961403125\n",
      "Dataset size 80000\n",
      "Trial 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64000, 40) (16000, 40)\n",
      "(64000, 2) (16000, 2)\n",
      "Fitting first model\n",
      "(64000, 43) (16000, 43) (64000, 40) (64000, 40)\n",
      "Fitting Second Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-625f323ad7d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting Second Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmodel_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_X_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_Y_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_Y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mrounded_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "avg_accuracies = {}\n",
    "zeroforce_avg_accuracy = {}\n",
    "for dataset_size in range(10000, 110000, 10000):\n",
    "    avg_accuracies[dataset_size] = []\n",
    "    zeroforce_avg_accuracy[dataset_size] = []\n",
    "for dataset_size in range(10000, 110000, 10000):\n",
    "    for trial in range(10):\n",
    "        print('Dataset size', dataset_size)\n",
    "        print('Trial', trial)\n",
    "        preamble_length = 20\n",
    "        data_length = 40\n",
    "        channel_length = 2\n",
    "        SNR= 10\n",
    "        X, Y = generate_combined_dataset(dataset_size, preamble_length, data_length, channel_length, SNR)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        # print(X)\n",
    "        # print(Y)\n",
    "        preamble_X_train, preamble_X_test = X_train[:, :preamble_length * 2],  X_test[:, :preamble_length * 2]\n",
    "        channel_Y_train, channel_Y_test = Y_train[:, :channel_length], Y_test[:, :channel_length]\n",
    "        print(preamble_X_train.shape, preamble_X_test.shape)\n",
    "        print(channel_Y_train.shape, channel_Y_test.shape)\n",
    "        model_1 = create_model(300, 300, preamble_X_train.shape[1], channel_Y_train.shape[1])\n",
    "        print('Fitting first model')\n",
    "        model_1.fit(preamble_X_train, channel_Y_train, epochs=100, verbose=0)\n",
    "\n",
    "        # predictions = model.predict(preamble_X_test)\n",
    "        # from sklearn.metrics import mean_squared_error\n",
    "        # from sklearn.metrics import mean_absolute_error\n",
    "        # mse = mean_squared_error(predictions, channel_Y_test)\n",
    "        # mabse = mean_absolute_error(predictions, channel_Y_test)\n",
    "        # print('Mean Squared Error: {0}'.format(mse))\n",
    "        # print('Mean Absolute Error: {0}'.format(mabse))\n",
    "        channel_X_train = model_1.predict(preamble_X_train)\n",
    "        channel_X_test = model_1.predict(preamble_X_test)\n",
    "        data_convolved_X_train, data_convolved_X_test = X_train[:, preamble_length * 2:],  X_test[:, preamble_length * 2:]\n",
    "        new_X_train = np.hstack([channel_X_train, data_convolved_X_train])\n",
    "        new_X_test = np.hstack([channel_X_test, data_convolved_X_test])\n",
    "        new_Y_train, new_Y_test = Y_train[:, channel_length:], Y_test[:, channel_length:]\n",
    "        print(new_X_train.shape, new_X_test.shape, new_Y_train.shape, new_Y_train.shape)\n",
    "        print('Fitting Second Model')\n",
    "        model_2 = create_model(300, 300, new_X_train.shape[1], new_Y_train.shape[1])\n",
    "        model_2.fit(new_X_train, new_Y_train, epochs=100, verbose=0)\n",
    "        predictions = model_2.predict(new_X_test)\n",
    "        rounded_predictions = np.where(predictions > 0, 1, -1)\n",
    "        accuracy = calc_accuracy(new_Y_test, rounded_predictions)\n",
    "        avg_accuracies[dataset_size].append(accuracy)\n",
    "        print('NN accuracy', accuracy)\n",
    "        zeroforce_accuracy = test_zf_accuracy(data_convolved_X_test, new_Y_test, channel_Y_test, data_length, channel_length)\n",
    "        print('Zero Force accuracy', zeroforce_accuracy)\n",
    "        zeroforce_avg_accuracy[dataset_size].append(zeroforce_accuracy)\n",
    "print(avg_accuracies)\n",
    "print(zeroforce_avg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10000: [0.96025, 0.957675, 0.9602875, 0.9616625, 0.9605, 0.958, 0.9567625, 0.96165, 0.9585, 0.9591875], 20000: [0.96551875, 0.964625, 0.96509375, 0.9649125, 0.9645125, 0.9653875, 0.96411875, 0.96456875, 0.9651, 0.964625], 30000: [0.96685, 0.9673541666666666, 0.9674708333333333, 0.9664166666666667, 0.9674625, 0.9677791666666666, 0.9659583333333334, 0.9662875, 0.9675791666666667, 0.9667625], 40000: [0.967096875, 0.9677875, 0.968296875, 0.96776875, 0.967840625, 0.96804375, 0.967665625, 0.96806875, 0.96758125, 0.9679375], 50000: [0.9692025, 0.968775, 0.9686575, 0.9687775, 0.9682325, 0.9681575, 0.96804, 0.966885, 0.96783, 0.9680725], 60000: [0.9701583333333333, 0.9700833333333333, 0.96921875, 0.9698520833333333, 0.96995, 0.96775625, 0.9686125, 0.9695770833333334, 0.9686708333333334, 0.9694833333333334], 70000: [0.9702910714285714, 0.9690071428571428, 0.9699196428571428, 0.968675, 0.9708142857142857, 0.9697625, 0.9685071428571429, 0.9683464285714286, 0.9693321428571429, 0.9700571428571428], 80000: [0.969975, 0.969625], 90000: [], 100000: []}\n"
     ]
    }
   ],
   "source": [
    "print(avg_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18fcaf5c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_acc = []\n",
    "zeroforce_acc = []\n",
    "for key in range(10000, 90000, 10000):\n",
    "    avg_nn_acc = sum(avg_accuracies[key]) / len(avg_accuracies[key])\n",
    "    zeroforce_accuracy = sum(zeroforce_avg_accuracy[key]) / len(zeroforce_avg_accuracy[key])\n",
    "    nn_acc.append(avg_nn_acc)\n",
    "    zeroforce_acc.append(zeroforce_accuracy)\n",
    "\n",
    "    \n",
    "plt.plot(range(10000, 90000, 10000),np.log(1 - np.array(nn_acc)), label='NN', marker='o')\n",
    "plt.plot(range(10000, 90000, 10000), np.log(1 - np.array(zeroforce_acc)), label='ZF', marker='o')\n",
    "plt.xlabel('Size dataset')\n",
    "plt.ylabel('log error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_accuracies_SNR = {1: [0.7807125, 0.7806125, 0.7812416666666666, 0.7802791666666666, 0.779, 0.7797333333333333, 0.7789958333333333, 0.7814375, 0.7812875, 0.778425], 2: [0.8084458333333333, 0.8081666666666667, 0.8093541666666667, 0.80965, 0.8071208333333333, 0.8082041666666666, 0.8069458333333334, 0.8083125, 0.8090125, 0.8074916666666667], 3: [0.8364708333333334, 0.8366666666666667, 0.8364791666666667, 0.8343166666666667, 0.835725, 0.8358166666666667, 0.8362708333333333, 0.8354375, 0.8356583333333333, 0.8355083333333333], 4: [0.8616833333333334, 0.8613291666666667, 0.8621041666666667, 0.8611791666666667, 0.863, 0.8620791666666666, 0.8626041666666666, 0.8609291666666666, 0.8619083333333334, 0.8608208333333334], 5: [0.88585, 0.8863958333333334, 0.885925, 0.8858666666666667, 0.8861166666666667, 0.8868166666666667, 0.8866916666666667, 0.8854125, 0.8869541666666667, 0.8877291666666667], 6: [0.9084833333333333, 0.9089, 0.9076791666666667, 0.9096833333333333, 0.9101791666666667, 0.9093625, 0.9080333333333334, 0.9087916666666667, 0.9071833333333333, 0.9078916666666667], 7: [0.9283666666666667, 0.927425, 0.9270875, 0.9282458333333333, 0.9284, 0.9284125, 0.9281541666666666, 0.92765, 0.9275916666666667, 0.9263208333333334], 8: [0.9429791666666667, 0.9445583333333334, 0.9438375, 0.9438833333333333, 0.944325, 0.9436791666666666, 0.9437333333333333, 0.94455, 0.9430291666666667, 0.9423875], 9: [0.9569041666666667, 0.956875, 0.9567958333333333, 0.9568291666666666, 0.956475, 0.9561958333333334, 0.9564166666666667, 0.9571625, 0.9569083333333334, 0.9554416666666666], 10: [0.9674416666666666, 0.9658291666666666, 0.9683375, 0.9678875, 0.96665, 0.9664416666666666, 0.9674791666666667, 0.9674], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR 11\n",
      "Trial 0\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9737\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9676208333333334\n",
      "SNR 11\n",
      "Trial 1\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9749791666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9709125\n",
      "SNR 11\n",
      "Trial 2\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9747666666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9706458333333333\n",
      "SNR 11\n",
      "Trial 3\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9732875\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9687916666666667\n",
      "SNR 11\n",
      "Trial 4\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9744125\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.97\n",
      "SNR 11\n",
      "Trial 5\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9750791666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9705291666666667\n",
      "SNR 11\n",
      "Trial 6\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9742541666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9704291666666667\n",
      "SNR 11\n",
      "Trial 7\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9740208333333333\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.971525\n",
      "SNR 11\n",
      "Trial 8\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9754291666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9711833333333333\n",
      "SNR 11\n",
      "Trial 9\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9753541666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9685166666666667\n",
      "SNR 12\n",
      "Trial 0\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.981575\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9792541666666666\n",
      "SNR 12\n",
      "Trial 1\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9810125\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9780708333333333\n",
      "SNR 12\n",
      "Trial 2\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9808875\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9770833333333333\n",
      "SNR 12\n",
      "Trial 3\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9807791666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.97845\n",
      "SNR 12\n",
      "Trial 4\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9814625\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9767833333333333\n",
      "SNR 12\n",
      "Trial 5\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9806166666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9768583333333334\n",
      "SNR 12\n",
      "Trial 6\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9818916666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9774208333333333\n",
      "SNR 12\n",
      "Trial 7\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9817583333333333\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9772166666666666\n",
      "SNR 12\n",
      "Trial 8\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9808458333333333\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9760916666666667\n",
      "SNR 12\n",
      "Trial 9\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9812541666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9779166666666667\n",
      "SNR 13\n",
      "Trial 0\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.985775\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9845958333333333\n",
      "SNR 13\n",
      "Trial 1\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9841291666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9825125\n",
      "SNR 13\n",
      "Trial 2\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9858916666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9832291666666667\n",
      "SNR 13\n",
      "Trial 3\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9852083333333334\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9845833333333334\n",
      "SNR 13\n",
      "Trial 4\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9863\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9844916666666667\n",
      "SNR 13\n",
      "Trial 5\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9842625\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9822083333333333\n",
      "SNR 13\n",
      "Trial 6\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9849666666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9839583333333334\n",
      "SNR 13\n",
      "Trial 7\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9856083333333333\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9822708333333333\n",
      "SNR 13\n",
      "Trial 8\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9852583333333333\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9832666666666666\n",
      "SNR 13\n",
      "Trial 9\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.985475\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.98395\n",
      "SNR 14\n",
      "Trial 0\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9881625\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9870416666666667\n",
      "SNR 14\n",
      "Trial 1\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.989275\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9876\n",
      "SNR 14\n",
      "Trial 2\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9894375\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9872625\n",
      "SNR 14\n",
      "Trial 3\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9886625\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9881416666666667\n",
      "SNR 14\n",
      "Trial 4\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9875041666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9884416666666667\n",
      "SNR 14\n",
      "Trial 5\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9888875\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9863791666666667\n",
      "SNR 14\n",
      "Trial 6\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9883291666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9872708333333333\n",
      "SNR 14\n",
      "Trial 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.98815\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9877375\n",
      "SNR 14\n",
      "Trial 8\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9889\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9876041666666666\n",
      "SNR 14\n",
      "Trial 9\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9885708333333333\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9881208333333333\n",
      "SNR 15\n",
      "Trial 0\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9917666666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9920541666666667\n",
      "SNR 15\n",
      "Trial 1\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9905875\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9912166666666666\n",
      "SNR 15\n",
      "Trial 2\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.99115\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9913375\n",
      "SNR 15\n",
      "Trial 3\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.990775\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9916708333333333\n",
      "SNR 15\n",
      "Trial 4\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.991275\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9910333333333333\n",
      "SNR 15\n",
      "Trial 5\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9913833333333333\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9908041666666667\n",
      "SNR 15\n",
      "Trial 6\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9914708333333333\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9901583333333334\n",
      "SNR 15\n",
      "Trial 7\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9912958333333334\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9909791666666666\n",
      "SNR 15\n",
      "Trial 8\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9902666666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.99105\n",
      "SNR 15\n",
      "Trial 9\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9919166666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9916625\n",
      "SNR 16\n",
      "Trial 0\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9931291666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.99345\n",
      "SNR 16\n",
      "Trial 1\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9927375\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9941708333333333\n",
      "SNR 16\n",
      "Trial 2\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9933291666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9946958333333333\n",
      "SNR 16\n",
      "Trial 3\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9931125\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9945\n",
      "SNR 16\n",
      "Trial 4\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9922416666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9944708333333333\n",
      "SNR 16\n",
      "Trial 5\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9921791666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9941041666666667\n",
      "SNR 16\n",
      "Trial 6\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.992575\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9941833333333333\n",
      "SNR 16\n",
      "Trial 7\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9923458333333334\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9940541666666667\n",
      "SNR 16\n",
      "Trial 8\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9925666666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9936083333333333\n",
      "SNR 16\n",
      "Trial 9\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9933041666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9942958333333334\n",
      "SNR 17\n",
      "Trial 0\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9951458333333333\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9967\n",
      "SNR 17\n",
      "Trial 1\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9947916666666666\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9961875\n",
      "SNR 17\n",
      "Trial 2\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n",
      "NN accuracy 0.9933291666666667\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9962416666666667\n",
      "SNR 17\n",
      "Trial 3\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "Fitting first model\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "Fitting Second Model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7566058fc8e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fitting Second Model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mmodel_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_X_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_Y_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_Y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mrounded_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "zeroforce_avg_accuracy_SNR = {}\n",
    "for SNR in range(1, 20):\n",
    "    zeroforce_avg_accuracy_SNR[SNR] = []\n",
    "for SNR in range(11, 20):\n",
    "    for trial in range(10):\n",
    "        print('SNR', SNR)\n",
    "        print('Trial', trial)\n",
    "        preamble_length = 20\n",
    "        data_length = 40\n",
    "        channel_length = 2\n",
    "        dataset_size = 30000\n",
    "        X, Y = generate_combined_dataset(dataset_size, preamble_length, data_length, channel_length, SNR)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        # print(X)\n",
    "        # print(Y)\n",
    "        preamble_X_train, preamble_X_test = X_train[:, :preamble_length * 2],  X_test[:, :preamble_length * 2]\n",
    "        channel_Y_train, channel_Y_test = Y_train[:, :channel_length], Y_test[:, :channel_length]\n",
    "        print(preamble_X_train.shape, preamble_X_test.shape)\n",
    "        print(channel_Y_train.shape, channel_Y_test.shape)\n",
    "        model_1 = create_model(300, 300, preamble_X_train.shape[1], channel_Y_train.shape[1])\n",
    "        print('Fitting first model')\n",
    "        model_1.fit(preamble_X_train, channel_Y_train, epochs=100, verbose=0)\n",
    "\n",
    "        # predictions = model.predict(preamble_X_test)\n",
    "        # from sklearn.metrics import mean_squared_error\n",
    "        # from sklearn.metrics import mean_absolute_error\n",
    "        # mse = mean_squared_error(predictions, channel_Y_test)\n",
    "        # mabse = mean_absolute_error(predictions, channel_Y_test)\n",
    "        # print('Mean Squared Error: {0}'.format(mse))\n",
    "        # print('Mean Absolute Error: {0}'.format(mabse))\n",
    "        channel_X_train = model_1.predict(preamble_X_train)\n",
    "        channel_X_test = model_1.predict(preamble_X_test)\n",
    "        data_convolved_X_train, data_convolved_X_test = X_train[:, preamble_length * 2:],  X_test[:, preamble_length * 2:]\n",
    "        new_X_train = np.hstack([channel_X_train, data_convolved_X_train])\n",
    "        new_X_test = np.hstack([channel_X_test, data_convolved_X_test])\n",
    "        new_Y_train, new_Y_test = Y_train[:, channel_length:], Y_test[:, channel_length:]\n",
    "        print(new_X_train.shape, new_X_test.shape, new_Y_train.shape, new_Y_train.shape)\n",
    "        print('Fitting Second Model')\n",
    "        model_2 = create_model(300, 300, new_X_train.shape[1], new_Y_train.shape[1])\n",
    "        model_2.fit(new_X_train, new_Y_train, epochs=100, verbose=0)\n",
    "        predictions = model_2.predict(new_X_test)\n",
    "        rounded_predictions = np.where(predictions > 0, 1, -1)\n",
    "        accuracy = calc_accuracy(new_Y_test, rounded_predictions)\n",
    "        avg_accuracies_SNR[SNR].append(accuracy)\n",
    "        print('NN accuracy', accuracy)\n",
    "        zeroforce_accuracy = test_zf_accuracy(data_convolved_X_test, new_Y_test, channel_Y_test, data_length, channel_length)\n",
    "        print('Zero Force accuracy', zeroforce_accuracy)\n",
    "        zeroforce_avg_accuracy_SNR[SNR].append(zeroforce_accuracy)\n",
    "print(avg_accuracies)\n",
    "print(zeroforce_avg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [0.7807125, 0.7806125, 0.7812416666666666, 0.7802791666666666, 0.779, 0.7797333333333333, 0.7789958333333333, 0.7814375, 0.7812875, 0.778425], 2: [0.8084458333333333, 0.8081666666666667, 0.8093541666666667, 0.80965, 0.8071208333333333, 0.8082041666666666, 0.8069458333333334, 0.8083125, 0.8090125, 0.8074916666666667], 3: [0.8364708333333334, 0.8366666666666667, 0.8364791666666667, 0.8343166666666667, 0.835725, 0.8358166666666667, 0.8362708333333333, 0.8354375, 0.8356583333333333, 0.8355083333333333], 4: [0.8616833333333334, 0.8613291666666667, 0.8621041666666667, 0.8611791666666667, 0.863, 0.8620791666666666, 0.8626041666666666, 0.8609291666666666, 0.8619083333333334, 0.8608208333333334], 5: [0.88585, 0.8863958333333334, 0.885925, 0.8858666666666667, 0.8861166666666667, 0.8868166666666667, 0.8866916666666667, 0.8854125, 0.8869541666666667, 0.8877291666666667], 6: [0.9084833333333333, 0.9089, 0.9076791666666667, 0.9096833333333333, 0.9101791666666667, 0.9093625, 0.9080333333333334, 0.9087916666666667, 0.9071833333333333, 0.9078916666666667], 7: [0.9283666666666667, 0.927425, 0.9270875, 0.9282458333333333, 0.9284, 0.9284125, 0.9281541666666666, 0.92765, 0.9275916666666667, 0.9263208333333334], 8: [0.9429791666666667, 0.9445583333333334, 0.9438375, 0.9438833333333333, 0.944325, 0.9436791666666666, 0.9437333333333333, 0.94455, 0.9430291666666667, 0.9423875], 9: [0.9569041666666667, 0.956875, 0.9567958333333333, 0.9568291666666666, 0.956475, 0.9561958333333334, 0.9564166666666667, 0.9571625, 0.9569083333333334, 0.9554416666666666], 10: [0.9674416666666666, 0.9658291666666666, 0.9683375, 0.9678875, 0.96665, 0.9664416666666666, 0.9674791666666667, 0.9674], 11: [], 12: [], 13: [], 14: [], 15: [], 16: [], 17: [], 18: [], 19: []}\n"
     ]
    }
   ],
   "source": [
    "print(avg_accuracies_SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [0.7807125, 0.7806125, 0.7812416666666666, 0.7802791666666666, 0.779, 0.7797333333333333, 0.7789958333333333, 0.7814375, 0.7812875, 0.778425], 2: [0.8084458333333333, 0.8081666666666667, 0.8093541666666667, 0.80965, 0.8071208333333333, 0.8082041666666666, 0.8069458333333334, 0.8083125, 0.8090125, 0.8074916666666667], 3: [0.8364708333333334, 0.8366666666666667, 0.8364791666666667, 0.8343166666666667, 0.835725, 0.8358166666666667, 0.8362708333333333, 0.8354375, 0.8356583333333333, 0.8355083333333333], 4: [0.8616833333333334, 0.8613291666666667, 0.8621041666666667, 0.8611791666666667, 0.863, 0.8620791666666666, 0.8626041666666666, 0.8609291666666666, 0.8619083333333334, 0.8608208333333334], 5: [0.88585, 0.8863958333333334, 0.885925, 0.8858666666666667, 0.8861166666666667, 0.8868166666666667, 0.8866916666666667, 0.8854125, 0.8869541666666667, 0.8877291666666667], 6: [0.9084833333333333, 0.9089, 0.9076791666666667, 0.9096833333333333, 0.9101791666666667, 0.9093625, 0.9080333333333334, 0.9087916666666667, 0.9071833333333333, 0.9078916666666667], 7: [0.9283666666666667, 0.927425, 0.9270875, 0.9282458333333333, 0.9284, 0.9284125, 0.9281541666666666, 0.92765, 0.9275916666666667, 0.9263208333333334], 8: [0.9429791666666667, 0.9445583333333334, 0.9438375, 0.9438833333333333, 0.944325, 0.9436791666666666, 0.9437333333333333, 0.94455, 0.9430291666666667, 0.9423875], 9: [0.9569041666666667, 0.956875, 0.9567958333333333, 0.9568291666666666, 0.956475, 0.9561958333333334, 0.9564166666666667, 0.9571625, 0.9569083333333334, 0.9554416666666666], 10: [0.9674416666666666, 0.9658291666666666, 0.9683375, 0.9678875, 0.96665, 0.9664416666666666, 0.9674791666666667, 0.9674], 11: [0.9737, 0.9749791666666666, 0.9747666666666667, 0.9732875, 0.9744125, 0.9750791666666667, 0.9742541666666666, 0.9740208333333333, 0.9754291666666667, 0.9753541666666666], 12: [0.981575, 0.9810125, 0.9808875, 0.9807791666666666, 0.9814625, 0.9806166666666667, 0.9818916666666667, 0.9817583333333333, 0.9808458333333333, 0.9812541666666666], 13: [0.985775, 0.9841291666666667, 0.9858916666666667, 0.9852083333333334, 0.9863, 0.9842625, 0.9849666666666667, 0.9856083333333333, 0.9852583333333333, 0.985475], 14: [0.9881625, 0.989275, 0.9894375, 0.9886625, 0.9875041666666666, 0.9888875, 0.9883291666666667, 0.98815, 0.9889, 0.9885708333333333], 15: [0.9917666666666667, 0.9905875, 0.99115, 0.990775, 0.991275, 0.9913833333333333, 0.9914708333333333, 0.9912958333333334, 0.9902666666666666, 0.9919166666666667], 16: [0.9931291666666666, 0.9927375, 0.9933291666666667, 0.9931125, 0.9922416666666667, 0.9921791666666666, 0.992575, 0.9923458333333334, 0.9925666666666667, 0.9933041666666667], 17: [0.9951458333333333, 0.9947916666666666, 0.9933291666666667], 18: [], 19: []}\n",
      "{1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [0.9676208333333334, 0.9709125, 0.9706458333333333, 0.9687916666666667, 0.97, 0.9705291666666667, 0.9704291666666667, 0.971525, 0.9711833333333333, 0.9685166666666667], 12: [0.9792541666666666, 0.9780708333333333, 0.9770833333333333, 0.97845, 0.9767833333333333, 0.9768583333333334, 0.9774208333333333, 0.9772166666666666, 0.9760916666666667, 0.9779166666666667], 13: [0.9845958333333333, 0.9825125, 0.9832291666666667, 0.9845833333333334, 0.9844916666666667, 0.9822083333333333, 0.9839583333333334, 0.9822708333333333, 0.9832666666666666, 0.98395], 14: [0.9870416666666667, 0.9876, 0.9872625, 0.9881416666666667, 0.9884416666666667, 0.9863791666666667, 0.9872708333333333, 0.9877375, 0.9876041666666666, 0.9881208333333333], 15: [0.9920541666666667, 0.9912166666666666, 0.9913375, 0.9916708333333333, 0.9910333333333333, 0.9908041666666667, 0.9901583333333334, 0.9909791666666666, 0.99105, 0.9916625], 16: [0.99345, 0.9941708333333333, 0.9946958333333333, 0.9945, 0.9944708333333333, 0.9941041666666667, 0.9941833333333333, 0.9940541666666667, 0.9936083333333333, 0.9942958333333334], 17: [0.9967, 0.9961875, 0.9962416666666667], 18: [], 19: []}\n"
     ]
    }
   ],
   "source": [
    "print(avg_accuracies_SNR)\n",
    "print(zeroforce_avg_accuracy_SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR 10\n",
      "Trial 0\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9603625\n",
      "SNR 10\n",
      "Trial 1\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9615083333333333\n",
      "SNR 10\n",
      "Trial 2\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9593125\n",
      "SNR 10\n",
      "Trial 3\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9597291666666666\n",
      "SNR 10\n",
      "Trial 4\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9606666666666667\n",
      "SNR 10\n",
      "Trial 5\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9651416666666667\n",
      "SNR 10\n",
      "Trial 6\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9598083333333334\n",
      "SNR 10\n",
      "Trial 7\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.96145\n",
      "SNR 10\n",
      "Trial 8\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9610666666666666\n",
      "SNR 10\n",
      "Trial 9\n",
      "(24000, 40) (6000, 40)\n",
      "(24000, 2) (6000, 2)\n",
      "(24000, 43) (6000, 43) (24000, 40) (24000, 40)\n",
      "(6000, 41)\n",
      "Zero Force accuracy 0.9633958333333333\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'avg_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-35c248a833eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Zero Force accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroforce_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mzeroforce_avg_accuracy_SNR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSNR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzeroforce_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzeroforce_avg_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'avg_accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "# zeroforce_avg_accuracy_SNR = {}\n",
    "# for SNR in range(1, 20):\n",
    "#     zeroforce_avg_accuracy_SNR[SNR] = []\n",
    "for SNR in range(10, 11):\n",
    "    for trial in range(10):\n",
    "        print('SNR', SNR)\n",
    "        print('Trial', trial)\n",
    "        preamble_length = 20\n",
    "        data_length = 40\n",
    "        channel_length = 2\n",
    "        dataset_size = 30000\n",
    "        X, Y = generate_combined_dataset(dataset_size, preamble_length, data_length, channel_length, SNR)\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        # print(X)\n",
    "        # print(Y)\n",
    "        preamble_X_train, preamble_X_test = X_train[:, :preamble_length * 2],  X_test[:, :preamble_length * 2]\n",
    "        channel_Y_train, channel_Y_test = Y_train[:, :channel_length], Y_test[:, :channel_length]\n",
    "        print(preamble_X_train.shape, preamble_X_test.shape)\n",
    "        print(channel_Y_train.shape, channel_Y_test.shape)\n",
    "#         model_1 = create_model(300, 300, preamble_X_train.shape[1], channel_Y_train.shape[1])\n",
    "#         print('Fitting first model')\n",
    "#         model_1.fit(preamble_X_train, channel_Y_train, epochs=100, verbose=0)\n",
    "\n",
    "        # predictions = model.predict(preamble_X_test)\n",
    "        # from sklearn.metrics import mean_squared_error\n",
    "        # from sklearn.metrics import mean_absolute_error\n",
    "        # mse = mean_squared_error(predictions, channel_Y_test)\n",
    "        # mabse = mean_absolute_error(predictions, channel_Y_test)\n",
    "        # print('Mean Squared Error: {0}'.format(mse))\n",
    "        # print('Mean Absolute Error: {0}'.format(mabse))\n",
    "#         channel_X_train = model_1.predict(preamble_X_train)\n",
    "#         channel_X_test = model_1.predict(preamble_X_test)\n",
    "        data_convolved_X_train, data_convolved_X_test = X_train[:, preamble_length * 2:],  X_test[:, preamble_length * 2:]\n",
    "        new_X_train = np.hstack([channel_X_train, data_convolved_X_train])\n",
    "        new_X_test = np.hstack([channel_X_test, data_convolved_X_test])\n",
    "        new_Y_train, new_Y_test = Y_train[:, channel_length:], Y_test[:, channel_length:]\n",
    "        print(new_X_train.shape, new_X_test.shape, new_Y_train.shape, new_Y_train.shape)\n",
    "#         print('Fitting Second Model')\n",
    "#         model_2 = create_model(300, 300, new_X_train.shape[1], new_Y_train.shape[1])\n",
    "#         model_2.fit(new_X_train, new_Y_train, epochs=100, verbose=0)\n",
    "#         predictions = model_2.predict(new_X_test)\n",
    "#         rounded_predictions = np.where(predictions > 0, 1, -1)\n",
    "#         accuracy = calc_accuracy(new_Y_test, rounded_predictions)\n",
    "#         avg_accuracies_SNR[SNR].append(accuracy)\n",
    "#         print('NN accuracy', accuracy)\n",
    "        zeroforce_accuracy = test_zf_accuracy(data_convolved_X_test, new_Y_test, channel_Y_test, data_length, channel_length)\n",
    "        print('Zero Force accuracy', zeroforce_accuracy)\n",
    "        zeroforce_avg_accuracy_SNR[SNR].append(zeroforce_accuracy)\n",
    "print(avg_accuracies)\n",
    "print(zeroforce_avg_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [0.7807125, 0.7806125, 0.7812416666666666, 0.7802791666666666, 0.779, 0.7797333333333333, 0.7789958333333333, 0.7814375, 0.7812875, 0.778425], 2: [0.8084458333333333, 0.8081666666666667, 0.8093541666666667, 0.80965, 0.8071208333333333, 0.8082041666666666, 0.8069458333333334, 0.8083125, 0.8090125, 0.8074916666666667], 3: [0.8364708333333334, 0.8366666666666667, 0.8364791666666667, 0.8343166666666667, 0.835725, 0.8358166666666667, 0.8362708333333333, 0.8354375, 0.8356583333333333, 0.8355083333333333], 4: [0.8616833333333334, 0.8613291666666667, 0.8621041666666667, 0.8611791666666667, 0.863, 0.8620791666666666, 0.8626041666666666, 0.8609291666666666, 0.8619083333333334, 0.8608208333333334], 5: [0.88585, 0.8863958333333334, 0.885925, 0.8858666666666667, 0.8861166666666667, 0.8868166666666667, 0.8866916666666667, 0.8854125, 0.8869541666666667, 0.8877291666666667], 6: [0.9084833333333333, 0.9089, 0.9076791666666667, 0.9096833333333333, 0.9101791666666667, 0.9093625, 0.9080333333333334, 0.9087916666666667, 0.9071833333333333, 0.9078916666666667], 7: [0.9283666666666667, 0.927425, 0.9270875, 0.9282458333333333, 0.9284, 0.9284125, 0.9281541666666666, 0.92765, 0.9275916666666667, 0.9263208333333334], 8: [0.9429791666666667, 0.9445583333333334, 0.9438375, 0.9438833333333333, 0.944325, 0.9436791666666666, 0.9437333333333333, 0.94455, 0.9430291666666667, 0.9423875], 9: [0.9569041666666667, 0.956875, 0.9567958333333333, 0.9568291666666666, 0.956475, 0.9561958333333334, 0.9564166666666667, 0.9571625, 0.9569083333333334, 0.9554416666666666], 10: [0.9674416666666666, 0.9658291666666666, 0.9683375, 0.9678875, 0.96665, 0.9664416666666666, 0.9674791666666667, 0.9674], 11: [0.9737, 0.9749791666666666, 0.9747666666666667, 0.9732875, 0.9744125, 0.9750791666666667, 0.9742541666666666, 0.9740208333333333, 0.9754291666666667, 0.9753541666666666], 12: [0.981575, 0.9810125, 0.9808875, 0.9807791666666666, 0.9814625, 0.9806166666666667, 0.9818916666666667, 0.9817583333333333, 0.9808458333333333, 0.9812541666666666], 13: [0.985775, 0.9841291666666667, 0.9858916666666667, 0.9852083333333334, 0.9863, 0.9842625, 0.9849666666666667, 0.9856083333333333, 0.9852583333333333, 0.985475], 14: [0.9881625, 0.989275, 0.9894375, 0.9886625, 0.9875041666666666, 0.9888875, 0.9883291666666667, 0.98815, 0.9889, 0.9885708333333333], 15: [0.9917666666666667, 0.9905875, 0.99115, 0.990775, 0.991275, 0.9913833333333333, 0.9914708333333333, 0.9912958333333334, 0.9902666666666666, 0.9919166666666667], 16: [0.9931291666666666, 0.9927375, 0.9933291666666667, 0.9931125, 0.9922416666666667, 0.9921791666666666, 0.992575, 0.9923458333333334, 0.9925666666666667, 0.9933041666666667], 17: [0.9951458333333333, 0.9947916666666666, 0.9933291666666667], 18: [], 19: []}\n",
      "{1: [0.785325, 0.7841166666666667, 0.7850083333333333, 0.7847875, 0.7850166666666667, 0.786675, 0.7816125, 0.7854041666666667, 0.7855333333333333, 0.7845125], 2: [0.8107666666666666, 0.8094833333333333, 0.8087333333333333, 0.8090125, 0.8067375, 0.8109791666666667, 0.809725, 0.8082458333333333, 0.8102916666666666, 0.8078666666666666], 3: [0.8346333333333333, 0.833025, 0.8348625, 0.8326583333333333, 0.8320541666666667, 0.8345291666666667, 0.834175, 0.833425, 0.8342375, 0.8338916666666667], 4: [0.85775, 0.8598125, 0.8572708333333333, 0.8585416666666666, 0.8612041666666667, 0.8584875, 0.8612875, 0.8593958333333334, 0.8589166666666667, 0.8598458333333333], 5: [0.8776958333333333, 0.88185, 0.8820708333333334, 0.8809958333333333, 0.8790583333333334, 0.8822875, 0.882025, 0.8843125, 0.8832208333333333, 0.8814791666666667], 6: [0.9032291666666666, 0.9032625, 0.9025333333333333, 0.901825, 0.9003208333333333, 0.9021166666666667, 0.899875, 0.9030791666666667, 0.9005916666666667, 0.9003375], 7: [0.9202916666666666, 0.9203416666666666, 0.9191125, 0.9213583333333333, 0.9227958333333334, 0.9195125, 0.9204291666666666, 0.9199541666666666, 0.9193791666666666, 0.9175666666666666], 8: [0.9376625, 0.9350458333333334, 0.935875, 0.9357291666666666, 0.9380375, 0.9356291666666666, 0.9376083333333334, 0.9379083333333333, 0.9363541666666667, 0.9348625], 9: [0.9499208333333333, 0.9497041666666667, 0.9513875, 0.9504625, 0.9495875, 0.9491625, 0.9510958333333334, 0.9504791666666667, 0.9493041666666666, 0.9503958333333333], 10: [0.9603625, 0.9615083333333333, 0.9593125, 0.9597291666666666, 0.9606666666666667, 0.9651416666666667, 0.9598083333333334, 0.96145, 0.9610666666666666, 0.9633958333333333], 11: [0.9676208333333334, 0.9709125, 0.9706458333333333, 0.9687916666666667, 0.97, 0.9705291666666667, 0.9704291666666667, 0.971525, 0.9711833333333333, 0.9685166666666667], 12: [0.9792541666666666, 0.9780708333333333, 0.9770833333333333, 0.97845, 0.9767833333333333, 0.9768583333333334, 0.9774208333333333, 0.9772166666666666, 0.9760916666666667, 0.9779166666666667], 13: [0.9845958333333333, 0.9825125, 0.9832291666666667, 0.9845833333333334, 0.9844916666666667, 0.9822083333333333, 0.9839583333333334, 0.9822708333333333, 0.9832666666666666, 0.98395], 14: [0.9870416666666667, 0.9876, 0.9872625, 0.9881416666666667, 0.9884416666666667, 0.9863791666666667, 0.9872708333333333, 0.9877375, 0.9876041666666666, 0.9881208333333333], 15: [0.9920541666666667, 0.9912166666666666, 0.9913375, 0.9916708333333333, 0.9910333333333333, 0.9908041666666667, 0.9901583333333334, 0.9909791666666666, 0.99105, 0.9916625], 16: [0.99345, 0.9941708333333333, 0.9946958333333333, 0.9945, 0.9944708333333333, 0.9941041666666667, 0.9941833333333333, 0.9940541666666667, 0.9936083333333333, 0.9942958333333334], 17: [0.9967, 0.9961875, 0.9962416666666667], 18: [], 19: []}\n"
     ]
    }
   ],
   "source": [
    "print(avg_accuracies_SNR)\n",
    "print(zeroforce_avg_accuracy_SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1776c6550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_acc_SNR = []\n",
    "zeroforce_acc_SNR = []\n",
    "for key in range(1, 17):\n",
    "    print(key)\n",
    "    avg_nn_acc = sum(avg_accuracies_SNR[key]) / len(avg_accuracies_SNR[key])\n",
    "    zeroforce_accuracy = sum(zeroforce_avg_accuracy_SNR[key]) / len(zeroforce_avg_accuracy_SNR[key])\n",
    "    nn_acc_SNR.append(avg_nn_acc)\n",
    "    zeroforce_acc_SNR.append(zeroforce_accuracy)\n",
    "\n",
    "    \n",
    "plt.plot(range(1, 17),np.log(1 - np.array(nn_acc_SNR)), label='NN', marker='o')\n",
    "plt.plot(range(1, 17), np.log(1 - np.array(zeroforce_acc_SNR)), label='ZF', marker='o')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('log error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. ... 1. 1. 0.]\n",
      " [0. 1. 1. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 1.]\n",
      " [1. 0. 0. ... 0. 1. 1.]\n",
      " [1. 0. 1. ... 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def create_end_to_end_model(input_layer_dim, hidden_layer_dim, input_dim, output_dim, data_length, channel_length):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_layer_dim, input_dim=input_dim, activation='sigmoid'))\n",
    "    model.add(Dense(hidden_layer_dim, activation='sigmoid'))\n",
    "    model.add(Dense(data_length + channel_length, activation='sigmoid'))\n",
    "    model.add(Dense(hidden_layer_dim, activation='sigmoid'))\n",
    "    model.add(Dense(output_dim, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "dataset_size = 1000000\n",
    "preamble_length = 20\n",
    "data_length = 40\n",
    "channel_length = 2\n",
    "SNR = 10\n",
    "X, Y = generate_data_set(dataset_size, preamble_length, data_length, channel_length, SNR)\n",
    "Y = (Y + 1) / 2\n",
    "print(Y)\n",
    "# print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.3873 - acc: 0.8047   \n",
      "Epoch 2/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.2381 - acc: 0.8917   \n",
      "Epoch 3/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.1835 - acc: 0.9208   \n",
      "Epoch 4/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.1443 - acc: 0.9398   \n",
      "Epoch 5/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.1035 - acc: 0.9587   \n",
      "Epoch 6/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0807 - acc: 0.9687   \n",
      "Epoch 7/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0658 - acc: 0.9750   \n",
      "Epoch 8/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0570 - acc: 0.9786   \n",
      "Epoch 9/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0492 - acc: 0.9818   \n",
      "Epoch 10/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0445 - acc: 0.9837   \n",
      "Epoch 11/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0410 - acc: 0.9850   \n",
      "Epoch 12/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0387 - acc: 0.9859   \n",
      "Epoch 13/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0364 - acc: 0.9868   \n",
      "Epoch 14/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0347 - acc: 0.9874   \n",
      "Epoch 15/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0334 - acc: 0.9879   \n",
      "Epoch 16/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0322 - acc: 0.9883   \n",
      "Epoch 17/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0311 - acc: 0.9887   \n",
      "Epoch 18/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0299 - acc: 0.9892   \n",
      "Epoch 19/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0287 - acc: 0.9896   \n",
      "Epoch 20/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0276 - acc: 0.9900   \n",
      "Epoch 21/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0269 - acc: 0.9903   \n",
      "Epoch 22/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0263 - acc: 0.9905   \n",
      "Epoch 23/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0258 - acc: 0.9907   \n",
      "Epoch 24/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0254 - acc: 0.9908   \n",
      "Epoch 25/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0250 - acc: 0.9910   \n",
      "Epoch 26/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0246 - acc: 0.9911   \n",
      "Epoch 27/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0243 - acc: 0.9912   \n",
      "Epoch 28/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0240 - acc: 0.9913   \n",
      "Epoch 29/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0237 - acc: 0.9914   \n",
      "Epoch 30/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0234 - acc: 0.9915   \n",
      "Epoch 31/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0232 - acc: 0.9916   \n",
      "Epoch 32/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0230 - acc: 0.9917   \n",
      "Epoch 33/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0227 - acc: 0.9918   \n",
      "Epoch 34/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0225 - acc: 0.9918   \n",
      "Epoch 35/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0223 - acc: 0.9919   \n",
      "Epoch 36/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0221 - acc: 0.9920   \n",
      "Epoch 37/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0219 - acc: 0.9921   \n",
      "Epoch 38/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0217 - acc: 0.9921   \n",
      "Epoch 39/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0215 - acc: 0.9922   \n",
      "Epoch 40/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0213 - acc: 0.9923   \n",
      "Epoch 41/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0212 - acc: 0.9923   \n",
      "Epoch 42/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0210 - acc: 0.9924   \n",
      "Epoch 43/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0208 - acc: 0.9925   \n",
      "Epoch 44/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0207 - acc: 0.9925   \n",
      "Epoch 45/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0205 - acc: 0.9926   \n",
      "Epoch 46/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0204 - acc: 0.9927   \n",
      "Epoch 47/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0202 - acc: 0.9927   \n",
      "Epoch 48/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0201 - acc: 0.9928   \n",
      "Epoch 49/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0199 - acc: 0.9928   \n",
      "Epoch 50/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0198 - acc: 0.9929   \n",
      "Epoch 51/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0196 - acc: 0.9929   \n",
      "Epoch 52/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0195 - acc: 0.9930   \n",
      "Epoch 53/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0194 - acc: 0.9930   \n",
      "Epoch 54/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0192 - acc: 0.9931   \n",
      "Epoch 55/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0191 - acc: 0.9931   \n",
      "Epoch 56/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0190 - acc: 0.9932   \n",
      "Epoch 57/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0189 - acc: 0.9932   \n",
      "Epoch 58/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0187 - acc: 0.9933   \n",
      "Epoch 59/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0186 - acc: 0.9933   \n",
      "Epoch 60/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0185 - acc: 0.9934   \n",
      "Epoch 61/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0184 - acc: 0.9935   \n",
      "Epoch 62/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0183 - acc: 0.9935   \n",
      "Epoch 63/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0182 - acc: 0.9935   \n",
      "Epoch 64/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0181 - acc: 0.9936   \n",
      "Epoch 65/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0180 - acc: 0.9936   \n",
      "Epoch 66/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0180 - acc: 0.9937   \n",
      "Epoch 67/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0178 - acc: 0.9937   \n",
      "Epoch 68/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0178 - acc: 0.9937   \n",
      "Epoch 69/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0178 - acc: 0.9937   \n",
      "Epoch 70/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0177 - acc: 0.9938   \n",
      "Epoch 71/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0177 - acc: 0.9938   \n",
      "Epoch 72/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0176 - acc: 0.9938   \n",
      "Epoch 73/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0175 - acc: 0.9939   \n",
      "Epoch 74/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0175 - acc: 0.9939   \n",
      "Epoch 75/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0174 - acc: 0.9939   \n",
      "Epoch 76/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0174 - acc: 0.9939   \n",
      "Epoch 77/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0174 - acc: 0.9939   \n",
      "Epoch 78/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0173 - acc: 0.9940   \n",
      "Epoch 79/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0173 - acc: 0.9940   \n",
      "Epoch 80/300\n",
      "800000/800000 [==============================] - 113s - loss: 0.0173 - acc: 0.9940   \n",
      "Epoch 81/300\n",
      "800000/800000 [==============================] - 121s - loss: 0.0172 - acc: 0.9940   \n",
      "Epoch 82/300\n",
      "800000/800000 [==============================] - 116s - loss: 0.0172 - acc: 0.9940   \n",
      "Epoch 83/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000/800000 [==============================] - 110s - loss: 0.0172 - acc: 0.9940   \n",
      "Epoch 84/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0171 - acc: 0.9941   \n",
      "Epoch 85/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0171 - acc: 0.9941   \n",
      "Epoch 86/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0170 - acc: 0.9941   \n",
      "Epoch 87/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0170 - acc: 0.9941   \n",
      "Epoch 88/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0169 - acc: 0.9941   \n",
      "Epoch 89/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0169 - acc: 0.9941   \n",
      "Epoch 90/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0169 - acc: 0.9941   \n",
      "Epoch 91/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0168 - acc: 0.9942   \n",
      "Epoch 92/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0168 - acc: 0.9941   \n",
      "Epoch 93/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0168 - acc: 0.9942   \n",
      "Epoch 94/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0167 - acc: 0.9942   \n",
      "Epoch 95/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0167 - acc: 0.9942   \n",
      "Epoch 96/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0167 - acc: 0.9942   \n",
      "Epoch 97/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0167 - acc: 0.9942   \n",
      "Epoch 98/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0166 - acc: 0.9942   \n",
      "Epoch 99/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0166 - acc: 0.9942   \n",
      "Epoch 100/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0165 - acc: 0.9942   \n",
      "Epoch 101/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0165 - acc: 0.9942   \n",
      "Epoch 102/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0165 - acc: 0.9943   \n",
      "Epoch 103/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0165 - acc: 0.9943   \n",
      "Epoch 104/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0164 - acc: 0.9943   \n",
      "Epoch 105/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0165 - acc: 0.9943   \n",
      "Epoch 106/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0164 - acc: 0.9943   \n",
      "Epoch 107/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0164 - acc: 0.9943   \n",
      "Epoch 108/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0163 - acc: 0.9943   \n",
      "Epoch 109/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0163 - acc: 0.9943   \n",
      "Epoch 110/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0163 - acc: 0.9943   \n",
      "Epoch 111/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0163 - acc: 0.9943   \n",
      "Epoch 112/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0163 - acc: 0.9943   \n",
      "Epoch 113/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0162 - acc: 0.9944   \n",
      "Epoch 114/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0162 - acc: 0.9943   \n",
      "Epoch 115/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0162 - acc: 0.9944   \n",
      "Epoch 116/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0161 - acc: 0.9944   \n",
      "Epoch 117/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0161 - acc: 0.9944   \n",
      "Epoch 118/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0161 - acc: 0.9944   \n",
      "Epoch 119/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0161 - acc: 0.9944   \n",
      "Epoch 120/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0161 - acc: 0.9944   \n",
      "Epoch 121/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0160 - acc: 0.9944   \n",
      "Epoch 122/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0161 - acc: 0.9944   \n",
      "Epoch 123/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0160 - acc: 0.9944   \n",
      "Epoch 124/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0160 - acc: 0.9944   \n",
      "Epoch 125/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0160 - acc: 0.9944   \n",
      "Epoch 126/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0160 - acc: 0.9944   \n",
      "Epoch 127/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0160 - acc: 0.9944   \n",
      "Epoch 128/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0159 - acc: 0.9944   \n",
      "Epoch 129/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0159 - acc: 0.9944   \n",
      "Epoch 130/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0159 - acc: 0.9944   \n",
      "Epoch 131/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0159 - acc: 0.9944   \n",
      "Epoch 132/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0158 - acc: 0.9945   \n",
      "Epoch 133/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0158 - acc: 0.9944   \n",
      "Epoch 134/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0158 - acc: 0.9945   \n",
      "Epoch 135/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0158 - acc: 0.9945   \n",
      "Epoch 136/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0158 - acc: 0.9945   \n",
      "Epoch 137/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0158 - acc: 0.9944   \n",
      "Epoch 138/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0158 - acc: 0.9944   \n",
      "Epoch 139/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0157 - acc: 0.9945   \n",
      "Epoch 140/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0157 - acc: 0.9945   \n",
      "Epoch 141/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0157 - acc: 0.9945   \n",
      "Epoch 142/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0157 - acc: 0.9945   \n",
      "Epoch 143/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0157 - acc: 0.9945   \n",
      "Epoch 144/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0157 - acc: 0.9945   \n",
      "Epoch 145/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0157 - acc: 0.9945   \n",
      "Epoch 146/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0156 - acc: 0.9945   \n",
      "Epoch 147/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0156 - acc: 0.9945   \n",
      "Epoch 148/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0156 - acc: 0.9945   \n",
      "Epoch 149/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0156 - acc: 0.9945   \n",
      "Epoch 150/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0156 - acc: 0.9945   \n",
      "Epoch 151/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0156 - acc: 0.9945   \n",
      "Epoch 152/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0155 - acc: 0.9945   \n",
      "Epoch 153/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0156 - acc: 0.9945   \n",
      "Epoch 154/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0155 - acc: 0.9945   \n",
      "Epoch 155/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0155 - acc: 0.9945   \n",
      "Epoch 156/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0155 - acc: 0.9945   \n",
      "Epoch 157/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0155 - acc: 0.9945   \n",
      "Epoch 158/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0155 - acc: 0.9945   \n",
      "Epoch 159/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0154 - acc: 0.9946   \n",
      "Epoch 160/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0154 - acc: 0.9946   \n",
      "Epoch 161/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0154 - acc: 0.9946   \n",
      "Epoch 162/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0154 - acc: 0.9945   - \n",
      "Epoch 163/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0154 - acc: 0.9946   \n",
      "Epoch 164/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0154 - acc: 0.9946   \n",
      "Epoch 165/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000/800000 [==============================] - 111s - loss: 0.0154 - acc: 0.9946   \n",
      "Epoch 166/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0154 - acc: 0.9946   \n",
      "Epoch 167/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0154 - acc: 0.9946   \n",
      "Epoch 168/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0154 - acc: 0.9946   \n",
      "Epoch 169/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0153 - acc: 0.9946   \n",
      "Epoch 170/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0153 - acc: 0.9946   \n",
      "Epoch 171/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0153 - acc: 0.9946   \n",
      "Epoch 172/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0153 - acc: 0.9946   \n",
      "Epoch 173/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0153 - acc: 0.9946   \n",
      "Epoch 174/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0153 - acc: 0.9946   \n",
      "Epoch 175/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0153 - acc: 0.9946   \n",
      "Epoch 176/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0153 - acc: 0.9946   \n",
      "Epoch 177/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0152 - acc: 0.9946   \n",
      "Epoch 178/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0153 - acc: 0.9946   \n",
      "Epoch 179/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0152 - acc: 0.9946   \n",
      "Epoch 180/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0152 - acc: 0.9946   \n",
      "Epoch 181/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0152 - acc: 0.9946   \n",
      "Epoch 182/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0152 - acc: 0.9946   \n",
      "Epoch 183/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0152 - acc: 0.9946   \n",
      "Epoch 184/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0152 - acc: 0.9946   \n",
      "Epoch 185/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0151 - acc: 0.9946   \n",
      "Epoch 186/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0151 - acc: 0.9946   \n",
      "Epoch 187/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0152 - acc: 0.9946   \n",
      "Epoch 188/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0151 - acc: 0.9946   \n",
      "Epoch 189/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0151 - acc: 0.9946   \n",
      "Epoch 190/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0151 - acc: 0.9946   \n",
      "Epoch 191/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0151 - acc: 0.9946   \n",
      "Epoch 192/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0151 - acc: 0.9946   \n",
      "Epoch 193/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0151 - acc: 0.9947   \n",
      "Epoch 194/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 195/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0151 - acc: 0.9947   \n",
      "Epoch 196/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0150 - acc: 0.9946   \n",
      "Epoch 197/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 198/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0151 - acc: 0.9946   \n",
      "Epoch 199/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 200/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0150 - acc: 0.9946   \n",
      "Epoch 201/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 202/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 203/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 204/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 205/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 206/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 207/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 208/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 209/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 210/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 211/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0150 - acc: 0.9947   \n",
      "Epoch 212/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0149 - acc: 0.9947   \n",
      "Epoch 213/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0149 - acc: 0.9947   \n",
      "Epoch 214/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0149 - acc: 0.9947   \n",
      "Epoch 215/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0149 - acc: 0.9947   \n",
      "Epoch 216/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0149 - acc: 0.9947   \n",
      "Epoch 217/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0149 - acc: 0.9947   \n",
      "Epoch 218/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0149 - acc: 0.9947   \n",
      "Epoch 219/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0149 - acc: 0.9947   \n",
      "Epoch 220/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 221/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0149 - acc: 0.9947   \n",
      "Epoch 222/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0149 - acc: 0.9947   \n",
      "Epoch 223/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0149 - acc: 0.9947   \n",
      "Epoch 224/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 225/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 226/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 227/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 228/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 229/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 230/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 231/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 232/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 233/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 234/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 235/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9947   \n",
      "Epoch 236/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 237/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9947   \n",
      "Epoch 238/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0148 - acc: 0.9947   \n",
      "Epoch 239/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9947   \n",
      "Epoch 240/300\n",
      "800000/800000 [==============================] - 113s - loss: 0.0147 - acc: 0.9948   \n",
      "Epoch 241/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9948   \n",
      "Epoch 242/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9947   \n",
      "Epoch 243/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9948   \n",
      "Epoch 244/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9947   \n",
      "Epoch 245/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9947   \n",
      "Epoch 246/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9947   \n",
      "Epoch 247/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000/800000 [==============================] - 110s - loss: 0.0147 - acc: 0.9948   \n",
      "Epoch 248/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9948   \n",
      "Epoch 249/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0147 - acc: 0.9948   \n",
      "Epoch 250/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9948   \n",
      "Epoch 251/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 252/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0147 - acc: 0.9948   \n",
      "Epoch 253/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0147 - acc: 0.9948   \n",
      "Epoch 254/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 255/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 256/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 257/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 258/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 259/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 260/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 261/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 262/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 263/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 264/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 265/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 266/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 267/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 268/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0146 - acc: 0.9948   \n",
      "Epoch 269/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 270/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 271/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 272/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 273/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 274/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 275/300\n",
      "800000/800000 [==============================] - 113s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 276/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 277/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 278/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 279/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 280/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 281/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 282/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 283/300\n",
      "800000/800000 [==============================] - 112s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 284/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 285/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 286/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 287/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0144 - acc: 0.9948   \n",
      "Epoch 288/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 289/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0144 - acc: 0.9948   \n",
      "Epoch 290/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0144 - acc: 0.9948   \n",
      "Epoch 291/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0144 - acc: 0.9948   \n",
      "Epoch 292/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0144 - acc: 0.9948   \n",
      "Epoch 293/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0144 - acc: 0.9948   \n",
      "Epoch 294/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0144 - acc: 0.9948   \n",
      "Epoch 295/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0145 - acc: 0.9948   \n",
      "Epoch 296/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0144 - acc: 0.9948   \n",
      "Epoch 297/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0144 - acc: 0.9949   \n",
      "Epoch 298/300\n",
      "800000/800000 [==============================] - 110s - loss: 0.0144 - acc: 0.9948   \n",
      "Epoch 299/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0144 - acc: 0.9949   \n",
      "Epoch 300/300\n",
      "800000/800000 [==============================] - 111s - loss: 0.0144 - acc: 0.9948   \n"
     ]
    }
   ],
   "source": [
    "model = create_end_to_end_model(300, 300, X.shape[1], Y.shape[1], data_length, channel_length)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "model.fit(X_train, Y_train, epochs=300)\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions = np.where(predictions > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02326783504366672"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error((Y_test * 2) - 1, (predictions * 2) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99251675"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(rounded_predictions, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_zf_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_end_to_end_model_mse(input_layer_dim, hidden_layer_dim, input_dim, output_dim, data_length, channel_length):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_layer_dim, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(hidden_layer_dim, activation='relu'))\n",
    "    model.add(Dense(data_length + channel_length, activation='relu'))\n",
    "    model.add(Dense(hidden_layer_dim, activation='relu'))\n",
    "    model.add(Dense(output_dim, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_set_equal(dataset_size, preamble_length, data_length, channel_length, SNR):\n",
    "    input_data_bits = np.random.randint(0,2,(dataset_size, data_length)) \n",
    "    orig_input_data_constellations = input_data_bits * 2 - 1\n",
    "    noise = np.random.normal(0, 0.1, orig_input_data_constellations.shape)\n",
    "    input_data_constellations = orig_input_data_constellations + noise\n",
    "    preambles = np.random.randint(0, 2, (dataset_size, preamble_length))\n",
    "    convolved_preambles = []\n",
    "    convolved_data_constellations = []\n",
    "    \n",
    "    for i in range(dataset_size):\n",
    "        channel_taps = np.random.uniform(0,1,channel_length)\n",
    "        if sum(channel_taps)>=1:\n",
    "            channel_taps = channel_taps / sum(channel_taps)\n",
    "        preamble_conv = add_awgn_noise(sig.convolve(preambles[i], channel_taps, mode='same'), SNR)\n",
    "        convolved_preambles.append(preamble_conv)\n",
    "        constellation_convolved = add_awgn_noise(sig.convolve(input_data_constellations[i], channel_taps, mode='same'), SNR)\n",
    "        convolved_data_constellations.append(constellation_convolved)\n",
    "    X = np.hstack([preambles, np.array(convolved_preambles), np.array(convolved_data_constellations)])\n",
    "    Y = input_data_constellations\n",
    "#     plt.scatter(np.zeros(input_data_constellations.shape[1]), input_data_constellations[0])\n",
    "#     plt.show()\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_data_set_equal(dataset_size, preamble_length, data_length, channel_length, SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/100\n",
      "800000/800000 [==============================] - 142s - loss: 0.4169 - mean_squared_error: 0.4169 - val_loss: 0.3282 - val_mean_squared_error: 0.3282\n",
      "Epoch 2/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.2471 - mean_squared_error: 0.2471 - val_loss: 0.1909 - val_mean_squared_error: 0.1909\n",
      "Epoch 3/100\n",
      "800000/800000 [==============================] - 137s - loss: 0.1653 - mean_squared_error: 0.1653 - val_loss: 0.1467 - val_mean_squared_error: 0.1467\n",
      "Epoch 4/100\n",
      "800000/800000 [==============================] - 135s - loss: 0.1358 - mean_squared_error: 0.1358 - val_loss: 0.1304 - val_mean_squared_error: 0.1304\n",
      "Epoch 5/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.1218 - mean_squared_error: 0.1218 - val_loss: 0.1146 - val_mean_squared_error: 0.1146\n",
      "Epoch 6/100\n",
      "800000/800000 [==============================] - 133s - loss: 0.1140 - mean_squared_error: 0.1140 - val_loss: 0.1093 - val_mean_squared_error: 0.1093\n",
      "Epoch 7/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.1091 - mean_squared_error: 0.1091 - val_loss: 0.1107 - val_mean_squared_error: 0.1107\n",
      "Epoch 8/100\n",
      "800000/800000 [==============================] - 132s - loss: 0.1060 - mean_squared_error: 0.1060 - val_loss: 0.1040 - val_mean_squared_error: 0.1040\n",
      "Epoch 9/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.1035 - mean_squared_error: 0.1035 - val_loss: 0.1056 - val_mean_squared_error: 0.1056\n",
      "Epoch 10/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.1013 - mean_squared_error: 0.1013 - val_loss: 0.0999 - val_mean_squared_error: 0.0999\n",
      "Epoch 11/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.0994 - mean_squared_error: 0.0994 - val_loss: 0.0967 - val_mean_squared_error: 0.0967\n",
      "Epoch 12/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.0980 - mean_squared_error: 0.0980 - val_loss: 0.0968 - val_mean_squared_error: 0.0968\n",
      "Epoch 13/100\n",
      "800000/800000 [==============================] - 125s - loss: 0.0969 - mean_squared_error: 0.0969 - val_loss: 0.0951 - val_mean_squared_error: 0.0951\n",
      "Epoch 14/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.0957 - mean_squared_error: 0.0957 - val_loss: 0.0934 - val_mean_squared_error: 0.0934\n",
      "Epoch 15/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0950 - mean_squared_error: 0.0950 - val_loss: 0.0951 - val_mean_squared_error: 0.0951\n",
      "Epoch 16/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0944 - mean_squared_error: 0.0944 - val_loss: 0.0942 - val_mean_squared_error: 0.0942\n",
      "Epoch 17/100\n",
      "800000/800000 [==============================] - 132s - loss: 0.0938 - mean_squared_error: 0.0938 - val_loss: 0.0950 - val_mean_squared_error: 0.0950\n",
      "Epoch 18/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.0931 - mean_squared_error: 0.0931 - val_loss: 0.0958 - val_mean_squared_error: 0.0958\n",
      "Epoch 19/100\n",
      "800000/800000 [==============================] - 124s - loss: 0.0928 - mean_squared_error: 0.0928 - val_loss: 0.0927 - val_mean_squared_error: 0.0927\n",
      "Epoch 20/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0924 - mean_squared_error: 0.0924 - val_loss: 0.0929 - val_mean_squared_error: 0.0929\n",
      "Epoch 21/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0917 - mean_squared_error: 0.0917 - val_loss: 0.0923 - val_mean_squared_error: 0.0923\n",
      "Epoch 22/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0913 - mean_squared_error: 0.0913 - val_loss: 0.0918 - val_mean_squared_error: 0.0918\n",
      "Epoch 23/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0911 - mean_squared_error: 0.0911 - val_loss: 0.0928 - val_mean_squared_error: 0.0928\n",
      "Epoch 24/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0906 - mean_squared_error: 0.0906 - val_loss: 0.0938 - val_mean_squared_error: 0.0938\n",
      "Epoch 25/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0904 - mean_squared_error: 0.0904 - val_loss: 0.0907 - val_mean_squared_error: 0.0907\n",
      "Epoch 26/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0900 - mean_squared_error: 0.0900 - val_loss: 0.0903 - val_mean_squared_error: 0.0903\n",
      "Epoch 27/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0897 - mean_squared_error: 0.0897 - val_loss: 0.0908 - val_mean_squared_error: 0.0908\n",
      "Epoch 28/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0895 - mean_squared_error: 0.0895 - val_loss: 0.0905 - val_mean_squared_error: 0.0905\n",
      "Epoch 29/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0891 - mean_squared_error: 0.0891 - val_loss: 0.0905 - val_mean_squared_error: 0.0905\n",
      "Epoch 30/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0889 - mean_squared_error: 0.0889 - val_loss: 0.0896 - val_mean_squared_error: 0.0896\n",
      "Epoch 31/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0887 - mean_squared_error: 0.0887 - val_loss: 0.0883 - val_mean_squared_error: 0.0883\n",
      "Epoch 32/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0885 - mean_squared_error: 0.0885 - val_loss: 0.0899 - val_mean_squared_error: 0.0899\n",
      "Epoch 33/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0882 - mean_squared_error: 0.0882 - val_loss: 0.0896 - val_mean_squared_error: 0.0896\n",
      "Epoch 34/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0879 - mean_squared_error: 0.0879 - val_loss: 0.0883 - val_mean_squared_error: 0.0883\n",
      "Epoch 35/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0878 - mean_squared_error: 0.0878 - val_loss: 0.0878 - val_mean_squared_error: 0.0878\n",
      "Epoch 36/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0877 - mean_squared_error: 0.0877 - val_loss: 0.0876 - val_mean_squared_error: 0.0876\n",
      "Epoch 37/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0874 - mean_squared_error: 0.0874 - val_loss: 0.0887 - val_mean_squared_error: 0.0887\n",
      "Epoch 38/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0870 - mean_squared_error: 0.0870 - val_loss: 0.0876 - val_mean_squared_error: 0.0876\n",
      "Epoch 39/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0868 - mean_squared_error: 0.0868 - val_loss: 0.0898 - val_mean_squared_error: 0.0898\n",
      "Epoch 40/100\n",
      "800000/800000 [==============================] - 121s - loss: 0.0867 - mean_squared_error: 0.0867 - val_loss: 0.0895 - val_mean_squared_error: 0.0895\n",
      "Epoch 41/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0865 - mean_squared_error: 0.0865 - val_loss: 0.0903 - val_mean_squared_error: 0.0903\n",
      "Epoch 42/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0865 - mean_squared_error: 0.0865 - val_loss: 0.0865 - val_mean_squared_error: 0.0865\n",
      "Epoch 43/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0860 - mean_squared_error: 0.0860 - val_loss: 0.0872 - val_mean_squared_error: 0.0872\n",
      "Epoch 44/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0856 - mean_squared_error: 0.0856 - val_loss: 0.0879 - val_mean_squared_error: 0.0879\n",
      "Epoch 45/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0851 - mean_squared_error: 0.0851 - val_loss: 0.0872 - val_mean_squared_error: 0.0872\n",
      "Epoch 46/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0847 - mean_squared_error: 0.0847 - val_loss: 0.0851 - val_mean_squared_error: 0.0851\n",
      "Epoch 47/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0843 - mean_squared_error: 0.0843 - val_loss: 0.0849 - val_mean_squared_error: 0.0849\n",
      "Epoch 48/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0843 - mean_squared_error: 0.0843 - val_loss: 0.0852 - val_mean_squared_error: 0.0852\n",
      "Epoch 49/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0841 - mean_squared_error: 0.0841 - val_loss: 0.0840 - val_mean_squared_error: 0.0840\n",
      "Epoch 50/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000/800000 [==============================] - 119s - loss: 0.0839 - mean_squared_error: 0.0839 - val_loss: 0.0873 - val_mean_squared_error: 0.0873\n",
      "Epoch 51/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0835 - mean_squared_error: 0.0835 - val_loss: 0.0829 - val_mean_squared_error: 0.0829\n",
      "Epoch 52/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0834 - mean_squared_error: 0.0834 - val_loss: 0.0847 - val_mean_squared_error: 0.0847\n",
      "Epoch 53/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0831 - mean_squared_error: 0.0831 - val_loss: 0.0862 - val_mean_squared_error: 0.0862\n",
      "Epoch 54/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0829 - mean_squared_error: 0.0829 - val_loss: 0.0861 - val_mean_squared_error: 0.0861\n",
      "Epoch 55/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.0829 - mean_squared_error: 0.0829 - val_loss: 0.0843 - val_mean_squared_error: 0.0843\n",
      "Epoch 56/100\n",
      "800000/800000 [==============================] - 132s - loss: 0.0831 - mean_squared_error: 0.0831 - val_loss: 0.0833 - val_mean_squared_error: 0.0833\n",
      "Epoch 57/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.0828 - mean_squared_error: 0.0828 - val_loss: 0.0843 - val_mean_squared_error: 0.0843\n",
      "Epoch 58/100\n",
      "800000/800000 [==============================] - 132s - loss: 0.0826 - mean_squared_error: 0.0826 - val_loss: 0.0860 - val_mean_squared_error: 0.0860\n",
      "Epoch 59/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.0822 - mean_squared_error: 0.0822 - val_loss: 0.0864 - val_mean_squared_error: 0.0864\n",
      "Epoch 60/100\n",
      "800000/800000 [==============================] - 129s - loss: 0.0829 - mean_squared_error: 0.0829 - val_loss: 0.0833 - val_mean_squared_error: 0.0833\n",
      "Epoch 61/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.0828 - mean_squared_error: 0.0828 - val_loss: 0.0822 - val_mean_squared_error: 0.0822\n",
      "Epoch 62/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.0822 - mean_squared_error: 0.0822 - val_loss: 0.0837 - val_mean_squared_error: 0.0837\n",
      "Epoch 63/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.0822 - mean_squared_error: 0.0822 - val_loss: 0.0841 - val_mean_squared_error: 0.0841\n",
      "Epoch 64/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.0821 - mean_squared_error: 0.0821 - val_loss: 0.0840 - val_mean_squared_error: 0.0840\n",
      "Epoch 65/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.0820 - mean_squared_error: 0.0820 - val_loss: 0.0857 - val_mean_squared_error: 0.0857\n",
      "Epoch 66/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.0820 - mean_squared_error: 0.0820 - val_loss: 0.0855 - val_mean_squared_error: 0.0855\n",
      "Epoch 67/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.0820 - mean_squared_error: 0.0820 - val_loss: 0.0821 - val_mean_squared_error: 0.0821\n",
      "Epoch 68/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.0820 - mean_squared_error: 0.0820 - val_loss: 0.0826 - val_mean_squared_error: 0.0826\n",
      "Epoch 69/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.0818 - mean_squared_error: 0.0818 - val_loss: 0.0831 - val_mean_squared_error: 0.0831\n",
      "Epoch 70/100\n",
      "800000/800000 [==============================] - 129s - loss: 0.0819 - mean_squared_error: 0.0819 - val_loss: 0.0827 - val_mean_squared_error: 0.0827\n",
      "Epoch 71/100\n",
      "800000/800000 [==============================] - 124s - loss: 0.0818 - mean_squared_error: 0.0818 - val_loss: 0.0824 - val_mean_squared_error: 0.0824\n",
      "Epoch 72/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0816 - mean_squared_error: 0.0816 - val_loss: 0.0837 - val_mean_squared_error: 0.0837\n",
      "Epoch 73/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.0816 - mean_squared_error: 0.0816 - val_loss: 0.0806 - val_mean_squared_error: 0.0806\n",
      "Epoch 74/100\n",
      "800000/800000 [==============================] - 135s - loss: 0.0815 - mean_squared_error: 0.0815 - val_loss: 0.0821 - val_mean_squared_error: 0.0821\n",
      "Epoch 75/100\n",
      "800000/800000 [==============================] - 135s - loss: 0.0813 - mean_squared_error: 0.0813 - val_loss: 0.0812 - val_mean_squared_error: 0.0812\n",
      "Epoch 76/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.0813 - mean_squared_error: 0.0813 - val_loss: 0.0840 - val_mean_squared_error: 0.0840\n",
      "Epoch 77/100\n",
      "800000/800000 [==============================] - 137s - loss: 0.0811 - mean_squared_error: 0.0811 - val_loss: 0.0840 - val_mean_squared_error: 0.0840\n",
      "Epoch 78/100\n",
      "800000/800000 [==============================] - 132s - loss: 0.0811 - mean_squared_error: 0.0811 - val_loss: 0.0830 - val_mean_squared_error: 0.0830\n",
      "Epoch 79/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.0810 - mean_squared_error: 0.0810 - val_loss: 0.0823 - val_mean_squared_error: 0.0823\n",
      "Epoch 80/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.0807 - mean_squared_error: 0.0807 - val_loss: 0.0813 - val_mean_squared_error: 0.0813\n",
      "Epoch 81/100\n",
      "800000/800000 [==============================] - 132s - loss: 0.0804 - mean_squared_error: 0.0804 - val_loss: 0.0840 - val_mean_squared_error: 0.0840\n",
      "Epoch 82/100\n",
      "800000/800000 [==============================] - 130s - loss: 0.0804 - mean_squared_error: 0.0804 - val_loss: 0.0804 - val_mean_squared_error: 0.0804\n",
      "Epoch 83/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.0803 - mean_squared_error: 0.0803 - val_loss: 0.0813 - val_mean_squared_error: 0.0813\n",
      "Epoch 84/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.0802 - mean_squared_error: 0.0802 - val_loss: 0.0818 - val_mean_squared_error: 0.0818\n",
      "Epoch 85/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0801 - mean_squared_error: 0.0801 - val_loss: 0.0805 - val_mean_squared_error: 0.0805\n",
      "Epoch 86/100\n",
      "800000/800000 [==============================] - 125s - loss: 0.0799 - mean_squared_error: 0.0799 - val_loss: 0.0803 - val_mean_squared_error: 0.0803\n",
      "Epoch 87/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0798 - mean_squared_error: 0.0798 - val_loss: 0.0806 - val_mean_squared_error: 0.0806\n",
      "Epoch 88/100\n",
      "800000/800000 [==============================] - 129s - loss: 0.0799 - mean_squared_error: 0.0799 - val_loss: 0.0794 - val_mean_squared_error: 0.0794\n",
      "Epoch 89/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0798 - mean_squared_error: 0.0798 - val_loss: 0.0842 - val_mean_squared_error: 0.0842\n",
      "Epoch 90/100\n",
      "800000/800000 [==============================] - 124s - loss: 0.0796 - mean_squared_error: 0.0796 - val_loss: 0.0828 - val_mean_squared_error: 0.0828\n",
      "Epoch 91/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0797 - mean_squared_error: 0.0797 - val_loss: 0.0847 - val_mean_squared_error: 0.0847\n",
      "Epoch 92/100\n",
      "800000/800000 [==============================] - 130s - loss: 0.0794 - mean_squared_error: 0.0794 - val_loss: 0.0817 - val_mean_squared_error: 0.0817\n",
      "Epoch 93/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0796 - mean_squared_error: 0.0796 - val_loss: 0.0810 - val_mean_squared_error: 0.0810\n",
      "Epoch 94/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0794 - mean_squared_error: 0.0794 - val_loss: 0.0814 - val_mean_squared_error: 0.0814\n",
      "Epoch 95/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.0795 - mean_squared_error: 0.0795 - val_loss: 0.0791 - val_mean_squared_error: 0.0791\n",
      "Epoch 96/100\n",
      "800000/800000 [==============================] - 124s - loss: 0.0795 - mean_squared_error: 0.0795 - val_loss: 0.0829 - val_mean_squared_error: 0.0829\n",
      "Epoch 97/100\n",
      "800000/800000 [==============================] - 125s - loss: 0.0794 - mean_squared_error: 0.0794 - val_loss: 0.0796 - val_mean_squared_error: 0.0796\n",
      "Epoch 98/100\n",
      "800000/800000 [==============================] - 129s - loss: 0.0793 - mean_squared_error: 0.0793 - val_loss: 0.0795 - val_mean_squared_error: 0.0795\n",
      "Epoch 99/100\n",
      "800000/800000 [==============================] - 136s - loss: 0.0793 - mean_squared_error: 0.0793 - val_loss: 0.0850 - val_mean_squared_error: 0.0850\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000/800000 [==============================] - 126s - loss: 0.0793 - mean_squared_error: 0.0793 - val_loss: 0.0823 - val_mean_squared_error: 0.0823\n"
     ]
    }
   ],
   "source": [
    "model = create_end_to_end_model_mse(300, 300, X.shape[1], Y.shape[1], data_length, channel_length)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test))\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98149675"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(Y_test)\n",
    "rounded_predictions = np.where(predictions > 0, 1, -1)\n",
    "Y_test_rounded = np.where(Y_test > 0, 1, -1)\n",
    "calc_accuracy(rounded_predictions, Y_test_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import commpy as cp\n",
    "def generate_combined_dataset_qam_4(dataset_size,  preamble_length, data_length, channel_length, SNR, constellations):\n",
    "    QAMModem = cp.modulation.QAMModem(constellations)\n",
    "    \n",
    "    # use the constellation object to modulate the data bits into complex numbers\n",
    "    \n",
    "    \n",
    "    preambles_constellations = []\n",
    "    input_data_constellations = []\n",
    "    convolved_preambles = []\n",
    "    convolved_data_constellations = []\n",
    "    \n",
    "    for i in range(dataset_size):\n",
    "        input_data_bits = np.random.randint(0, 2, data_length)\n",
    "        preambles = np.random.randint(0, 2, preamble_length)\n",
    "        \n",
    "        input_data_constellation = QAMModem.modulate(input_data_bits)\n",
    "        real_input = np.real(input_data_constellation)\n",
    "        im_input = np.imag(input_data_constellation)\n",
    "        real_im_input = np.vstack((real_input,im_input)).reshape((-1,),order='F')\n",
    "        input_data_constellations.append(real_im_input)\n",
    "        \n",
    "        preambles_constellation =  QAMModem.modulate(preambles)\n",
    "        real_preambles = np.real(preambles_constellation)\n",
    "        im_preambles = np.imag(preambles_constellation)\n",
    "        real_im_preambles = np.vstack((real_preambles,im_preambles)).reshape((-1,),order='F')\n",
    "        preambles_constellations.append(real_im_preambles)\n",
    "        \n",
    "        channel_taps = np.random.uniform(0,1,channel_length)\n",
    "        if sum(channel_taps) >= 1:\n",
    "            channel_taps = channel_taps / sum(channel_taps)\n",
    "        preamble_conv = sig.convolve(preambles_constellation, channel_taps, mode='same')\n",
    "        real_preambles_conv = np.real(preamble_conv)\n",
    "        im_preambles_conv = np.imag(preamble_conv)\n",
    "        real_im_preambles_conv = np.vstack((real_preambles_conv,im_preambles_conv)).reshape((-1,),order='F')\n",
    "        convolved_preambles.append(real_im_preambles_conv)\n",
    "        \n",
    "        constellation_convolved = add_awgn_noise(sig.convolve(input_data_constellation, channel_taps, mode='same'), SNR)\n",
    "        real_constellation_conv = np.real(constellation_convolved)\n",
    "        im_constellation_conv = np.imag(constellation_convolved)\n",
    "        real_im_constellation_conv = np.vstack((real_constellation_conv,im_constellation_conv)).reshape((-1,),order='F')\n",
    "        convolved_data_constellations.append(real_im_constellation_conv)\n",
    "        \n",
    "    X1 = np.hstack([preambles_constellations, np.array(convolved_preambles), np.array(convolved_data_constellations)])\n",
    "    Y = np.array(input_data_constellations)\n",
    "#     plt.scatter(np.zeros(input_data_constellations.shape[1]), input_data_constellations[0])\n",
    "#     plt.show()\n",
    "    return X1, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800000, 40) (800000, 20)\n",
      "(200000, 40) (200000, 20)\n"
     ]
    }
   ],
   "source": [
    "X, Y = generate_combined_dataset_qam_4(1000000, 20, 40, 2, 10, 16)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)\n",
    "# model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test))\n",
    "# predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/100\n",
      "800000/800000 [==============================] - 126s - loss: 1.1419 - mean_squared_error: 1.1419 - val_loss: 0.9330 - val_mean_squared_error: 0.9330\n",
      "Epoch 2/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.9093 - mean_squared_error: 0.9093 - val_loss: 0.8912 - val_mean_squared_error: 0.8912\n",
      "Epoch 3/100\n",
      "800000/800000 [==============================] - 123s - loss: 0.8793 - mean_squared_error: 0.8793 - val_loss: 0.8643 - val_mean_squared_error: 0.8643\n",
      "Epoch 4/100\n",
      "800000/800000 [==============================] - 129s - loss: 0.8652 - mean_squared_error: 0.8652 - val_loss: 0.8626 - val_mean_squared_error: 0.8626\n",
      "Epoch 5/100\n",
      "800000/800000 [==============================] - 125s - loss: 0.8556 - mean_squared_error: 0.8556 - val_loss: 0.8592 - val_mean_squared_error: 0.8592\n",
      "Epoch 6/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.8488 - mean_squared_error: 0.8488 - val_loss: 0.8402 - val_mean_squared_error: 0.8402\n",
      "Epoch 7/100\n",
      "800000/800000 [==============================] - 132s - loss: 0.8438 - mean_squared_error: 0.8438 - val_loss: 0.8398 - val_mean_squared_error: 0.8398\n",
      "Epoch 8/100\n",
      "800000/800000 [==============================] - 133s - loss: 0.8396 - mean_squared_error: 0.8396 - val_loss: 0.8295 - val_mean_squared_error: 0.8295\n",
      "Epoch 9/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.8365 - mean_squared_error: 0.8365 - val_loss: 0.8398 - val_mean_squared_error: 0.8398\n",
      "Epoch 10/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.8337 - mean_squared_error: 0.8337 - val_loss: 0.8335 - val_mean_squared_error: 0.8335\n",
      "Epoch 11/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.8312 - mean_squared_error: 0.8312 - val_loss: 0.8373 - val_mean_squared_error: 0.8373\n",
      "Epoch 12/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.8291 - mean_squared_error: 0.8291 - val_loss: 0.8374 - val_mean_squared_error: 0.8374\n",
      "Epoch 13/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.8268 - mean_squared_error: 0.8268 - val_loss: 0.8307 - val_mean_squared_error: 0.8307\n",
      "Epoch 14/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.8252 - mean_squared_error: 0.8252 - val_loss: 0.8348 - val_mean_squared_error: 0.8348\n",
      "Epoch 15/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.8241 - mean_squared_error: 0.8241 - val_loss: 0.8160 - val_mean_squared_error: 0.8160\n",
      "Epoch 16/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.8228 - mean_squared_error: 0.8228 - val_loss: 0.8300 - val_mean_squared_error: 0.8300\n",
      "Epoch 17/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.8210 - mean_squared_error: 0.8210 - val_loss: 0.8255 - val_mean_squared_error: 0.8255\n",
      "Epoch 18/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.8203 - mean_squared_error: 0.8203 - val_loss: 0.8388 - val_mean_squared_error: 0.8388\n",
      "Epoch 19/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.8189 - mean_squared_error: 0.8189 - val_loss: 0.8210 - val_mean_squared_error: 0.8210\n",
      "Epoch 20/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.8179 - mean_squared_error: 0.8179 - val_loss: 0.8214 - val_mean_squared_error: 0.8214\n",
      "Epoch 21/100\n",
      "800000/800000 [==============================] - 135s - loss: 0.8172 - mean_squared_error: 0.8172 - val_loss: 0.8229 - val_mean_squared_error: 0.8229\n",
      "Epoch 22/100\n",
      "800000/800000 [==============================] - 133s - loss: 0.8167 - mean_squared_error: 0.8167 - val_loss: 0.8170 - val_mean_squared_error: 0.8170\n",
      "Epoch 23/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.8161 - mean_squared_error: 0.8161 - val_loss: 0.8211 - val_mean_squared_error: 0.8211\n",
      "Epoch 24/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.8152 - mean_squared_error: 0.8152 - val_loss: 0.8075 - val_mean_squared_error: 0.8075\n",
      "Epoch 25/100\n",
      "800000/800000 [==============================] - 132s - loss: 0.8149 - mean_squared_error: 0.8149 - val_loss: 0.8296 - val_mean_squared_error: 0.8296\n",
      "Epoch 26/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.8143 - mean_squared_error: 0.8143 - val_loss: 0.8128 - val_mean_squared_error: 0.8128\n",
      "Epoch 27/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.8137 - mean_squared_error: 0.8137 - val_loss: 0.8249 - val_mean_squared_error: 0.8249\n",
      "Epoch 28/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.8134 - mean_squared_error: 0.8134 - val_loss: 0.8301 - val_mean_squared_error: 0.8301\n",
      "Epoch 29/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.8129 - mean_squared_error: 0.8129 - val_loss: 0.8125 - val_mean_squared_error: 0.8125\n",
      "Epoch 30/100\n",
      "800000/800000 [==============================] - 129s - loss: 0.8122 - mean_squared_error: 0.8122 - val_loss: 0.8116 - val_mean_squared_error: 0.8116\n",
      "Epoch 31/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.8119 - mean_squared_error: 0.8119 - val_loss: 0.8123 - val_mean_squared_error: 0.8123\n",
      "Epoch 32/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.8117 - mean_squared_error: 0.8117 - val_loss: 0.8082 - val_mean_squared_error: 0.8082\n",
      "Epoch 33/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.8108 - mean_squared_error: 0.8108 - val_loss: 0.8175 - val_mean_squared_error: 0.8175\n",
      "Epoch 34/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.8105 - mean_squared_error: 0.8105 - val_loss: 0.8127 - val_mean_squared_error: 0.8127\n",
      "Epoch 35/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.8100 - mean_squared_error: 0.8100 - val_loss: 0.8175 - val_mean_squared_error: 0.8175\n",
      "Epoch 36/100\n",
      "800000/800000 [==============================] - 129s - loss: 0.8094 - mean_squared_error: 0.8094 - val_loss: 0.8110 - val_mean_squared_error: 0.8110\n",
      "Epoch 37/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.8093 - mean_squared_error: 0.8093 - val_loss: 0.8072 - val_mean_squared_error: 0.8072\n",
      "Epoch 38/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.8091 - mean_squared_error: 0.8091 - val_loss: 0.8131 - val_mean_squared_error: 0.8131\n",
      "Epoch 39/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.8089 - mean_squared_error: 0.8089 - val_loss: 0.8079 - val_mean_squared_error: 0.8079\n",
      "Epoch 40/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.8085 - mean_squared_error: 0.8085 - val_loss: 0.8081 - val_mean_squared_error: 0.8081\n",
      "Epoch 41/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.8085 - mean_squared_error: 0.8085 - val_loss: 0.8105 - val_mean_squared_error: 0.8105\n",
      "Epoch 42/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.8080 - mean_squared_error: 0.8080 - val_loss: 0.8114 - val_mean_squared_error: 0.8114\n",
      "Epoch 43/100\n",
      "800000/800000 [==============================] - 129s - loss: 0.8078 - mean_squared_error: 0.8078 - val_loss: 0.8114 - val_mean_squared_error: 0.8114\n",
      "Epoch 44/100\n",
      "800000/800000 [==============================] - 130s - loss: 0.8075 - mean_squared_error: 0.8075 - val_loss: 0.8165 - val_mean_squared_error: 0.8165\n",
      "Epoch 45/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.8074 - mean_squared_error: 0.8074 - val_loss: 0.8119 - val_mean_squared_error: 0.8119\n",
      "Epoch 46/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.8070 - mean_squared_error: 0.8070 - val_loss: 0.8085 - val_mean_squared_error: 0.8085\n",
      "Epoch 47/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.8064 - mean_squared_error: 0.8064 - val_loss: 0.8153 - val_mean_squared_error: 0.8153\n",
      "Epoch 48/100\n",
      "800000/800000 [==============================] - 130s - loss: 0.8063 - mean_squared_error: 0.8063 - val_loss: 0.8180 - val_mean_squared_error: 0.8180\n",
      "Epoch 49/100\n",
      "800000/800000 [==============================] - 133s - loss: 0.8062 - mean_squared_error: 0.8062 - val_loss: 0.8068 - val_mean_squared_error: 0.8068\n",
      "Epoch 50/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.8059 - mean_squared_error: 0.8059 - val_loss: 0.8087 - val_mean_squared_error: 0.8087\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000/800000 [==============================] - 129s - loss: 0.8059 - mean_squared_error: 0.8059 - val_loss: 0.8073 - val_mean_squared_error: 0.8073\n",
      "Epoch 52/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.8056 - mean_squared_error: 0.8056 - val_loss: 0.8093 - val_mean_squared_error: 0.8093\n",
      "Epoch 53/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.8055 - mean_squared_error: 0.8055 - val_loss: 0.8119 - val_mean_squared_error: 0.8119\n",
      "Epoch 54/100\n",
      "800000/800000 [==============================] - 132s - loss: 0.8054 - mean_squared_error: 0.8054 - val_loss: 0.8044 - val_mean_squared_error: 0.8044\n",
      "Epoch 55/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.8046 - mean_squared_error: 0.8046 - val_loss: 0.8096 - val_mean_squared_error: 0.8096\n",
      "Epoch 56/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.8044 - mean_squared_error: 0.8044 - val_loss: 0.8099 - val_mean_squared_error: 0.8099\n",
      "Epoch 57/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.8039 - mean_squared_error: 0.8039 - val_loss: 0.8039 - val_mean_squared_error: 0.8039\n",
      "Epoch 58/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.8031 - mean_squared_error: 0.8031 - val_loss: 0.8078 - val_mean_squared_error: 0.8078\n",
      "Epoch 59/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.8025 - mean_squared_error: 0.8025 - val_loss: 0.8143 - val_mean_squared_error: 0.8143\n",
      "Epoch 60/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.8026 - mean_squared_error: 0.8026 - val_loss: 0.8102 - val_mean_squared_error: 0.8102\n",
      "Epoch 61/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.8024 - mean_squared_error: 0.8024 - val_loss: 0.8195 - val_mean_squared_error: 0.8195\n",
      "Epoch 62/100\n",
      "800000/800000 [==============================] - 125s - loss: 0.8019 - mean_squared_error: 0.8019 - val_loss: 0.8140 - val_mean_squared_error: 0.8140\n",
      "Epoch 63/100\n",
      "800000/800000 [==============================] - 129s - loss: 0.8016 - mean_squared_error: 0.8016 - val_loss: 0.8155 - val_mean_squared_error: 0.8155\n",
      "Epoch 64/100\n",
      "800000/800000 [==============================] - 135s - loss: 0.8016 - mean_squared_error: 0.8016 - val_loss: 0.8096 - val_mean_squared_error: 0.8096\n",
      "Epoch 65/100\n",
      "800000/800000 [==============================] - 133s - loss: 0.8014 - mean_squared_error: 0.8014 - val_loss: 0.8051 - val_mean_squared_error: 0.8051\n",
      "Epoch 66/100\n",
      "800000/800000 [==============================] - 137s - loss: 0.8009 - mean_squared_error: 0.8009 - val_loss: 0.8188 - val_mean_squared_error: 0.8188\n",
      "Epoch 67/100\n",
      "800000/800000 [==============================] - 136s - loss: 0.8011 - mean_squared_error: 0.8011 - val_loss: 0.8123 - val_mean_squared_error: 0.8123\n",
      "Epoch 68/100\n",
      "800000/800000 [==============================] - 136s - loss: 0.8006 - mean_squared_error: 0.8006 - val_loss: 0.8124 - val_mean_squared_error: 0.8124\n",
      "Epoch 69/100\n",
      "800000/800000 [==============================] - 130s - loss: 0.8006 - mean_squared_error: 0.8006 - val_loss: 0.8114 - val_mean_squared_error: 0.8114\n",
      "Epoch 70/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.8003 - mean_squared_error: 0.8003 - val_loss: 0.8088 - val_mean_squared_error: 0.8088\n",
      "Epoch 71/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.8002 - mean_squared_error: 0.8002 - val_loss: 0.7980 - val_mean_squared_error: 0.7980\n",
      "Epoch 72/100\n",
      "800000/800000 [==============================] - 130s - loss: 0.8000 - mean_squared_error: 0.8000 - val_loss: 0.7951 - val_mean_squared_error: 0.7951\n",
      "Epoch 73/100\n",
      "800000/800000 [==============================] - 130s - loss: 0.7999 - mean_squared_error: 0.7999 - val_loss: 0.8196 - val_mean_squared_error: 0.8196\n",
      "Epoch 74/100\n",
      "800000/800000 [==============================] - 128s - loss: 0.7998 - mean_squared_error: 0.7998 - val_loss: 0.7989 - val_mean_squared_error: 0.7989\n",
      "Epoch 75/100\n",
      "800000/800000 [==============================] - 129s - loss: 0.7998 - mean_squared_error: 0.7998 - val_loss: 0.8084 - val_mean_squared_error: 0.8084\n",
      "Epoch 76/100\n",
      "800000/800000 [==============================] - 129s - loss: 0.7996 - mean_squared_error: 0.7996 - val_loss: 0.8042 - val_mean_squared_error: 0.8042\n",
      "Epoch 77/100\n",
      "800000/800000 [==============================] - 131s - loss: 0.7993 - mean_squared_error: 0.7993 - val_loss: 0.8009 - val_mean_squared_error: 0.8009\n",
      "Epoch 78/100\n",
      "800000/800000 [==============================] - 130s - loss: 0.7993 - mean_squared_error: 0.7993 - val_loss: 0.8063 - val_mean_squared_error: 0.8063\n",
      "Epoch 79/100\n",
      "800000/800000 [==============================] - 135s - loss: 0.7991 - mean_squared_error: 0.7991 - val_loss: 0.8087 - val_mean_squared_error: 0.8087\n",
      "Epoch 80/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7990 - mean_squared_error: 0.7990 - val_loss: 0.8010 - val_mean_squared_error: 0.8010\n",
      "Epoch 81/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7990 - mean_squared_error: 0.7990 - val_loss: 0.8157 - val_mean_squared_error: 0.8157\n",
      "Epoch 82/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7991 - mean_squared_error: 0.7991 - val_loss: 0.8237 - val_mean_squared_error: 0.8237\n",
      "Epoch 83/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7985 - mean_squared_error: 0.7985 - val_loss: 0.7981 - val_mean_squared_error: 0.7981\n",
      "Epoch 84/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7986 - mean_squared_error: 0.7986 - val_loss: 0.8086 - val_mean_squared_error: 0.8086\n",
      "Epoch 85/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7985 - mean_squared_error: 0.7985 - val_loss: 0.7997 - val_mean_squared_error: 0.7997\n",
      "Epoch 86/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7986 - mean_squared_error: 0.7986 - val_loss: 0.8043 - val_mean_squared_error: 0.8043\n",
      "Epoch 87/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7983 - mean_squared_error: 0.7983 - val_loss: 0.8016 - val_mean_squared_error: 0.8016\n",
      "Epoch 88/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7980 - mean_squared_error: 0.7980 - val_loss: 0.7996 - val_mean_squared_error: 0.7996\n",
      "Epoch 89/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7983 - mean_squared_error: 0.7983 - val_loss: 0.8022 - val_mean_squared_error: 0.8022\n",
      "Epoch 90/100\n",
      "800000/800000 [==============================] - 133s - loss: 0.7980 - mean_squared_error: 0.7980 - val_loss: 0.8022 - val_mean_squared_error: 0.8022\n",
      "Epoch 91/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7977 - mean_squared_error: 0.7977 - val_loss: 0.8031 - val_mean_squared_error: 0.8031\n",
      "Epoch 92/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7979 - mean_squared_error: 0.7979 - val_loss: 0.8007 - val_mean_squared_error: 0.8007\n",
      "Epoch 93/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7977 - mean_squared_error: 0.7977 - val_loss: 0.8090 - val_mean_squared_error: 0.8090\n",
      "Epoch 94/100\n",
      "800000/800000 [==============================] - 135s - loss: 0.7977 - mean_squared_error: 0.7977 - val_loss: 0.8091 - val_mean_squared_error: 0.8091\n",
      "Epoch 95/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7977 - mean_squared_error: 0.7977 - val_loss: 0.7966 - val_mean_squared_error: 0.7966\n",
      "Epoch 96/100\n",
      "800000/800000 [==============================] - 133s - loss: 0.7978 - mean_squared_error: 0.7978 - val_loss: 0.8037 - val_mean_squared_error: 0.8037\n",
      "Epoch 97/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7974 - mean_squared_error: 0.7974 - val_loss: 0.8077 - val_mean_squared_error: 0.8077\n",
      "Epoch 98/100\n",
      "800000/800000 [==============================] - 135s - loss: 0.7975 - mean_squared_error: 0.7975 - val_loss: 0.8025 - val_mean_squared_error: 0.8025\n",
      "Epoch 99/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7974 - mean_squared_error: 0.7974 - val_loss: 0.8080 - val_mean_squared_error: 0.8080\n",
      "Epoch 100/100\n",
      "800000/800000 [==============================] - 134s - loss: 0.7974 - mean_squared_error: 0.7974 - val_loss: 0.7975 - val_mean_squared_error: 0.7975\n"
     ]
    }
   ],
   "source": [
    "model = create_end_to_end_model_mse(300, 300, X.shape[1], Y.shape[1], 40, 2)\n",
    "model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test))\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_class(predictions):\n",
    "    class_predictions = np.zeros((predictions.shape[0], predictions.shape[1]))\n",
    "    for i in range(predictions.shape[0]):\n",
    "        for j in range(predictions.shape[1]):\n",
    "            elem = predictions[i][j]\n",
    "            if elem > 2:\n",
    "                class_predictions[i][j] = 3\n",
    "            elif elem > 0:\n",
    "                class_predictions[i][j] = 1\n",
    "            elif elem > -2:\n",
    "                class_predictions[i][j] = -1\n",
    "            else:\n",
    "                class_predictions[i][j] = -3\n",
    "    return class_predictions\n",
    "\n",
    "class_predictions = convert_to_class(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3. -1.  3. ...  3.  3. -1.]\n",
      " [-1.  1. -3. ...  3.  3.  1.]\n",
      " [ 1. -3.  1. ... -1. -3. -1.]\n",
      " ...\n",
      " [-1.  3. -1. ... -1.  1.  1.]\n",
      " [ 1. -3.  3. ... -1. -3. -1.]\n",
      " [ 1. -3.  1. ... -1. -3.  1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-3., -1.,  3., ...,  3.,  3., -1.],\n",
       "       [-1.,  1., -3., ...,  3.,  3., -1.],\n",
       "       [-1., -3.,  3., ..., -1., -3., -1.],\n",
       "       ...,\n",
       "       [-1.,  1., -1., ..., -1.,  1.,  3.],\n",
       "       [ 1., -1.,  3., ..., -1., -1., -3.],\n",
       "       [-1., -3.,  3., ..., -1., -3.,  1.]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(class_predictions)\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7723265"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_accuracy(class_predictions, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000000, 82) (1000000, 80)\n",
      "[[ 0.58510367  0.27991328  0.6990543  ...  0.63497462  0.83208688\n",
      "   0.71619867]\n",
      " [ 0.377737    0.622263   -0.46781764 ...  1.38120405  0.99868608\n",
      "   0.64274304]\n",
      " [ 0.69829844  0.30170156  0.73131857 ... -0.00832878 -0.86258825\n",
      "  -1.23054454]\n",
      " ...\n",
      " [ 0.64788962  0.03211591 -0.95232911 ... -0.60684815  0.63445482\n",
      "  -0.50402474]\n",
      " [ 0.36509102  0.63490898  0.37701816 ... -1.27908754 -1.20342509\n",
      "  -0.2966609 ]\n",
      " [ 0.61860753  0.38139247  0.87333623 ... -1.05399182  0.31911646\n",
      "   0.4181788 ]]\n",
      "[[ 1.0457358  -0.82488697  0.93957313 ...  0.94173252  0.99020137\n",
      "   0.91457268]\n",
      " [-1.00228004  0.95907844 -1.08495527 ...  0.97244609  1.07311545\n",
      "  -0.90530487]\n",
      " [ 0.81306578  1.03109152 -1.05824808 ... -1.00051518 -1.04101842\n",
      "  -1.06215571]\n",
      " ...\n",
      " [-0.91371751 -0.81738698 -1.05443542 ... -0.89855549  1.06235014\n",
      "  -0.95620143]\n",
      " [ 1.17281214  1.011111   -0.99881102 ... -1.10124419 -1.22408167\n",
      "   0.93205307]\n",
      " [ 1.02925093 -0.91499863 -0.95087766 ... -1.04681055  0.91293204\n",
      "   0.77888403]]\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.2595 - mean_squared_error: 0.2595 - val_loss: 0.2121 - val_mean_squared_error: 0.2121\n",
      "Epoch 2/100\n",
      "800000/800000 [==============================] - 121s - loss: 0.2058 - mean_squared_error: 0.2058 - val_loss: 0.2013 - val_mean_squared_error: 0.2013\n",
      "Epoch 3/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.1978 - mean_squared_error: 0.1978 - val_loss: 0.1934 - val_mean_squared_error: 0.1934\n",
      "Epoch 4/100\n",
      "800000/800000 [==============================] - 114s - loss: 0.1929 - mean_squared_error: 0.1929 - val_loss: 0.1924 - val_mean_squared_error: 0.1924\n",
      "Epoch 5/100\n",
      "800000/800000 [==============================] - 118s - loss: 0.1896 - mean_squared_error: 0.1896 - val_loss: 0.1858 - val_mean_squared_error: 0.1858\n",
      "Epoch 6/100\n",
      "800000/800000 [==============================] - 117s - loss: 0.1873 - mean_squared_error: 0.1873 - val_loss: 0.1849 - val_mean_squared_error: 0.1849\n",
      "Epoch 7/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.1851 - mean_squared_error: 0.1851 - val_loss: 0.1853 - val_mean_squared_error: 0.1853\n",
      "Epoch 8/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.1838 - mean_squared_error: 0.1838 - val_loss: 0.1834 - val_mean_squared_error: 0.1834\n",
      "Epoch 9/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.1816 - mean_squared_error: 0.1816 - val_loss: 0.1790 - val_mean_squared_error: 0.1790\n",
      "Epoch 10/100\n",
      "800000/800000 [==============================] - 124s - loss: 0.1781 - mean_squared_error: 0.1781 - val_loss: 0.1752 - val_mean_squared_error: 0.1752\n",
      "Epoch 11/100\n",
      "800000/800000 [==============================] - 123s - loss: 0.1738 - mean_squared_error: 0.1738 - val_loss: 0.1711 - val_mean_squared_error: 0.1711\n",
      "Epoch 12/100\n",
      "800000/800000 [==============================] - 124s - loss: 0.1682 - mean_squared_error: 0.1682 - val_loss: 0.1651 - val_mean_squared_error: 0.1651\n",
      "Epoch 13/100\n",
      "800000/800000 [==============================] - 122s - loss: 0.1622 - mean_squared_error: 0.1622 - val_loss: 0.1585 - val_mean_squared_error: 0.1585\n",
      "Epoch 14/100\n",
      "800000/800000 [==============================] - 122s - loss: 0.1533 - mean_squared_error: 0.1533 - val_loss: 0.1469 - val_mean_squared_error: 0.1469\n",
      "Epoch 15/100\n",
      "800000/800000 [==============================] - 117s - loss: 0.1398 - mean_squared_error: 0.1398 - val_loss: 0.1327 - val_mean_squared_error: 0.1327\n",
      "Epoch 16/100\n",
      "800000/800000 [==============================] - 121s - loss: 0.1232 - mean_squared_error: 0.1232 - val_loss: 0.1145 - val_mean_squared_error: 0.1145\n",
      "Epoch 17/100\n",
      "800000/800000 [==============================] - 122s - loss: 0.1072 - mean_squared_error: 0.1072 - val_loss: 0.1017 - val_mean_squared_error: 0.1017\n",
      "Epoch 18/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0969 - mean_squared_error: 0.0969 - val_loss: 0.0930 - val_mean_squared_error: 0.0930\n",
      "Epoch 19/100\n",
      "800000/800000 [==============================] - 118s - loss: 0.0911 - mean_squared_error: 0.0911 - val_loss: 0.0891 - val_mean_squared_error: 0.0891\n",
      "Epoch 20/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0881 - mean_squared_error: 0.0881 - val_loss: 0.0872 - val_mean_squared_error: 0.0872\n",
      "Epoch 21/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0857 - mean_squared_error: 0.0857 - val_loss: 0.0843 - val_mean_squared_error: 0.0843\n",
      "Epoch 22/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0841 - mean_squared_error: 0.0841 - val_loss: 0.0827 - val_mean_squared_error: 0.0827\n",
      "Epoch 23/100\n",
      "800000/800000 [==============================] - 118s - loss: 0.0832 - mean_squared_error: 0.0832 - val_loss: 0.0834 - val_mean_squared_error: 0.0834\n",
      "Epoch 24/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0826 - mean_squared_error: 0.0826 - val_loss: 0.0825 - val_mean_squared_error: 0.0825\n",
      "Epoch 25/100\n",
      "800000/800000 [==============================] - 116s - loss: 0.0820 - mean_squared_error: 0.0820 - val_loss: 0.0813 - val_mean_squared_error: 0.0813\n",
      "Epoch 26/100\n",
      "800000/800000 [==============================] - 116s - loss: 0.0815 - mean_squared_error: 0.0815 - val_loss: 0.0812 - val_mean_squared_error: 0.0812\n",
      "Epoch 27/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0809 - mean_squared_error: 0.0809 - val_loss: 0.0810 - val_mean_squared_error: 0.0810\n",
      "Epoch 28/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0804 - mean_squared_error: 0.0804 - val_loss: 0.0815 - val_mean_squared_error: 0.0815\n",
      "Epoch 29/100\n",
      "800000/800000 [==============================] - 121s - loss: 0.0801 - mean_squared_error: 0.0801 - val_loss: 0.0802 - val_mean_squared_error: 0.0802\n",
      "Epoch 30/100\n",
      "800000/800000 [==============================] - 118s - loss: 0.0798 - mean_squared_error: 0.0798 - val_loss: 0.0799 - val_mean_squared_error: 0.0799\n",
      "Epoch 31/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0795 - mean_squared_error: 0.0795 - val_loss: 0.0793 - val_mean_squared_error: 0.0793\n",
      "Epoch 32/100\n",
      "800000/800000 [==============================] - 121s - loss: 0.0792 - mean_squared_error: 0.0792 - val_loss: 0.0788 - val_mean_squared_error: 0.0788\n",
      "Epoch 33/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0790 - mean_squared_error: 0.0790 - val_loss: 0.0799 - val_mean_squared_error: 0.0799\n",
      "Epoch 34/100\n",
      "800000/800000 [==============================] - 122s - loss: 0.0787 - mean_squared_error: 0.0787 - val_loss: 0.0790 - val_mean_squared_error: 0.0790\n",
      "Epoch 35/100\n",
      "800000/800000 [==============================] - 127s - loss: 0.0786 - mean_squared_error: 0.0786 - val_loss: 0.0787 - val_mean_squared_error: 0.0787\n",
      "Epoch 36/100\n",
      "800000/800000 [==============================] - 125s - loss: 0.0784 - mean_squared_error: 0.0784 - val_loss: 0.0782 - val_mean_squared_error: 0.0782\n",
      "Epoch 37/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0783 - mean_squared_error: 0.0783 - val_loss: 0.0780 - val_mean_squared_error: 0.0780\n",
      "Epoch 38/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0781 - mean_squared_error: 0.0781 - val_loss: 0.0782 - val_mean_squared_error: 0.0782\n",
      "Epoch 39/100\n",
      "800000/800000 [==============================] - 121s - loss: 0.0781 - mean_squared_error: 0.0781 - val_loss: 0.0774 - val_mean_squared_error: 0.0774\n",
      "Epoch 40/100\n",
      "800000/800000 [==============================] - 123s - loss: 0.0779 - mean_squared_error: 0.0779 - val_loss: 0.0778 - val_mean_squared_error: 0.0778\n",
      "Epoch 41/100\n",
      "800000/800000 [==============================] - 124s - loss: 0.0778 - mean_squared_error: 0.0778 - val_loss: 0.0782 - val_mean_squared_error: 0.0782\n",
      "Epoch 42/100\n",
      "800000/800000 [==============================] - 121s - loss: 0.0776 - mean_squared_error: 0.0776 - val_loss: 0.0767 - val_mean_squared_error: 0.0767\n",
      "Epoch 43/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0774 - mean_squared_error: 0.0774 - val_loss: 0.0766 - val_mean_squared_error: 0.0766\n",
      "Epoch 44/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000/800000 [==============================] - 119s - loss: 0.0773 - mean_squared_error: 0.0773 - val_loss: 0.0777 - val_mean_squared_error: 0.0777\n",
      "Epoch 45/100\n",
      "800000/800000 [==============================] - 118s - loss: 0.0772 - mean_squared_error: 0.0772 - val_loss: 0.0773 - val_mean_squared_error: 0.0773\n",
      "Epoch 46/100\n",
      "800000/800000 [==============================] - 117s - loss: 0.0770 - mean_squared_error: 0.0770 - val_loss: 0.0767 - val_mean_squared_error: 0.0767\n",
      "Epoch 47/100\n",
      "800000/800000 [==============================] - 118s - loss: 0.0768 - mean_squared_error: 0.0768 - val_loss: 0.0766 - val_mean_squared_error: 0.0766\n",
      "Epoch 48/100\n",
      "800000/800000 [==============================] - 115s - loss: 0.0767 - mean_squared_error: 0.0767 - val_loss: 0.0771 - val_mean_squared_error: 0.0771\n",
      "Epoch 49/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0766 - mean_squared_error: 0.0766 - val_loss: 0.0767 - val_mean_squared_error: 0.0767\n",
      "Epoch 50/100\n",
      "800000/800000 [==============================] - 123s - loss: 0.0765 - mean_squared_error: 0.0765 - val_loss: 0.0753 - val_mean_squared_error: 0.0753\n",
      "Epoch 51/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0764 - mean_squared_error: 0.0764 - val_loss: 0.0760 - val_mean_squared_error: 0.0760\n",
      "Epoch 52/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0763 - mean_squared_error: 0.0763 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
      "Epoch 53/100\n",
      "800000/800000 [==============================] - 124s - loss: 0.0762 - mean_squared_error: 0.0762 - val_loss: 0.0758 - val_mean_squared_error: 0.0758\n",
      "Epoch 54/100\n",
      "800000/800000 [==============================] - 122s - loss: 0.0762 - mean_squared_error: 0.0762 - val_loss: 0.0770 - val_mean_squared_error: 0.0770\n",
      "Epoch 55/100\n",
      "800000/800000 [==============================] - 123s - loss: 0.0761 - mean_squared_error: 0.0761 - val_loss: 0.0761 - val_mean_squared_error: 0.0761\n",
      "Epoch 56/100\n",
      "800000/800000 [==============================] - 117s - loss: 0.0761 - mean_squared_error: 0.0761 - val_loss: 0.0754 - val_mean_squared_error: 0.0754\n",
      "Epoch 57/100\n",
      "800000/800000 [==============================] - 117s - loss: 0.0761 - mean_squared_error: 0.0761 - val_loss: 0.0758 - val_mean_squared_error: 0.0758\n",
      "Epoch 58/100\n",
      "800000/800000 [==============================] - 121s - loss: 0.0761 - mean_squared_error: 0.0761 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
      "Epoch 59/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0761 - mean_squared_error: 0.0761 - val_loss: 0.0760 - val_mean_squared_error: 0.0760\n",
      "Epoch 60/100\n",
      "800000/800000 [==============================] - 125s - loss: 0.0760 - mean_squared_error: 0.0760 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
      "Epoch 61/100\n",
      "800000/800000 [==============================] - 116s - loss: 0.0760 - mean_squared_error: 0.0760 - val_loss: 0.0766 - val_mean_squared_error: 0.0766\n",
      "Epoch 62/100\n",
      "800000/800000 [==============================] - 125s - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0753 - val_mean_squared_error: 0.0753\n",
      "Epoch 63/100\n",
      "800000/800000 [==============================] - 118s - loss: 0.0760 - mean_squared_error: 0.0760 - val_loss: 0.0764 - val_mean_squared_error: 0.0764\n",
      "Epoch 64/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0760 - mean_squared_error: 0.0760 - val_loss: 0.0759 - val_mean_squared_error: 0.0759\n",
      "Epoch 65/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0756 - val_mean_squared_error: 0.0756\n",
      "Epoch 66/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0760 - val_mean_squared_error: 0.0760\n",
      "Epoch 67/100\n",
      "800000/800000 [==============================] - 122s - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0770 - val_mean_squared_error: 0.0770\n",
      "Epoch 68/100\n",
      "800000/800000 [==============================] - 126s - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0753 - val_mean_squared_error: 0.0753\n",
      "Epoch 69/100\n",
      "800000/800000 [==============================] - 124s - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0762 - val_mean_squared_error: 0.0762\n",
      "Epoch 70/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0750 - val_mean_squared_error: 0.0750\n",
      "Epoch 71/100\n",
      "800000/800000 [==============================] - 115s - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
      "Epoch 72/100\n",
      "800000/800000 [==============================] - 119s - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0757 - val_mean_squared_error: 0.0757\n",
      "Epoch 73/100\n",
      "800000/800000 [==============================] - 121s - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.0767 - val_mean_squared_error: 0.0767\n",
      "Epoch 74/100\n",
      "800000/800000 [==============================] - 122s - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.0765 - val_mean_squared_error: 0.0765\n",
      "Epoch 75/100\n",
      "800000/800000 [==============================] - 121s - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.0757 - val_mean_squared_error: 0.0757\n",
      "Epoch 76/100\n",
      "800000/800000 [==============================] - 122s - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
      "Epoch 77/100\n",
      "800000/800000 [==============================] - 124s - loss: 0.0759 - mean_squared_error: 0.0759 - val_loss: 0.0764 - val_mean_squared_error: 0.0764\n",
      "Epoch 78/100\n",
      "800000/800000 [==============================] - 123s - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.0769 - val_mean_squared_error: 0.0769\n",
      "Epoch 79/100\n",
      "800000/800000 [==============================] - 112s - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.0763 - val_mean_squared_error: 0.0763\n",
      "Epoch 80/100\n",
      "800000/800000 [==============================] - 106s - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.0761 - val_mean_squared_error: 0.0761\n",
      "Epoch 81/100\n",
      "800000/800000 [==============================] - 113s - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.0750 - val_mean_squared_error: 0.0750\n",
      "Epoch 82/100\n",
      "800000/800000 [==============================] - 121s - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.0762 - val_mean_squared_error: 0.0762\n",
      "Epoch 83/100\n",
      "800000/800000 [==============================] - 125s - loss: 0.0757 - mean_squared_error: 0.0757 - val_loss: 0.0766 - val_mean_squared_error: 0.0766\n",
      "Epoch 84/100\n",
      "800000/800000 [==============================] - 117s - loss: 0.0758 - mean_squared_error: 0.0758 - val_loss: 0.0760 - val_mean_squared_error: 0.0760\n",
      "Epoch 85/100\n",
      "800000/800000 [==============================] - 116s - loss: 0.0757 - mean_squared_error: 0.0757 - val_loss: 0.0760 - val_mean_squared_error: 0.0760\n",
      "Epoch 86/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0757 - mean_squared_error: 0.0757 - val_loss: 0.0747 - val_mean_squared_error: 0.0747\n",
      "Epoch 87/100\n",
      "800000/800000 [==============================] - 118s - loss: 0.0757 - mean_squared_error: 0.0757 - val_loss: 0.0762 - val_mean_squared_error: 0.0762\n",
      "Epoch 88/100\n",
      "800000/800000 [==============================] - 115s - loss: 0.0757 - mean_squared_error: 0.0757 - val_loss: 0.0754 - val_mean_squared_error: 0.0754\n",
      "Epoch 89/100\n",
      "800000/800000 [==============================] - 116s - loss: 0.0757 - mean_squared_error: 0.0757 - val_loss: 0.0759 - val_mean_squared_error: 0.0759\n",
      "Epoch 90/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0757 - mean_squared_error: 0.0757 - val_loss: 0.0764 - val_mean_squared_error: 0.0764\n",
      "Epoch 91/100\n",
      "800000/800000 [==============================] - 125s - loss: 0.0756 - mean_squared_error: 0.0756 - val_loss: 0.0755 - val_mean_squared_error: 0.0755\n",
      "Epoch 92/100\n",
      "800000/800000 [==============================] - 117s - loss: 0.0756 - mean_squared_error: 0.0756 - val_loss: 0.0762 - val_mean_squared_error: 0.0762\n",
      "Epoch 93/100\n",
      "800000/800000 [==============================] - 113s - loss: 0.0756 - mean_squared_error: 0.0756 - val_loss: 0.0744 - val_mean_squared_error: 0.0744\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000/800000 [==============================] - 107s - loss: 0.0756 - mean_squared_error: 0.0756 - val_loss: 0.0753 - val_mean_squared_error: 0.0753\n",
      "Epoch 95/100\n",
      "800000/800000 [==============================] - 118s - loss: 0.0756 - mean_squared_error: 0.0756 - val_loss: 0.0758 - val_mean_squared_error: 0.0758\n",
      "Epoch 96/100\n",
      "800000/800000 [==============================] - 118s - loss: 0.0756 - mean_squared_error: 0.0756 - val_loss: 0.0760 - val_mean_squared_error: 0.0760\n",
      "Epoch 97/100\n",
      "800000/800000 [==============================] - 124s - loss: 0.0756 - mean_squared_error: 0.0756 - val_loss: 0.0753 - val_mean_squared_error: 0.0753\n",
      "Epoch 98/100\n",
      "800000/800000 [==============================] - 123s - loss: 0.0756 - mean_squared_error: 0.0756 - val_loss: 0.0765 - val_mean_squared_error: 0.0765\n",
      "Epoch 99/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0756 - mean_squared_error: 0.0756 - val_loss: 0.0765 - val_mean_squared_error: 0.0765\n",
      "Epoch 100/100\n",
      "800000/800000 [==============================] - 120s - loss: 0.0756 - mean_squared_error: 0.0756 - val_loss: 0.0765 - val_mean_squared_error: 0.0765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c1485748>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_channel_data_set_noise(dataset_size, data_length, channel_length, SNR):\n",
    "    input_data_bits = np.random.randint(0,2,(dataset_size, data_length)) \n",
    "    orig_input_data_constellations = input_data_bits * 2 - 1\n",
    "    noise = np.random.normal(0, 0.1, orig_input_data_constellations.shape)\n",
    "#     noise = 0\n",
    "    input_data_constellations = orig_input_data_constellations + noise\n",
    "    convolved_data_constellations = []\n",
    "    channels= []\n",
    "    \n",
    "    for i in range(dataset_size):\n",
    "        channel_taps = np.random.uniform(0,1,channel_length)\n",
    "        if sum(channel_taps)>=1:\n",
    "            channel_taps = channel_taps / sum(channel_taps)\n",
    "        channels.append(channel_taps)\n",
    "        constellation_convolved = add_awgn_noise(sig.convolve(input_data_constellations[i], channel_taps, mode='same'), SNR)\n",
    "        convolved_data_constellations.append(constellation_convolved)\n",
    "    X = np.hstack([np.array(channels), np.array(convolved_data_constellations)])\n",
    "    Y = input_data_constellations\n",
    "    return X, Y\n",
    "\n",
    "X, Y = generate_channel_data_set_noise(1000000, 80, 2, 10)\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "print(X)\n",
    "print(Y)\n",
    "def create_model_channel(input_layer_dim, hidden_layer_dim, input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_layer_dim, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(hidden_layer_dim, activation='relu'))\n",
    "    model.add(Dense(output_dim, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "    return model\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "model = create_model_channel(300, 300, X_train.shape[1], Y_train.shape[1])\n",
    "model.fit(X_train, Y_train, epochs=100, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99905435, -0.94742967,  1.03344634, ...,  0.81244934,\n",
       "         0.93319476, -1.00454789],\n",
       "       [ 1.06305385,  1.07217184,  1.0943496 , ..., -1.06118516,\n",
       "         0.94746957, -0.90838987],\n",
       "       [ 0.9532574 ,  0.99904465,  0.9737466 , ..., -0.89300743,\n",
       "        -1.12426887, -1.06055373],\n",
       "       ...,\n",
       "       [ 1.0556749 , -1.25187042,  1.01688018, ..., -0.98736371,\n",
       "        -1.12501909,  1.02163349],\n",
       "       [-1.09924976,  1.13468543, -0.90543492, ...,  1.03334248,\n",
       "         1.03980909,  0.92940934],\n",
       "       [ 1.06156287, -0.93012383,  1.02566621, ...,  0.97234898,\n",
       "         0.98817978,  0.98679479]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0248083 , -1.1096332 ,  0.67372817, ...,  0.9441439 ,\n",
       "         1.2120521 , -0.7651621 ],\n",
       "       [ 1.1253104 ,  0.74559355,  1.1236753 , ..., -1.1236415 ,\n",
       "         1.2498152 , -0.7722392 ],\n",
       "       [ 1.1139121 ,  0.7753333 ,  1.1115692 , ..., -1.1459732 ,\n",
       "        -0.9868889 , -0.6038163 ],\n",
       "       ...,\n",
       "       [ 1.0229745 , -1.0331888 ,  1.0730779 , ..., -0.8623979 ,\n",
       "        -0.9004607 ,  1.0323029 ],\n",
       "       [-1.0327303 ,  0.85857   , -0.94071835, ...,  1.151061  ,\n",
       "         1.0192033 ,  0.88029015],\n",
       "       [ 0.9582402 , -1.0864005 ,  0.93315023, ...,  0.98341596,\n",
       "         0.85102725,  0.8240269 ]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 1.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 1.],\n",
       "       [1., 0., 1., ..., 0., 1., 1.]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
