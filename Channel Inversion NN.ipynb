{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs: Channel coefficients\n",
    "\n",
    "Outpus: Inverse channel coefficients\n",
    "\n",
    "Error: L2 norm of vector difference between inverse estimates and actual estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create training and test data for the NN\n",
    "\n",
    "# number of random channels that will be trained and tested on\n",
    "num_train=1000000\n",
    "num_test=10\n",
    "\n",
    "# assume we know the channel_length\n",
    "channel_length = 2\n",
    "\n",
    "# assume we are working with just real parts\n",
    "channel_train = np.zeros((num_train,channel_length))\n",
    "inverse_train = np.zeros((num_train,channel_length))\n",
    "channel_test = np.zeros((num_test,channel_length))\n",
    "inverse_test =np.zeros((num_test,channel_length))\n",
    "\n",
    "for i in range(0, num_train):\n",
    "    channel_train[i,:]=np.random.uniform(0.2,1,2)\n",
    "    # if the total power is greater than 1, then normalize\n",
    "    if sum(channel_train[i])>=1:\n",
    "        channel_train[i] = channel_train[i]/(sum(channel_train[i]))\n",
    "    \n",
    "    inverse_train[i,0] = 1/channel_train[i,0]\n",
    "    for j in range(1, channel_length):\n",
    "        inverse_train[i,j] = -channel_train[i,j]\n",
    "        \n",
    "        \n",
    "for i in range(0, num_test):\n",
    "    channel_test[i,:]=np.random.uniform(0.2,1,2)\n",
    "    # if the total power is greater than 1, then normalize\n",
    "    if sum(channel_test[i])>=1:\n",
    "        channel_test[i] = channel_test[i]/(sum(channel_test[i]))\n",
    "    \n",
    "    inverse_test[i,0] = 1/channel_test[i,0]\n",
    "    for j in range(1, channel_length):\n",
    "        inverse_test[i,j] = -channel_test[i,j]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import commpy as cp\n",
    "import scipy.signal as sig\n",
    "\n",
    "# define the number of constellation points\n",
    "c=4 \n",
    "# length of array of bits (must be divisible by c)\n",
    "n=4*c\n",
    "\n",
    "# create the modulation object with c constellation points\n",
    "# QAMModem = cp.modulation.QAMModem(c)\n",
    "# create a random array of bits\n",
    "input_data_bits=np.random.randint(0,2,n) \n",
    "# use the constellation object to modulate the data bits into complex numbers\n",
    "# input_data_constellations = QAMModem.modulate(input_data_bits)\n",
    "print('Input data bits',input_data_bits)\n",
    "# print('Input data constellations',input_data_constellations)\n",
    "\n",
    "# an example of a channel function with two consecutive taps\n",
    "channel_function=channel_train[3]\n",
    "print('Channel function: ', channel_function)\n",
    "# convolution of the input complex data with the channel transfer function\n",
    "# channel_output = sig.convolve(input_data_constellations, channel_function, mode='full')\n",
    "channel_output = sig.convolve(input_data_bits, channel_function, mode='full')\n",
    "\n",
    "print('Channel output',channel_output)\n",
    "\n",
    "channel_inversion = inverse_train[3]\n",
    "equalizer_output = sig.convolve(channel_output, channel_inversion, mode='full')\n",
    "\n",
    "print('Equalizer output', equalizer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NN to do channel inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laura/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Cost 3.1964173316955566, MC Cost: 2.8843770027160645\n",
      "Epoch 100, Cost 0.2004367709159851, MC Cost: 0.26523855328559875\n",
      "Epoch 200, Cost 0.12041230499744415, MC Cost: 0.1443423330783844\n",
      "Epoch 300, Cost 0.05108363926410675, MC Cost: 0.03997243568301201\n",
      "Epoch 400, Cost 0.04993143305182457, MC Cost: 0.031865816563367844\n",
      "Epoch 500, Cost 0.025084692984819412, MC Cost: 0.013579720631241798\n",
      "Epoch 600, Cost 0.011015092954039574, MC Cost: 0.002462635049596429\n",
      "Epoch 700, Cost 0.0035076921340078115, MC Cost: 0.0014392725424841046\n",
      "Epoch 800, Cost 0.0027044443413615227, MC Cost: 0.0016085223760455847\n",
      "Epoch 900, Cost 0.002129240892827511, MC Cost: 0.0013587165158241987\n",
      "Epoch 1000, Cost 0.001772560877725482, MC Cost: 0.0011234795674681664\n",
      "Epoch 1100, Cost 0.002080019097775221, MC Cost: 0.001055353321135044\n",
      "Epoch 1200, Cost 0.0020889476872980595, MC Cost: 0.0009121649782173336\n",
      "Epoch 1300, Cost 0.0015305374981835485, MC Cost: 0.0006902244640514255\n",
      "Epoch 1400, Cost 0.001191539573483169, MC Cost: 0.0007154232589527965\n",
      "Epoch 1500, Cost 0.0009257692145183682, MC Cost: 0.0005934456130489707\n",
      "Epoch 1600, Cost 0.0007863332284614444, MC Cost: 0.0004655960947275162\n",
      "Epoch 1700, Cost 0.0008904470596462488, MC Cost: 0.0004080214421264827\n",
      "Epoch 1800, Cost 0.0004580306413117796, MC Cost: 0.0002969536290038377\n",
      "Epoch 1900, Cost 0.0003989639808423817, MC Cost: 0.00023689493536949158\n",
      "Epoch 2000, Cost 0.00035596347879618406, MC Cost: 0.00023198341659735888\n",
      "Epoch 2100, Cost 0.00040137858013622463, MC Cost: 0.000122821846161969\n",
      "Epoch 2200, Cost 0.000299300707411021, MC Cost: 0.00012286101991776377\n",
      "Epoch 2300, Cost 0.0002143069141311571, MC Cost: 0.00010165841376874596\n",
      "Epoch 2400, Cost 0.00012137424346292391, MC Cost: 5.981140930089168e-05\n",
      "Epoch 2500, Cost 0.0001412393758073449, MC Cost: 2.9688768336200155e-05\n",
      "Epoch 2600, Cost 0.00020655401749536395, MC Cost: 4.89681915496476e-05\n",
      "Epoch 2700, Cost 0.0001015672241919674, MC Cost: 3.0448578399955295e-05\n",
      "Epoch 2800, Cost 5.976952161290683e-05, MC Cost: 1.945289841387421e-05\n",
      "Epoch 2900, Cost 8.105123561108485e-05, MC Cost: 2.844384653144516e-05\n",
      "Epoch 3000, Cost 8.042492117965594e-05, MC Cost: 9.784382200450636e-06\n",
      "Epoch 3100, Cost 7.796304998919368e-05, MC Cost: 1.0390512215963099e-05\n",
      "Epoch 3200, Cost 0.00010667391325114295, MC Cost: 1.9532062651705928e-05\n",
      "Epoch 3300, Cost 5.9750185755547136e-05, MC Cost: 2.848857911885716e-05\n",
      "Epoch 3400, Cost 8.87135392986238e-05, MC Cost: 6.417286931537092e-05\n",
      "Epoch 3500, Cost 0.0001441539789084345, MC Cost: 6.315309292403981e-05\n",
      "Epoch 3600, Cost 6.677211058558896e-05, MC Cost: 1.116993462346727e-05\n",
      "Epoch 3700, Cost 7.931636355351657e-05, MC Cost: 6.021630179020576e-05\n",
      "Epoch 3800, Cost 3.914029730367474e-05, MC Cost: 6.281568403210258e-06\n",
      "Epoch 3900, Cost 3.0069037165958434e-05, MC Cost: 7.459896551154088e-06\n",
      "Epoch 4000, Cost 2.4420452973572537e-05, MC Cost: 7.719504537817556e-06\n",
      "Epoch 4100, Cost 2.7364943889551796e-05, MC Cost: 6.69196151648066e-06\n",
      "Epoch 4200, Cost 4.6754375944146886e-05, MC Cost: 1.2660835636779666e-05\n",
      "Epoch 4300, Cost 5.896152651985176e-05, MC Cost: 5.0873408326879144e-05\n",
      "Epoch 4400, Cost 5.776436591986567e-05, MC Cost: 1.3184786439524032e-05\n",
      "Epoch 4500, Cost 2.7289237550576217e-05, MC Cost: 1.0974295946653001e-05\n",
      "Epoch 4600, Cost 5.26466392329894e-05, MC Cost: 2.873732228181325e-05\n",
      "Epoch 4700, Cost 3.223209932912141e-05, MC Cost: 6.639597813773435e-06\n",
      "Epoch 4800, Cost 6.256124470382929e-05, MC Cost: 5.075391527498141e-05\n",
      "Epoch 4900, Cost 9.099646558752283e-05, MC Cost: 0.00010267032484989613\n",
      "Epoch 5000, Cost 2.8452548576751724e-05, MC Cost: 1.5198182154563256e-05\n",
      "Epoch 5100, Cost 4.865844312007539e-05, MC Cost: 6.189419946167618e-05\n",
      "Epoch 5200, Cost 0.00011056834773626179, MC Cost: 7.581156387459487e-05\n",
      "Epoch 5300, Cost 0.00012079977750545368, MC Cost: 0.00010754179675132036\n",
      "Epoch 5400, Cost 1.9471717678243294e-05, MC Cost: 1.1061085388064384e-05\n",
      "Epoch 5500, Cost 3.658175410237163e-05, MC Cost: 2.0639057765947655e-05\n",
      "Epoch 5600, Cost 1.8016657122643664e-05, MC Cost: 7.974578693392687e-06\n",
      "Epoch 5700, Cost 0.00013738441339228302, MC Cost: 0.0002277209860039875\n",
      "Epoch 5800, Cost 4.545797492028214e-05, MC Cost: 3.6926878237864e-05\n",
      "Epoch 5900, Cost 2.5294857550761662e-05, MC Cost: 6.275053692661459e-06\n",
      "Epoch 6000, Cost 0.0003005652688443661, MC Cost: 0.0003245470579713583\n",
      "Epoch 6100, Cost 7.709437340963632e-05, MC Cost: 8.60321888467297e-05\n",
      "Epoch 6200, Cost 2.1222178474999964e-05, MC Cost: 7.631553671672009e-06\n",
      "Epoch 6300, Cost 2.9644057576660998e-05, MC Cost: 5.828110079164617e-05\n",
      "Epoch 6400, Cost 0.0004691647191066295, MC Cost: 0.0005409131990745664\n",
      "Epoch 6500, Cost 5.67861607123632e-05, MC Cost: 6.750888132955879e-05\n",
      "Epoch 6600, Cost 5.1665327191585675e-05, MC Cost: 2.7227404643781483e-05\n",
      "Epoch 6700, Cost 1.5534200429101475e-05, MC Cost: 2.3888071154942736e-05\n",
      "Epoch 6800, Cost 0.0001921294315252453, MC Cost: 0.00013023233623243868\n",
      "Epoch 6900, Cost 2.5734510927577503e-05, MC Cost: 1.447672002541367e-05\n",
      "Epoch 7000, Cost 4.501274088397622e-05, MC Cost: 3.8912268792046234e-05\n",
      "Epoch 7100, Cost 0.00022201953106559813, MC Cost: 0.00021366441796999425\n",
      "Epoch 7200, Cost 5.5438733397750184e-05, MC Cost: 4.02177101932466e-05\n",
      "Epoch 7300, Cost 9.122711344389245e-05, MC Cost: 8.979265840025619e-05\n",
      "Epoch 7400, Cost 9.347411833005026e-05, MC Cost: 9.169396798824891e-05\n",
      "Epoch 7500, Cost 0.00018472326337359846, MC Cost: 0.00010106625268235803\n",
      "Epoch 7600, Cost 5.920290277572349e-05, MC Cost: 4.750879816128872e-05\n",
      "Epoch 7700, Cost 2.9931310564279556e-05, MC Cost: 2.0644536562031135e-05\n",
      "Epoch 7800, Cost 4.307129347580485e-05, MC Cost: 2.9214623282314278e-05\n",
      "Epoch 7900, Cost 0.00012591129052452743, MC Cost: 9.787485760170966e-05\n",
      "Epoch 8000, Cost 1.1966882993874606e-05, MC Cost: 5.4352508414012846e-06\n",
      "Epoch 8100, Cost 2.4295291950693354e-05, MC Cost: 1.558678923174739e-05\n",
      "Epoch 8200, Cost 7.81155249569565e-05, MC Cost: 6.036448758095503e-05\n",
      "Epoch 8300, Cost 5.066610174253583e-05, MC Cost: 3.7948375393170863e-05\n",
      "Epoch 8400, Cost 7.910579006420448e-05, MC Cost: 7.456216553691775e-05\n",
      "Epoch 8500, Cost 2.5525258024572395e-05, MC Cost: 5.594535105046816e-05\n",
      "Epoch 8600, Cost 4.3303756683599204e-05, MC Cost: 4.4137530494481325e-05\n",
      "Epoch 8700, Cost 0.00012892788799945265, MC Cost: 0.00018240651115775108\n",
      "Epoch 8800, Cost 3.0288894777186215e-05, MC Cost: 2.4668313926667906e-05\n",
      "Epoch 8900, Cost 1.0236139132757671e-05, MC Cost: 3.997792191512417e-06\n",
      "Epoch 9000, Cost 1.0447138265590183e-05, MC Cost: 7.267826731549576e-06\n",
      "Epoch 9100, Cost 1.9529388737282716e-05, MC Cost: 6.627099537581671e-06\n",
      "Epoch 9200, Cost 0.00048145937034860253, MC Cost: 0.00047693372471258044\n",
      "Epoch 9300, Cost 1.8028533304459415e-05, MC Cost: 8.893814083421603e-06\n",
      "Epoch 9400, Cost 1.8393286154605448e-05, MC Cost: 2.354525713599287e-05\n",
      "Epoch 9500, Cost 6.855859101051465e-05, MC Cost: 5.448812589747831e-05\n",
      "Epoch 9600, Cost 9.294356277678162e-05, MC Cost: 5.291345951263793e-05\n",
      "Epoch 9700, Cost 2.9929935408290476e-05, MC Cost: 2.1522113456740044e-05\n",
      "Epoch 9800, Cost 1.040152164932806e-05, MC Cost: 3.203613596269861e-05\n",
      "Epoch 9900, Cost 1.1224310583202168e-05, MC Cost: 4.755004738399293e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3a08f74d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed()\n",
    "\n",
    "m = 2 # of consecutive taps\n",
    "learning_rate = 0.001\n",
    "epochs = 10000\n",
    "batch_size = 1000\n",
    "test_averaging=100\n",
    "decay = 1 - 10*1e-10\n",
    "\n",
    "\n",
    "# declare the training data placeholders\n",
    "# input x - just one is x0\n",
    "real_channel = tf.placeholder(tf.float32, [None, m])\n",
    "real_inverse = tf.placeholder(tf.float32, [None, m])\n",
    "\n",
    "layer_1 = tf.layers.dense(\n",
    "  real_channel, 150, tf.nn.tanh, use_bias=True)\n",
    "layer_2 = tf.layers.dense(\n",
    "  layer_1, 150, tf.nn.tanh, use_bias=True)\n",
    "layer_3 = tf.layers.dense(\n",
    "  layer_2, m, activation=tf.identity, use_bias=True)\n",
    "\n",
    "inverse_channel = layer_3\n",
    "\n",
    "\n",
    "adaptive_learning_rate = tf.placeholder_with_default(learning_rate, [])\n",
    "\n",
    "inversion_cost = tf.reduce_mean(tf.reduce_mean((real_inverse-inverse_channel)**2, axis=1))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=adaptive_learning_rate).minimize(inversion_cost)\n",
    "                                                                                  \n",
    "# finally setup the initialisation operator\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# start the session\n",
    "with tf.Session() as sess:\n",
    "    #    initialize the variables\n",
    "    sess.run(init_op)\n",
    "    mc_losses = []\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        channel_batch = [1,0]\n",
    "        real_inv_batch = [1,0]\n",
    "        for rand in range(0,batch_size-1):\n",
    "            rand_int = np.random.randint(0,num_train)\n",
    "            channel_batch = np.vstack((channel_batch, channel_train[rand_int]))\n",
    "            real_inv_batch = np.vstack((real_inv_batch, inverse_train[rand_int]))\n",
    "            \n",
    "        channel_batch = channel_batch.reshape((batch_size, m))\n",
    "        real_inv_batch = real_inv_batch.reshape((batch_size, m))\n",
    "        \n",
    "        _,cost,inverse_channel2 = sess.run([optimizer, inversion_cost, inverse_channel], \n",
    "                                          feed_dict={real_channel: channel_batch, real_inverse: real_inv_batch,\n",
    "                         adaptive_learning_rate: learning_rate * (decay**epoch)})\n",
    "#         print(LA.norm(real_inv_batch-inverse_channel))\n",
    "        \n",
    "            \n",
    "        if epoch % 100 == 0: \n",
    "            plt.plot(epoch, cost, 'bo')\n",
    "            mc_cost, mc_inversion = sess.run([inversion_cost, inverse_channel], \n",
    "                                             feed_dict={real_channel: channel_test, \n",
    "                                                        real_inverse: inverse_test})\n",
    "            print('Epoch {}, Cost {}, MC Cost: {}'.format(epoch, cost, mc_cost))\n",
    "\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(mc_inversion)\n",
    "\n",
    "print(inverse_test)\n",
    "\n",
    "print(LA.norm(mc_inversion-inverse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search over architectures to find NN to learn how to invert the channel coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import uuid\n",
    "import sys\n",
    "import scipy.stats \n",
    "import matplotlib\n",
    "import itertools \n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import pickle\n",
    "\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "NUMSTEPS = 20 #For printing and saving to tensorboard\n",
    "\n",
    "NUMBATCHESTESTING = 100 #Number of batches for testing\n",
    "UNIQUEID=  uuid.uuid1().__str__()[:6]\n",
    "# TENSORBOARDPATH = \"tensorboard-layers-api/basic_\" \n",
    "# RESULTSFILEPATH = \"1d/results/dat/basic_\"  \n",
    "# METARESULTSFILEPATH = \"1d/results/meta_basic_1\"  #WARNING: CHANGE THIS FOR DIFFERENT RUNS\n",
    "# FIGPATH = \"1d/figures/basic_\"+ UNIQUEID\n",
    "# CHECKPOINTPATH = \"1d/checkpoints/basic_\" + UNIQUEID   \n",
    "\n",
    "\n",
    "############## \n",
    "#Helper functions\n",
    "##############\n",
    "\n",
    "def cartesian_product(*arrays): \n",
    "    return itertools.product(*arrays)\n",
    "\n",
    "\n",
    "def list_to_str(prefix, cur_list):\n",
    "    cur_list_str = [prefix+str(elem) for elem in cur_list]\n",
    "    return cur_list_str\n",
    "    \n",
    "    \n",
    "def tup_to_str(tups):\n",
    "    cur_str = \"\"\n",
    "    for k,tup in enumerate(tups):\n",
    "        cur_str += str(tup)\n",
    "        if k != len(tups)-1:\n",
    "            cur_str+=','\n",
    "    return cur_str\n",
    "\n",
    "def variable_summaries(var_str, var):\n",
    "    '''Attach a lot of summaries to a Tensor (for TensorBoard visualization).'''\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('norm_'+var_str, tf.norm(var))\n",
    "\n",
    "\n",
    "def neural_net(param_string=\"\", params={}, verbose=False, m=2):\n",
    "\n",
    "    '''m= channel_length, default 2\n",
    "        param_string: String encoding information about hyperparameters\n",
    "       \n",
    "       params: parameter dictionary that contains\n",
    "       num_batches: number of batches \n",
    "       batch_size: number of samples per batch\n",
    "       optimizer: the optimizer used to minimize cost\n",
    "    '''\n",
    "    tf.reset_default_graph()\n",
    "  \n",
    "    global_layer_num = 1\n",
    "    \n",
    "    seed = params['seed']\n",
    "    np.random.seed(seed)\n",
    "    tf.set_random_seed(seed)\n",
    "\n",
    "    #Learning rate and optimizer\n",
    "    optimizer_function = params['optimizer_function']\n",
    "    learning_rate = float(params['learning_rate'])\n",
    "\n",
    "    #Placeholders for inputs\n",
    "    coefs = tf.placeholder(tf.float32, [None, m])\n",
    "    real_inv = tf.placeholder(tf.float32, [None, m])\n",
    "#     z = tf.placeholder(tf.float32, [None, m])\n",
    "    adaptive_learning_rate = tf.placeholder_with_default(learning_rate, [])\n",
    "    \n",
    "\n",
    "\n",
    "    ###################################################\n",
    "    #Layers\n",
    "    ####################################################\n",
    "\n",
    "    #The layers   \n",
    "    layer_structures  = params['layer_structures']\n",
    "    ##########################        \n",
    "    #First layer structure\n",
    "    ##########################\n",
    "    layer_structure = layer_structures[0]\n",
    "    num_units = layer_structure[0]\n",
    "    layer_activation = layer_structure[1]\n",
    "    num_layers = layer_structure[2]\n",
    "    net = coefs #Input to first layer of this structure\n",
    "    for k in range(num_layers):\n",
    "        net = tf.layers.dense(inputs=net, units=num_units, activation=layer_activation , use_bias=True, \n",
    "                              name = 'layer' + str(global_layer_num),\n",
    "                              kernel_initializer=tf.glorot_normal_initializer())\n",
    "        \n",
    "        #Add to tensor board summary\n",
    "        with tf.variable_scope('layer' + str(global_layer_num), reuse=True):\n",
    "            with tf.name_scope('weights'):\n",
    "                w = tf.get_variable('kernel')\n",
    "                b = tf.get_variable('bias')\n",
    "                variable_summaries('w'+str(global_layer_num), w)\n",
    "                variable_summaries('b'+str(global_layer_num), b)\n",
    "\n",
    "        global_layer_num += 1       \n",
    "    h1 = net\n",
    "\n",
    "    ############################        \n",
    "    #Second layer structure\n",
    "    ############################\n",
    "\n",
    "    layer_structure = layer_structures[1]\n",
    "    num_units = layer_structure[0]\n",
    "    layer_activation = layer_structure[1]\n",
    "    num_layers = layer_structure[2]\n",
    "    net = h1 #Input to first layer of this structure\n",
    "    for k in range(num_layers):\n",
    "        net = tf.layers.dense(inputs=net, units=m, activation=layer_activation,use_bias=True, \n",
    "                              name = 'layer' + str(global_layer_num),\n",
    "                              kernel_initializer=tf.glorot_normal_initializer())\n",
    "        #Add to tensor board summary\n",
    "        with tf.variable_scope('layer' + str(global_layer_num), reuse=True):\n",
    "            with tf.name_scope('weights'):\n",
    "                w = tf.get_variable('kernel')\n",
    "                b = tf.get_variable('bias')\n",
    "                variable_summaries('w'+str(global_layer_num), w)\n",
    "                variable_summaries('b'+str(global_layer_num), b)\n",
    "        global_layer_num += 1    \n",
    "    inverse = net\n",
    "\n",
    "    cost = tf.reduce_mean(tf.reduce_mean((inverse-real_inv)**2,axis=1))\n",
    "    optimizer = optimizer_function(adaptive_learning_rate).minimize(cost)\n",
    "\n",
    "    ###################\n",
    "    #Tensor board summary\n",
    "    ####################\n",
    "    tf.summary.scalar(\"cost\", cost)\n",
    "    tf.summary.scalar(\"adaptive_learning_rate\", adaptive_learning_rate)\n",
    "    merged_summary_op = tf.summary.merge_all()   \n",
    "\n",
    "\n",
    "    ######################################\n",
    "    #Session\n",
    "    #####################################\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        #Creates/saves in directory named tensorboard-layers-api in same directory as the .ipynb\n",
    "#         uniq_id = TENSORBOARDPATH +  UNIQUEID +\"/\" + param_string\n",
    "#         summary_writer = tf.summary.FileWriter(uniq_id, graph=tf.get_default_graph())\n",
    "\n",
    "\n",
    "        #######################################\n",
    "        #Training\n",
    "        #Train the nn on training set constructed by sampling x0 and z independently from gaussian\n",
    "        #distributions with means 0 and standard deviations sigma_x0 and sigma_z respectively\n",
    "        #######################################\n",
    "\n",
    "        if verbose is True:\n",
    "            print(\"Training...\")\n",
    "\n",
    "        #Initialization\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op) \n",
    "        \n",
    "        run_num = params['run_num']\n",
    "        #Save initializations to file\n",
    "        saver = tf.train.Saver()\n",
    "#         cur_checkpoint_path = params['checkpoint_path']\n",
    "#         saver.save(sess, cur_checkpoint_path)\n",
    "\n",
    "        batch_size = params['batch_size']\n",
    "        max_epochs = params['max_epochs']\n",
    "        learning_rate_decay= float(params['learning_rate_decay'])\n",
    "\n",
    "        step_size= np.ceil(max_epochs/NUMSTEPS) #For printing and saving to tensor board\n",
    "\n",
    "        ''''''\n",
    "        train_costs = np.zeros((max_epochs,1))\n",
    "        current_start=0\n",
    "        for step in range(max_epochs): #Splitting into multiple batches so as to not run into memory issues\n",
    "            batch_coefs = channel_train[current_start:(current_start+batch_size),:]\n",
    "            batch_real_inv = inverse_train[current_start:(current_start+batch_size),:]\n",
    "            current_start=current_start+batch_size\n",
    "\n",
    "            #Adaptive learning rate \n",
    "            cur_adaptive_learning_rate = learning_rate*float(learning_rate_decay**step)\n",
    "\n",
    "\n",
    "            _,batch_cost,summary = sess.run([optimizer,cost,merged_summary_op],       \n",
    "                        feed_dict = {coefs:batch_coefs, real_inv:batch_real_inv, \n",
    "                                     adaptive_learning_rate:cur_adaptive_learning_rate})\n",
    "            train_costs[step] = batch_cost\n",
    "\n",
    "            if step % step_size == 0: \n",
    "#                 summary_writer.add_summary(summary, step)\n",
    "                if verbose is True:\n",
    "                    print(\"---Step: {}, Cost: {}\".format(step,batch_cost))\n",
    "        \n",
    "        #Save final weights\n",
    "#         final_checkpoint_path = params['final_checkpoint_path']\n",
    "#         saver.save(sess, final_checkpoint_path)\n",
    "\n",
    "        ######################################\n",
    "        #Testing on sampled Gaussians\n",
    "        #Evaluate the nn on test set constructed by sampling x0 and z independently from gaussian\n",
    "        #distributions  with means 0 and standard deviations sigma_x0 and sigma_z respectively            \n",
    "        ######################################################\n",
    "        if verbose is True:\n",
    "            print(\"Evaluating on sampled gaussians...\")\n",
    "        num_batches = params['num_batches']\n",
    "        batch_size = params['batch_size']\n",
    "        ''''''\n",
    "        test_costs = np.zeros((num_batches,1))\n",
    "        current_start=current_start+batch_size\n",
    "        for i in range(num_batches): #Splitting into multiple batches so as to not run into memory issues\n",
    "            batch_coefs = channel_test[current_start:(current_start+batch_size),:]\n",
    "            batch_real_inv = inverse_test[current_start:(current_start+batch_size),:]\n",
    "            current_start=current_start+batch_size\n",
    "\n",
    "            batch_cost = sess.run(cost, feed_dict = {coefs:batch_coefs, real_inv:batch_real_inv})\n",
    "            test_costs[i] = batch_cost\n",
    "            if verbose is True:\n",
    "                if i % step_size == 0:\n",
    "                    print(\"---Batch: {}, Cost: {}\".format(i,batch_cost))\n",
    "\n",
    "        avg_test_cost = float(np.mean(test_costs, axis = 0))\n",
    "        if verbose is True:\n",
    "            print(\"Average test cost: {}\".format(avg_test_cost))\n",
    "        \n",
    "        ##########################################################################\n",
    "        ###Test over representative x0 and z landscape to get complete picture\n",
    "        ##########################################################################\n",
    "        \n",
    "        \n",
    "\n",
    "        return avg_test_cost\n",
    "\n",
    "\n",
    "\n",
    "# def main():\n",
    "#######\n",
    "#Cartesian product to get the layer structures\n",
    "######\n",
    "#First find tuples for different types of layers\n",
    "\n",
    "hidden_units_type1 = [8,16,32]\n",
    "hidden_units_type1_str= list_to_str('hu_',hidden_units_type1)\n",
    "\n",
    "activations_type1 = [tf.nn.tanh, tf.nn.sigmoid]\n",
    "activations_type1_str = ['tanh', 'sigmoid', 'relu']\n",
    "\n",
    "num_layers_type1 = [1,2]\n",
    "num_layers_type1_str = list_to_str('nu_',num_layers_type1)\n",
    "\n",
    "\n",
    "m=2\n",
    "hidden_units_type2 = [m]\n",
    "hidden_units_type2_str= list_to_str('hu_',hidden_units_type2)\n",
    "activations_type2 = [tf.identity]\n",
    "activations_type2_str = ['identity']\n",
    "num_layers_type2= [1]\n",
    "num_layers_type2_str = list_to_str('nu_',num_layers_type2)\n",
    "\n",
    "layers_type1 = cartesian_product(hidden_units_type1, activations_type1, num_layers_type1)\n",
    "layers_type1_str = ['ls1_' + tup_to_str(tup) for tup in cartesian_product(hidden_units_type1_str, \n",
    "                                                                          activations_type1_str, \n",
    "                                                                          num_layers_type1_str)]\n",
    "\n",
    "\n",
    "layers_type2 = cartesian_product(hidden_units_type2, activations_type2, num_layers_type2)\n",
    "layers_type2_str = ['ls2_'+tup_to_str(tup) for tup in cartesian_product(hidden_units_type2_str, \n",
    "                                                                        activations_type2_str, \n",
    "                                                                        num_layers_type2_str)]\n",
    "\n",
    "layers = cartesian_product(layers_type1, layers_type2)\n",
    "\n",
    "layers_str = [tup_to_str(tup) for tup in cartesian_product(layers_type1_str, layers_type2_str)]\n",
    "\n",
    "#Next find tuples for the whole parameter space\n",
    "learning_rates = [1e-3,1e-2, 1e-1]\n",
    "learning_rates_str = ['lr_'+str(elem) for elem in learning_rates]\n",
    "learning_rate_decays = [1.-1e-9]\n",
    "learning_rate_decays_str = ['1-1e-9']\n",
    "max_epochs = [1000]\n",
    "max_epochs_str = ['me_' + str(elem) for elem in max_epochs]\n",
    "batch_sizes = [1000]\n",
    "batch_sizes_str = ['bsz_' +str(elem) for elem in batch_sizes]\n",
    "\n",
    "given_seeds = np.arange(10)\n",
    "given_seeds_str = [str(elem) for elem in given_seeds]\n",
    "\n",
    "optimizer_functions = [tf.train.AdamOptimizer]\n",
    "optimizer_functions_str = ['Adam']\n",
    "\n",
    "hyperparam_tuples = list(cartesian_product(layers, learning_rates, learning_rate_decays,max_epochs, \n",
    "                                           batch_sizes, optimizer_functions, given_seeds))\n",
    "hyperparam_tuples_str = [tup_to_str(tup) for tup in list(cartesian_product(layers_str, learning_rates_str, \n",
    "                                                                           learning_rate_decays_str, \n",
    "                                                                           max_epochs_str,batch_sizes_str, \n",
    "                                                                           optimizer_functions_str, \n",
    "                                                                           given_seeds_str))]\n",
    "\n",
    "\n",
    "##################\n",
    "#Main Loop\n",
    " ##################\n",
    "\n",
    "\n",
    "# meta_results_file = METARESULTSFILEPATH + '.txt'\n",
    "\n",
    "# with open(meta_results_file, 'w') as f:\n",
    "#    f.write(UNIQUEID + '\\n')\n",
    "#    f.close()\n",
    "\n",
    "hyperparam_results_list = []\n",
    "for k,tup in enumerate(tqdm(hyperparam_tuples)):\n",
    "\n",
    "#    if k > 1000:\n",
    "#     continue\n",
    "   layer_structure = tup[0]\n",
    "   learning_rate = tup[1]\n",
    "   learning_rate_decay = tup[2]\n",
    "   max_epoch = tup[3]\n",
    "   batch_size = tup[4]\n",
    "   optimizer_function = tup[5]\n",
    "   seed = tup[6]\n",
    "\n",
    "\n",
    "#    checkpoint_path = CHECKPOINTPATH + \"/model_\" + str(k) + \".ckpt\"\n",
    "#    final_checkpoint_path = CHECKPOINTPATH + \"/final_model_\" + str(k) + \".ckpt\"\n",
    "  \n",
    "   tup_str = hyperparam_tuples_str[k]\n",
    "   params = {'run_num':k, \n",
    "             'seed':seed, 'num_batches':NUMBATCHESTESTING, 'batch_size':batch_size, \n",
    "             'optimizer_function':optimizer_function, 'learning_rate':learning_rate, \n",
    "             'max_epochs':max_epoch,'learning_rate_decay':learning_rate_decay, \n",
    "             'layer_structures':layer_structure}\n",
    "\n",
    "   print(tup_str)\n",
    "#         avg_test_cost, avg_test_cost_rep, x0_test_rep, u1_test_rep, u2_test_rep, x1_test_rep, x2_test_rep =    \n",
    "#neural_net(param_string= tup_str, params=params, verbose = True)   \n",
    "   \n",
    "\n",
    "   avg_test_cost = neural_net(param_string= tup_str, params=params, verbose = True,m = m)   \n",
    "   \n",
    "\n",
    "   \n",
    "#    avg_cost_test_rep = np.sum(cost_test_rep*x_density)/np.sum(x_density)\n",
    "   \n",
    "#    hyperparam_results_dict = {}\n",
    "#    hyperparam_results_dict['tup_str'] = tup_str\n",
    "#    hyperparam_results_dict['tup'] = tup\n",
    "#    hyperparam_results_dict['cost'] = cost_test_rep\n",
    "#    hyperparam_results_dict['x1'] = x1_test_rep\n",
    "#    hyperparam_results_dict['x2']= x2_test_rep\n",
    "#    hyperparam_results_dict['x0'] = x_grid\n",
    "#    hyperparam_results_dict['u2']= u2_test_rep\n",
    "#    hyperparam_results_dict['params'] = params\n",
    "   \n",
    "#    hyperparam_results_list.append([avg_cost_test_rep, hyperparam_results_dict])\n",
    "   \n",
    "   \n",
    "#    results_file = RESULTSFILEPATH + UNIQUEID + \"_\" + str(k) + '.dat'\n",
    "   \n",
    "#    with open(results_file, 'wb') as f:\n",
    "#        pickle.dump(hyperparam_results_dict,f)\n",
    "#        f.close()\n",
    "   \n",
    "#    with open(meta_results_file, 'a') as f:\n",
    "#        f.write(str(k) + \" \" + results_file + \" \" + checkpoint_path + \" \" + final_checkpoint_path + \" \" +  str(avg_cost_test_rep) + \" \" + tup_str +  '\\n')\n",
    "\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print (\"compute time: \", stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_train[0:3,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
