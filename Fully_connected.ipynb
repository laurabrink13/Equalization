{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data: channels and approximate channel inverses \n",
    "### Generate test data: channels and approximate channel inverses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Create training and test data for the NN\n",
    "\n",
    "# number of random channels that will be trained and tested on\n",
    "num_train=1000 # 1000000\n",
    "num_test=10\n",
    "\n",
    "# assume we know the channel_length\n",
    "channel_length = 2\n",
    "num_preambles = 10\n",
    "preamble_length = 10\n",
    "\n",
    "preambles = np.random.randint(0,2,(num_preambles,preamble_length)) \n",
    "\n",
    "# channel_train: assume we are working with just real parts, list of all used channels\n",
    "# preamble_train: use same channel on num_preambles different preambles and store them to train with\n",
    "# inverse_train: contains channel_inverse per preamble, inverse is composed of [1/a0, -a1/a0, ...... -an/a0]\n",
    "channel_train = np.zeros((num_train,channel_length))\n",
    "preamble_train = np.zeros((num_train*num_preambles, preamble_length))\n",
    "inverse_train = np.zeros((num_train*num_preambles,channel_length))\n",
    "\n",
    "\n",
    "channel_test = np.zeros((num_train,channel_length))\n",
    "preamble_test = np.zeros((num_train*num_preambles, preamble_length))\n",
    "inverse_test = np.zeros((num_train*num_preambles,channel_length))\n",
    "\n",
    "for i in range(0, num_train):\n",
    "    channel_train[i,:]=np.random.uniform(0.2,1,channel_length)\n",
    "    # if the total power is greater than 1, then normalize\n",
    "    if sum(channel_train[i])>=1:\n",
    "        channel_train[i] = channel_train[i]/(sum(channel_train[i]))\n",
    "        \n",
    "    for k in range(num_preambles):\n",
    "        preamble_train[i+k,:] = sig.convolve(preambles[k], channel_train[i], mode='same')\n",
    "        inverse_train[i+k,0] = 1/channel_train[i,0]\n",
    "        for j in range(1, channel_length):\n",
    "            inverse_train[i+k,j] = -channel_train[i,j]/channel_train[i,0]\n",
    "\n",
    "        \n",
    "        \n",
    "for i in range(0, num_test):\n",
    "    channel_test[i,:]=np.random.uniform(0.2,1,channel_length)\n",
    "    # if the total power is greater than 1, then normalize\n",
    "    if sum(channel_test[i])>=1:\n",
    "        channel_test[i] = channel_test[i]/(sum(channel_test[i]))\n",
    "        \n",
    "    for k in range(num_preambles):\n",
    "        preamble_test[i+k,:] = sig.convolve(preambles[k], channel_test[i], mode='same')\n",
    "        inverse_test[i+k,0] = 1/channel_test[i,0]\n",
    "        for j in range(1, channel_length):\n",
    "            inverse_test[i+k,j] = -channel_test[i,j]/channel_test[i,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Cost 0.19186481833457947, MC Cost: 0.005473173689097166\n",
      "Epoch 100, Cost 0.020923946052789688, MC Cost: 0.0008548892801627517\n",
      "Epoch 200, Cost 0.011456516571342945, MC Cost: 0.0007086237310431898\n",
      "Epoch 300, Cost 0.022645557299256325, MC Cost: 0.0009990964317694306\n",
      "Epoch 400, Cost 0.019382866099476814, MC Cost: 0.0010039209155365825\n",
      "Epoch 500, Cost 0.009550025686621666, MC Cost: 0.0010812105610966682\n",
      "Epoch 600, Cost 0.006211585830897093, MC Cost: 0.0010736543918028474\n",
      "Epoch 700, Cost 0.015087827108800411, MC Cost: 0.0011320747435092926\n",
      "Epoch 800, Cost 0.012006004340946674, MC Cost: 0.0011656692950055003\n",
      "Epoch 900, Cost 0.001837265444919467, MC Cost: 0.0011306690284982324\n",
      "Epoch 1000, Cost 0.0025865298230201006, MC Cost: 0.0011016279458999634\n",
      "Epoch 1100, Cost 0.0020973174832761288, MC Cost: 0.0011620091972872615\n",
      "Epoch 1200, Cost 0.0006591113051399589, MC Cost: 0.0011808364652097225\n",
      "Epoch 1300, Cost 0.0018501713639125228, MC Cost: 0.0011766751995310187\n",
      "Epoch 1400, Cost 0.0006652083829976618, MC Cost: 0.001355501706711948\n",
      "Epoch 1500, Cost 0.0009563352214172482, MC Cost: 0.0012641478097066283\n",
      "Epoch 1600, Cost 0.0004996597417630255, MC Cost: 0.0011892799520865083\n",
      "Epoch 1700, Cost 0.0007939243223518133, MC Cost: 0.0012959266314283013\n",
      "Epoch 1800, Cost 0.00038795507862232625, MC Cost: 0.0012856109533458948\n",
      "Epoch 1900, Cost 0.00047530076699331403, MC Cost: 0.0012623048387467861\n",
      "Epoch 2000, Cost 0.00031672080513089895, MC Cost: 0.001217990298755467\n",
      "Epoch 2100, Cost 0.00015965044440235943, MC Cost: 0.0012222703080624342\n",
      "Epoch 2200, Cost 0.00021965122141409665, MC Cost: 0.0012409003684297204\n",
      "Epoch 2300, Cost 0.00030902621801942587, MC Cost: 0.0012683902168646455\n",
      "Epoch 2400, Cost 0.00028768263291567564, MC Cost: 0.0012047835625708103\n",
      "Epoch 2500, Cost 0.0001732817618176341, MC Cost: 0.001210292219184339\n",
      "Epoch 2600, Cost 0.00012198680633446202, MC Cost: 0.0012103929184377193\n",
      "Epoch 2700, Cost 0.0002397682110313326, MC Cost: 0.0012043975293636322\n",
      "Epoch 2800, Cost 3.4022970794467255e-05, MC Cost: 0.0011717653833329678\n",
      "Epoch 2900, Cost 0.0009100612951442599, MC Cost: 0.001193568343296647\n",
      "Epoch 3000, Cost 9.023742313729599e-05, MC Cost: 0.0011404023971408606\n",
      "Epoch 3100, Cost 0.0001016928072203882, MC Cost: 0.0011667687213048339\n",
      "Epoch 3200, Cost 0.00017612297961022705, MC Cost: 0.0012089250376448035\n",
      "Epoch 3300, Cost 0.00036246838863007724, MC Cost: 0.001138840802013874\n",
      "Epoch 3400, Cost 0.00011006298882421106, MC Cost: 0.0012077507562935352\n",
      "Epoch 3500, Cost 0.00047858705511316657, MC Cost: 0.0011835889890789986\n",
      "Epoch 3600, Cost 9.808737377170473e-05, MC Cost: 0.0011986736208200455\n",
      "Epoch 3700, Cost 0.00015843144501559436, MC Cost: 0.0011557331308722496\n",
      "Epoch 3800, Cost 2.2715888917446136e-05, MC Cost: 0.001206224667839706\n",
      "Epoch 3900, Cost 3.4054282878059894e-05, MC Cost: 0.0011687681544572115\n",
      "Epoch 4000, Cost 0.00012123933265684173, MC Cost: 0.0011493368074297905\n",
      "Epoch 4100, Cost 7.373074913630262e-05, MC Cost: 0.0011891876347362995\n",
      "Epoch 4200, Cost 6.652885349467397e-05, MC Cost: 0.0011992767686024308\n",
      "Epoch 4300, Cost 7.81510752858594e-05, MC Cost: 0.0011652681278064847\n",
      "Epoch 4400, Cost 0.00011167229968123138, MC Cost: 0.0011618140852078795\n",
      "Epoch 4500, Cost 3.0923540180083364e-05, MC Cost: 0.0011307636741548777\n",
      "Epoch 4600, Cost 6.791113992221653e-05, MC Cost: 0.0011044247075915337\n",
      "Epoch 4700, Cost 0.00013352162204682827, MC Cost: 0.0011471655452623963\n",
      "Epoch 4800, Cost 9.500589658273384e-05, MC Cost: 0.001209489768370986\n",
      "Epoch 4900, Cost 5.0894297601189464e-05, MC Cost: 0.0011558816768229008\n",
      "Epoch 5000, Cost 8.8823726400733e-05, MC Cost: 0.0011549383634701371\n",
      "Epoch 5100, Cost 5.749077899963595e-05, MC Cost: 0.001140353619121015\n",
      "Epoch 5200, Cost 2.3453212634194642e-05, MC Cost: 0.0011418901849538088\n",
      "Epoch 5300, Cost 2.2117335902294144e-05, MC Cost: 0.0011348845437169075\n",
      "Epoch 5400, Cost 3.057920912397094e-05, MC Cost: 0.0011529808398336172\n",
      "Epoch 5500, Cost 0.00015782291302457452, MC Cost: 0.0011785064125433564\n",
      "Epoch 5600, Cost 5.242819315753877e-05, MC Cost: 0.0011473981430754066\n",
      "Epoch 5700, Cost 5.44533395441249e-05, MC Cost: 0.001117944484576583\n",
      "Epoch 5800, Cost 1.1672581422317307e-05, MC Cost: 0.0011618435382843018\n",
      "Epoch 5900, Cost 2.1759580704383552e-05, MC Cost: 0.0011259684106335044\n",
      "Epoch 6000, Cost 1.595508547325153e-05, MC Cost: 0.0011158345732837915\n",
      "Epoch 6100, Cost 6.165356171550229e-05, MC Cost: 0.0011133152293041348\n",
      "Epoch 6200, Cost 4.1940547816921026e-05, MC Cost: 0.0011317552998661995\n",
      "Epoch 6300, Cost 8.298338070744649e-05, MC Cost: 0.0010971539886668324\n",
      "Epoch 6400, Cost 1.2850046005041804e-05, MC Cost: 0.0010935075115412474\n",
      "Epoch 6500, Cost 2.1768091755802743e-05, MC Cost: 0.0011144833406433463\n",
      "Epoch 6600, Cost 0.00012839293049182743, MC Cost: 0.0011282226769253612\n",
      "Epoch 6700, Cost 2.5907420422299765e-05, MC Cost: 0.0010970962466672063\n",
      "Epoch 6800, Cost 2.384688559686765e-05, MC Cost: 0.0010886563686653972\n",
      "Epoch 6900, Cost 7.14095076546073e-05, MC Cost: 0.0010849604150280356\n",
      "Epoch 7000, Cost 1.833787973737344e-05, MC Cost: 0.0010913512669503689\n",
      "Epoch 7100, Cost 7.411744263663422e-06, MC Cost: 0.0010875873267650604\n",
      "Epoch 7200, Cost 4.2507508624112234e-05, MC Cost: 0.0010806237114593387\n",
      "Epoch 7300, Cost 1.283795245399233e-05, MC Cost: 0.001084761111997068\n",
      "Epoch 7400, Cost 3.598876719479449e-05, MC Cost: 0.0010726165492087603\n",
      "Epoch 7500, Cost 1.2189042536192574e-05, MC Cost: 0.001071407925337553\n",
      "Epoch 7600, Cost 6.683631363557652e-05, MC Cost: 0.0010941873770207167\n",
      "Epoch 7700, Cost 5.517704630619846e-05, MC Cost: 0.001093985396437347\n",
      "Epoch 7800, Cost 1.197093752125511e-05, MC Cost: 0.0010665275622159243\n",
      "Epoch 7900, Cost 0.0001355509302811697, MC Cost: 0.0010612182086333632\n",
      "Epoch 8000, Cost 2.028589733527042e-05, MC Cost: 0.0010513417655602098\n",
      "Epoch 8100, Cost 8.24196649773512e-06, MC Cost: 0.0010500404750928283\n",
      "Epoch 8200, Cost 1.9083205188508146e-05, MC Cost: 0.0010490280110388994\n",
      "Epoch 8300, Cost 3.82456200895831e-05, MC Cost: 0.0010542006930336356\n",
      "Epoch 8400, Cost 1.92894040083047e-05, MC Cost: 0.0010919735068455338\n",
      "Epoch 8500, Cost 0.00022678727691527456, MC Cost: 0.0011338441399857402\n",
      "Epoch 8600, Cost 2.4367676815018058e-05, MC Cost: 0.0010400167666375637\n",
      "Epoch 8700, Cost 2.1841207853867672e-05, MC Cost: 0.0010503139346837997\n",
      "Epoch 8800, Cost 6.790437510062475e-06, MC Cost: 0.0010317483684048057\n",
      "Epoch 8900, Cost 2.3603508452652022e-05, MC Cost: 0.0010305186733603477\n",
      "Epoch 9000, Cost 3.8764246710343286e-05, MC Cost: 0.001062476192601025\n",
      "Epoch 9100, Cost 1.857684037531726e-05, MC Cost: 0.0010461590718477964\n",
      "Epoch 9200, Cost 5.80820778850466e-05, MC Cost: 0.0010372358374297619\n",
      "Epoch 9300, Cost 0.00033809695742093027, MC Cost: 0.0010818878654390574\n",
      "Epoch 9400, Cost 0.0001270975189981982, MC Cost: 0.0010751386871561408\n",
      "Epoch 9500, Cost 2.2986976546235383e-05, MC Cost: 0.0010101284133270383\n",
      "Epoch 9600, Cost 4.6248525904957205e-05, MC Cost: 0.001019443734548986\n",
      "Epoch 9700, Cost 1.8139358871849254e-05, MC Cost: 0.0010176640935242176\n",
      "Epoch 9800, Cost 2.5311639546998776e-05, MC Cost: 0.0010231136111542583\n",
      "Epoch 9900, Cost 3.918203674402321e-06, MC Cost: 0.0010050948476418853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n# start the session\\nwith tf.Session() as sess:\\n    #    initialize the variables\\n    sess.run(init)\\n    mc_losses = []\\n        \\n    for epoch in range(epochs):\\n        \\n        preamble_batch = preambles[0]\\n        real_inv_batch = [1,0]\\n        for rand in range(0,batch_size-1):\\n            rand_int = np.random.randint(0,num_train)\\n            preamble_batch = np.vstack((preamble_batch, preamble_train[rand_int]))\\n            real_inv_batch = np.vstack((real_inv_batch, inverse_train[rand_int]))\\n            \\n        preamble_batch = preamble_batch.reshape((batch_size, preamble_length))\\n        real_inv_batch = real_inv_batch.reshape((batch_size, channel_length))\\n        print(preamble_batch)\\n        print(real_inv_batch)\\n        \\n        _,cost,inverse_channel2 = sess.run([optimizer, cost, inverse_channel], \\n                                          feed_dict={recieved_preamble: preamble_batch, real_inverse: real_inv_batch,\\n                         adaptive_learning_rate: learning_rate * (decay**epoch)})\\n#         print(LA.norm(real_inv_batch-inverse_channel))\\n        \\n            \\n        if epoch % 100 == 0: \\n            plt.plot(epoch, cost, 'bo')\\n            print(inverse_test)\\n            mc_cost, mc_inversion = sess.run([cost, inverse_channel], \\n                                              feed_dict={recieved_preamble: preamble_test, real_inverse: inverse_test})\\n            print('Epoch {}, Cost {}, MC Cost: {}'.format(epoch, cost, mc_cost))\\n            \""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12813d550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fix a random seed to have the same results\n",
    "np.random.seed()\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 10000#10000\n",
    "batch_size = 1000\n",
    "test_averaging=100\n",
    "decay = 1 - 10*1e-10\n",
    "\n",
    "# placeholders for input and output\n",
    "adaptive_learning_rate = tf.placeholder_with_default(learning_rate, [])\n",
    "recieved_preamble = tf.placeholder(tf.float32, [None, preamble_length])\n",
    "real_inverse = tf.placeholder(tf.float32, [None, channel_length])\n",
    "\n",
    "layer1 = tf.contrib.layers.fully_connected(recieved_preamble, num_outputs=150, activation_fn=tf.nn.tanh)\n",
    "layer2 = tf.contrib.layers.fully_connected(layer1, num_outputs=150, activation_fn=tf.nn.tanh)\n",
    "layer3 = tf.contrib.layers.fully_connected(layer2, num_outputs=channel_length, activation_fn=tf.identity)\n",
    "\n",
    "inverse_channel = layer3\n",
    "\n",
    "cost_fn = tf.reduce_mean(tf.reduce_mean((real_inverse-inverse_channel)**2, axis=1))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=adaptive_learning_rate).minimize(cost_fn)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # for each batch start the batch with passing the first preamble through the identity channel\n",
    "        identity_channel = np.zeros(channel_length)\n",
    "        identity_channel[0] = 1\n",
    "        preamble_batch = sig.convolve(preambles[0], identity_channel, mode='same')\n",
    "        real_inv_batch = identity_channel\n",
    "        \n",
    "        for rand in range(0,batch_size-1):\n",
    "            rand_int = np.random.randint(0,num_train*num_preambles)\n",
    "            preamble_batch = np.vstack((preamble_batch, preamble_train[rand_int]))\n",
    "            real_inv_batch = np.vstack((real_inv_batch, inverse_train[rand_int]))\n",
    "            \n",
    "        preamble_batch.reshape((batch_size, preamble_length))\n",
    "        real_inv_batch.reshape((batch_size, channel_length))\n",
    "        \n",
    "        _,cost,inverse_channel2 = sess.run(\n",
    "            [optimizer, cost_fn, inverse_channel], \n",
    "            feed_dict={recieved_preamble: preamble_batch, \n",
    "                       real_inverse: real_inv_batch,adaptive_learning_rate: learning_rate * (decay**epoch)})\n",
    "        if epoch % 100 == 0: \n",
    "            plt.plot(epoch, cost, 'bo')\n",
    "            mc_cost, mc_inversion = sess.run(\n",
    "                [cost_fn, inverse_channel], feed_dict={recieved_preamble: preamble_test, real_inverse: inverse_test})\n",
    "            print('Epoch {}, Cost {}, MC Cost: {}'.format(epoch, cost, mc_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
