{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This IPython notebook defines several variations of convolutional neural networks for channel estimation. The training inputs are the preamble + preamble passed through channel; the predicted output is the channel taps that correspond to the input. We explore several ideas here:\n",
    "#### (A) multi-scale convolution (learned) filters applied separately to the [preamble input] and to the [preamble thorugh channel] input\n",
    "#### (B) multi-scale convolution (learned) filters applied to both (e.g., 2D convolution filters)\n",
    "\n",
    "We make several assumptions about the channel model here as well:\n",
    "* Channel length is <= 20\n",
    "* Channel energy (am I saying this correctly?) is 1 (also, does normalizing channel taps by l2 norm ensure this?)\n",
    "* Channel is sparse (most entries near 0, except for a few spikes)\n",
    "  * Potential simplifying assumption (maybe include initially?) first entry of channel is 'large'\n",
    "  \n",
    "  \n",
    "Questions: \n",
    "1. for my preamble, I am using +/- 1; Nikhil used 1/0 .. which is correct? (It should not matter really for training/testing since it is a simple affine transform between the two, but I want to do the \"correct\" thing)\n",
    "2. do my assumptions make sense? for a real model I mean\n",
    "3. am I adding noise correctly for the SNR I am setting\n",
    "4. More of a \"TODO\" but...I am only training and testing on preamble inputs, not additional data -- the reasoning is that for additional data, we really want something that handles sequences (e.g., and RNN) in my opinion and this is more of an exploratoration of convolutional layers here\n",
    "\n",
    "## ALSO NOTE: I am making a lot of things very modular on purpose..I want to discuss with everyone the problem statement again (I still feel like a lot of things are unclear/ambiguous) and then we can move a lot of this modular code to a rigid \"util.py\" file that everyone should import from so that we can more easily guarantee correctness and consistency and speed up development time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions...we really should standardize this in a Python file [TODO!!!!]\n",
    "\"\"\"Generates random sequence [1 1 1 -1 1 -1 -1 ...] of length LENGTH.\"\"\"\n",
    "def gen_preamble(length=100):\n",
    "    return np.random.randint(2, size=(1,length))*2 - 1\n",
    "\n",
    "\"\"\"Generates N channels of length LENGTH, each with NUM_TAPS taps. This\n",
    "   means that NUM_TAPS of the entries will be non-zero, and the rest will\n",
    "   be 'close' to 0 (e.g., noise). \n",
    "   Example below.\n",
    "   \n",
    "   >>> np.around(gen_channel(),2)\n",
    "   >>> array([[-0.08,  0.  , -0.06,  0.02,  0.  ,  0.02, -0.85,  0.05, -0.03,\n",
    "        -0.07,  0.5 , -0.02, -0.  , -0.05, -0.  ,  0.03, -0.07, -0.04,\n",
    "        -0.01,  0.08]])\"\"\"\n",
    "def gen_channel(N=1,num_taps=2,length=20):\n",
    "    ret = np.zeros((N, length))\n",
    "    tap_idxs = np.random.randint(length, size=(N, num_taps))\n",
    "    tap_vals = ((np.random.randint(10, size=(N, num_taps))+1)*(np.random.randint(2, size=(N, num_taps))*2 - 1)) / 10.\n",
    "    for i in range(N):\n",
    "        np.put(ret[i], tap_idxs[i], tap_vals[i])\n",
    "    ret += 5e-2*np.random.randn(N,length)\n",
    "    return ret / np.linalg.norm(ret,axis=1,keepdims=True)\n",
    "\n",
    "\"\"\"Simulates passing data through a noisy channel.\n",
    "   If SNR == -1, then no noise. Otherwise, uses AWGN model.\n",
    "   \n",
    "   Returned value has shape (1, len(channel.T) + len(data.T) - 1).\n",
    "   With default settings, this means it is (1, 119).\"\"\"\n",
    "def apply_channel(channel, data, snr=-1):\n",
    "    ret = sig.convolve(data, channel, mode='full')\n",
    "    if snr > 0:\n",
    "        ret += (1./np.sqrt(snr)) * np.random.randn(len(ret))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for networks..should also put this in util.py!\n",
    "\"\"\"Run before building a new network. Rests randomization for repeatability.\"\"\"\n",
    "def reset():\n",
    "    tf.reset_default_graph()\n",
    "    np.random.seed(0)\n",
    "    tf.set_random_seed(0)\n",
    "    \n",
    "\"\"\"Defines the loss function.\"\"\"\n",
    "def define_loss(placeholders, loss_type):\n",
    "    output, correct_output = placeholders\n",
    "    return tf.reduce_mean(tf.reduce_sum((output-correct_output)**2, axis=1))\n",
    "    \n",
    "\"\"\"Defines the optimizer.\"\"\"\n",
    "def define_optimizer(loss, trainable_weights, optimizer, lr):\n",
    "    opt = tf.train.AdamOptimizer(lr)\n",
    "    gradients = opt.compute_gradients(loss, trainable_weights)\n",
    "    train_step = opt.apply_gradients(gradients)\n",
    "    return train_step\n",
    "\n",
    "\"\"\"Defines a trainable variable with truncated normal initialization.\"\"\"\n",
    "def define_variable(name, shape, stddev):\n",
    "    var = tf.get_variable(name, shape, initializer=\n",
    "                    tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32),\n",
    "                    dtype=tf.float32)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the networks\n",
    "\"\"\"Builds the network [model 1] -- a basic convolution network; use as a base\n",
    "   for the next network models.\n",
    "   Elements in PARAMS:\n",
    "   \n",
    "   * 'preamble_len' : length of preamble; [default = 100]\n",
    "   * 'channel_len' : length of channel; [default = 20]\n",
    "   * 'use_max_pool': True to use max pooling in first part of net; [default = False]\n",
    "   * 'loss' : loss function to use\n",
    "   * 'optimizer' : optimizer to use\n",
    "   * 'lr' : base learning rate\n",
    "   \n",
    "   \"\"\"\n",
    "def build_network1(params=None):\n",
    "    if params == None:\n",
    "        params = {'preamble_len':100, 'channel_len':20,\n",
    "                  'use_max_pool':False, 'loss':\"\", 'optimizer':\"\", 'lr':4e-5}\n",
    "    preamble = tf.placeholder(tf.float32, [None, params['preamble_len'], 1], name=\"preamble_input\")\n",
    "    # use same length as preamble as per discussion on April 12\n",
    "    received = tf.placeholder(tf.float32, [None, params['preamble_len'], 1], name=\"received_preamble\")\n",
    "    channel_true = tf.placeholder(tf.float32, [None, params['channel_len']])\n",
    "    \n",
    "    inputs=[preamble,received,channel_true]\n",
    "    outputs=[]\n",
    "    weights=[]\n",
    "    \n",
    "    nets=[preamble,received]\n",
    "    \n",
    "    # Process PREAMBLE and RECEIVED separately through convolutions\n",
    "    num_filters = [1, 30, 30, 10]\n",
    "    for i in [1,2]:\n",
    "        net = nets[i-1]\n",
    "        for j in range(1, len(num_filters)):\n",
    "            num_filter = num_filters[j]\n",
    "            prev = num_filters[j-1]\n",
    "            with tf.variable_scope(\"conv%d_%d\" % (j+1, i)) as scope:\n",
    "                # use same weight initializer for all, and always use 3x_ convolutions\n",
    "                kernel = define_variable('conv_weights', [3, prev, num_filter], 5e-2)\n",
    "                biases = define_variable('conv_biases', [num_filter], 5e-3)\n",
    "                weights.extend([kernel, biases])\n",
    "                # apply network\n",
    "                net = tf.nn.conv1d(net, kernel, stride=1, padding='SAME')\n",
    "                net = tf.nn.bias_add(net, biases)\n",
    "                net = tf.nn.relu(net)\n",
    "                if params['use_max_pool']:\n",
    "                    net = tf.nn.max_pool(net, [1, 3, 1], [1, 2, 1], padding='SAME')\n",
    "        nets[i-1] = net\n",
    "        \n",
    "    # Concatenate\n",
    "    output = tf.concat(nets, axis=1)\n",
    "    with tf.variable_scope(\"conv1_concat\") as scope:\n",
    "        kernel = define_variable('conv_weights', [3, num_filters[-1], 10], 5e-2)\n",
    "        biases = define_variable('conv_biases', [10], 5e-3)\n",
    "        weights.extend([kernel, biases])\n",
    "        # apply network\n",
    "        net = tf.nn.conv1d(net, kernel, stride=1, padding='SAME')\n",
    "        net = tf.nn.bias_add(net, biases)\n",
    "        net = tf.nn.relu(net)\n",
    "    with tf.variable_scope(\"fc2_concat\") as scope:\n",
    "        dim = output.get_shape()[1].value*output.get_shape()[2].value\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        \n",
    "        kernel = define_variable('conv_weights', [dim, params['channel_len']], 5e-2)\n",
    "        biases = define_variable('conv_biases', [params['channel_len']], 5e-3)\n",
    "        weights.extend([kernel, biases])\n",
    "        # apply network\n",
    "        output = tf.reshape(output, [batch_size, -1])\n",
    "        output = tf.nn.sigmoid(tf.matmul(output, kernel) + biases)\n",
    "    \n",
    "    outputs=[output]\n",
    "    \n",
    "    loss = define_loss([output, channel_true], params['loss'])\n",
    "    train = define_optimizer(loss, weights, params['optimizer'], params['lr'])\n",
    "\n",
    "    return inputs, outputs, weights, loss, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train network\n",
    "reset()\n",
    "inputs, outputs, weights, loss, train = build_network1()\n",
    "num_iter=10000\n",
    "batch_size=100\n",
    "# use a single fixed preamble\n",
    "preamble=gen_preamble()\n",
    "for i in range(0,num_iter):\n",
    "    channels = gen_channel(N=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
