{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This IPython notebook defines several variations of convolutional neural networks for channel estimation. The training inputs are the preamble + preamble passed through channel; the predicted output is the channel taps that correspond to the input. We explore several ideas here:\n",
    "#### (A) multi-scale convolution (learned) filters applied separately to the [preamble input] and to the [preamble thorugh channel] input\n",
    "#### (B) multi-scale convolution (learned) filters applied to both (e.g., 2D convolution filters)\n",
    "\n",
    "We make several assumptions about the channel model here as well:\n",
    "* Channel length is <= 20\n",
    "* Channel energy (am I saying this correctly?) is 1 (also, does normalizing channel taps by l2 norm ensure this?)\n",
    "* Channel is sparse (most entries near 0, except for a few spikes)\n",
    "  * Potential simplifying assumption (maybe include initially?) first entry of channel is 'large'\n",
    "  \n",
    "  \n",
    "Questions: \n",
    "1. for my preamble, I am using +/- 1; Nikhil used 1/0 .. which is correct? (It should not matter really for training/testing since it is a simple affine transform between the two, but I want to do the \"correct\" thing)\n",
    "2. do my assumptions make sense? for a real model I mean\n",
    "3. am I adding noise correctly for the SNR I am setting\n",
    "4. More of a \"TODO\" but...I am only training and testing on preamble inputs, not additional data -- the reasoning is that for additional data, we really want something that handles sequences (e.g., and RNN) in my opinion and this is more of an exploratoration of convolutional layers here\n",
    "\n",
    "## ALSO NOTE: I am making a lot of things very modular on purpose..I want to discuss with everyone the problem statement again (I still feel like a lot of things are unclear/ambiguous) and then we can move a lot of this modular code to a rigid \"util.py\" file that everyone should import from so that we can more easily guarantee correctness and consistency and speed up development time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions...we really should standardize this in a Python file [TODO!!!!]\n",
    "\"\"\"Generates random sequence [1 1 1 -1 1 -1 -1 ...] of length LENGTH.\"\"\"\n",
    "def gen_preamble(length=100):\n",
    "    return np.random.randint(2, size=(1,length))*2 - 1\n",
    "\n",
    "\"\"\"Generates N channels of length LENGTH, each with NUM_TAPS taps. This\n",
    "   means that NUM_TAPS of the entries will be non-zero, and the rest will\n",
    "   be 'close' to 0 (e.g., noise). \n",
    "   Example below.\n",
    "   \n",
    "   >>> np.around(gen_channel(),2)\n",
    "   >>> array([[-0.08,  0.  , -0.06,  0.02,  0.  ,  0.02, -0.85,  0.05, -0.03,\n",
    "        -0.07,  0.5 , -0.02, -0.  , -0.05, -0.  ,  0.03, -0.07, -0.04,\n",
    "        -0.01,  0.08]])\"\"\"\n",
    "def gen_channel(N=1,num_taps=2,length=20):\n",
    "    ret = np.zeros((N, length))\n",
    "    tap_idxs = np.random.randint(length, size=(N, num_taps))\n",
    "    tap_vals = ((np.random.randint(10, size=(N, num_taps))+1)*\\\n",
    "                (np.random.randint(2, size=(N, num_taps))*2 - 1))\\\n",
    "                / 10.\n",
    "    for i in range(N):\n",
    "        np.put(ret[i], tap_idxs[i], tap_vals[i])\n",
    "    ret += 5e-2*np.random.randn(N,length)\n",
    "    return ret / np.linalg.norm(ret,axis=1,keepdims=True)\n",
    "\n",
    "\"\"\"Simulates passing data through a noisy channel.\n",
    "   If SNR == -1, then no noise. Otherwise, uses AWGN model.\n",
    "   \n",
    "   Returned value has shape (1, len(channel.T) + len(data.T) - 1).\n",
    "   With default settings, this means it is (1, 119).\"\"\"\n",
    "def apply_channel(channel, data, snr=-1):\n",
    "    ret = sig.convolve(data, channel, mode='full')\n",
    "    if snr > 0:\n",
    "        ret += (1./np.sqrt(snr)) * np.random.randn(len(ret))\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for networks..should also put this in util.py!\n",
    "\"\"\"Run before building a new network. Rests randomization for repeatability.\"\"\"\n",
    "def reset():\n",
    "    tf.reset_default_graph()\n",
    "    np.random.seed(0)\n",
    "    tf.set_random_seed(0)\n",
    "    \n",
    "\"\"\"Defines the loss function.\"\"\"\n",
    "def define_loss(placeholders, loss_type):\n",
    "    output, correct_output = placeholders\n",
    "    return tf.reduce_mean(tf.reduce_sum((output-correct_output)**2, axis=1))\n",
    "    \n",
    "\"\"\"Defines the optimizer.\"\"\"\n",
    "def define_optimizer(loss, trainable_weights, optimizer, lr):\n",
    "    opt = tf.train.AdamOptimizer(lr)\n",
    "    gradients = opt.compute_gradients(loss, trainable_weights)\n",
    "    train_step = opt.apply_gradients(gradients)\n",
    "    return train_step\n",
    "\n",
    "\"\"\"Defines a trainable variable with truncated normal initialization.\"\"\"\n",
    "def define_variable(name, shape, stddev):\n",
    "    var = tf.get_variable(name, shape, initializer=\n",
    "                    tf.truncated_normal_initializer(stddev=stddev, dtype=tf.float32),\n",
    "                    dtype=tf.float32)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the networks\n",
    "\"\"\"Builds the network [model 1] -- a basic convolution network; use as a base\n",
    "   for the next network models.\n",
    "   Elements in PARAMS:\n",
    "   \n",
    "   * 'preamble_len' : length of preamble; [default = 100]\n",
    "   * 'channel_len' : length of channel; [default = 20]\n",
    "   * 'use_max_pool': True to use max pooling in first part of net; [default = False]\n",
    "   * 'loss' : loss function to use\n",
    "   * 'optimizer' : optimizer to use\n",
    "   * 'lr' : base learning rate\n",
    "   \n",
    "   \"\"\"\n",
    "def build_network1(params=None):\n",
    "    if params == None:\n",
    "        params = {'preamble_len':100, 'channel_len':20,\n",
    "                  'use_max_pool':False, 'loss':\"\", 'optimizer':\"\", 'lr':4e-5}\n",
    "    preamble = tf.placeholder(tf.float32, [1, params['preamble_len'], 1], name=\"preamble_input\")\n",
    "    # use same length as preamble as per discussion on April 12\n",
    "    received = tf.placeholder(tf.float32, [None, params['preamble_len'], 1], name=\"received_preamble\")\n",
    "    channel_true = tf.placeholder(tf.float32, [None, params['channel_len']])\n",
    "    batch_size = tf.shape(received)[0]\n",
    "    \n",
    "    inputs=[preamble,received,channel_true]\n",
    "    outputs=[]\n",
    "    weights=[]\n",
    "    \n",
    "    nets=[preamble,received]\n",
    "    \n",
    "    # Process PREAMBLE and RECEIVED separately through convolutions\n",
    "    num_filters = [1, 30, 30, 10]\n",
    "    for i in [1,2]:\n",
    "        net = nets[i-1]\n",
    "        for j in range(1, len(num_filters)):\n",
    "            num_filter = num_filters[j]\n",
    "            prev = num_filters[j-1]\n",
    "            with tf.variable_scope(\"conv%d_%d\" % (j+1, i)) as scope:\n",
    "                # use same weight initializer for all, and always use 3x_ convolutions\n",
    "                kernel = define_variable('conv_weights', [3, prev, num_filter], 5e-2)\n",
    "                biases = define_variable('conv_biases', [num_filter], 5e-3)\n",
    "                weights.extend([kernel, biases])\n",
    "                # apply network\n",
    "                net = tf.nn.conv1d(net, kernel, stride=1, padding='SAME')\n",
    "                net = tf.nn.bias_add(net, biases)\n",
    "                net = tf.nn.relu(net)\n",
    "                if params['use_max_pool']:\n",
    "                    net = tf.nn.max_pool(net, [1, 3, 1], [1, 2, 1], padding='SAME')\n",
    "        nets[i-1] = net\n",
    "        \n",
    "    # Concatenate\n",
    "    nets[0] = tf.tile(nets[0], [batch_size, 1, 1])\n",
    "    output = tf.concat(nets, axis=1)\n",
    "    with tf.variable_scope(\"conv1_concat\") as scope:\n",
    "        kernel = define_variable('conv_weights', [3, num_filters[-1], 10], 5e-2)\n",
    "        biases = define_variable('conv_biases', [10], 5e-3)\n",
    "        weights.extend([kernel, biases])\n",
    "        # apply network\n",
    "        net = tf.nn.conv1d(net, kernel, stride=1, padding='SAME')\n",
    "        net = tf.nn.bias_add(net, biases)\n",
    "        net = tf.nn.relu(net)\n",
    "    with tf.variable_scope(\"fc2_concat\") as scope:\n",
    "        dim = output.get_shape()[1].value*output.get_shape()[2].value\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        \n",
    "        kernel = define_variable('conv_weights', [dim, params['channel_len']], 5e-2)\n",
    "        biases = define_variable('conv_biases', [params['channel_len']], 5e-3)\n",
    "        weights.extend([kernel, biases])\n",
    "        # apply network\n",
    "        output = tf.reshape(output, [batch_size, -1])\n",
    "        output = tf.nn.sigmoid(tf.matmul(output, kernel) + biases)\n",
    "    \n",
    "    outputs=[output]\n",
    "    \n",
    "    loss = define_loss([output, channel_true], params['loss'])\n",
    "    train = define_optimizer(loss, weights, params['optimizer'], params['lr'])\n",
    "\n",
    "    return inputs, outputs, weights, loss, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.0226917\n",
      "10 6.0540795\n",
      "20 6.153272\n",
      "30 5.959567\n",
      "40 6.021763\n",
      "50 5.864655\n",
      "60 5.6800714\n",
      "70 5.734802\n",
      "80 5.6470795\n",
      "90 5.625172\n",
      "100 5.304321\n",
      "110 5.3177366\n",
      "120 4.884723\n",
      "130 4.5628405\n",
      "140 4.402691\n",
      "150 3.9965124\n",
      "160 3.604062\n",
      "170 3.291089\n",
      "180 2.796199\n",
      "190 2.4410746\n",
      "200 2.147702\n",
      "210 1.8360114\n",
      "220 1.6814902\n",
      "230 1.5715724\n",
      "240 1.4261998\n",
      "250 1.3141888\n",
      "260 1.2320234\n",
      "270 1.2061732\n",
      "280 1.1661541\n",
      "290 1.1247128\n",
      "300 1.1252065\n",
      "310 1.1267849\n",
      "320 1.0958221\n",
      "330 1.0722934\n",
      "340 1.054806\n",
      "350 1.0763851\n",
      "360 1.0445817\n",
      "370 1.0490582\n",
      "380 1.047296\n",
      "390 1.0449301\n",
      "400 1.0428039\n",
      "410 1.0282346\n",
      "420 1.028836\n",
      "430 1.0342497\n",
      "440 1.0379522\n",
      "450 1.0285667\n",
      "460 1.0359975\n",
      "470 1.0195144\n",
      "480 1.0244099\n",
      "490 1.0278993\n",
      "500 1.0182813\n",
      "510 1.0241394\n",
      "520 1.0141172\n",
      "530 1.0180893\n",
      "540 1.0145458\n",
      "550 1.014996\n",
      "560 1.0219283\n",
      "570 1.0192297\n",
      "580 1.0140438\n",
      "590 1.0111045\n",
      "600 1.0115474\n",
      "610 1.008149\n",
      "620 1.011463\n",
      "630 1.0092558\n",
      "640 1.0146624\n",
      "650 1.0041976\n",
      "660 1.007657\n",
      "670 1.0085491\n",
      "680 1.0059404\n",
      "690 1.0120081\n",
      "700 1.0077308\n",
      "710 1.0080199\n",
      "720 1.0091538\n",
      "730 1.0029987\n",
      "740 0.9989605\n",
      "750 1.004403\n",
      "760 1.0036945\n",
      "770 1.0024381\n",
      "780 1.0059371\n",
      "790 1.0080476\n",
      "800 1.0039119\n",
      "810 1.0109948\n",
      "820 1.0034192\n",
      "830 1.0048296\n",
      "840 1.0050496\n",
      "850 1.0095146\n",
      "860 1.0050483\n",
      "870 1.0073091\n",
      "880 1.001306\n",
      "890 1.0021889\n",
      "900 1.0051587\n",
      "910 1.0031483\n",
      "920 1.0000521\n",
      "930 1.0036236\n",
      "940 1.0043437\n",
      "950 1.0077665\n",
      "960 1.003537\n",
      "970 1.0049707\n",
      "980 1.0057875\n",
      "990 1.0039234\n",
      "1000 1.0005933\n",
      "1010 1.0041927\n",
      "1020 0.9992818\n",
      "1030 1.0020074\n",
      "1040 0.999734\n",
      "1050 1.0005757\n",
      "1060 1.0065978\n",
      "1070 1.0052019\n",
      "1080 1.0034977\n",
      "1090 0.9997581\n",
      "1100 1.00327\n",
      "1110 1.0025516\n",
      "1120 1.0033997\n",
      "1130 1.0007956\n",
      "1140 1.0026764\n",
      "1150 1.0033145\n",
      "1160 1.0037224\n",
      "1170 1.0075481\n",
      "1180 1.0011371\n",
      "1190 1.0040033\n",
      "1200 1.0042236\n",
      "1210 0.99910337\n",
      "1220 1.0028119\n",
      "1230 1.0002923\n",
      "1240 1.0004704\n",
      "1250 0.99892944\n",
      "1260 1.0021433\n",
      "1270 1.0008993\n",
      "1280 1.0009457\n",
      "1290 1.0045307\n",
      "1300 1.0048797\n",
      "1310 0.99975675\n",
      "1320 1.0001241\n",
      "1330 1.0026562\n",
      "1340 1.0044826\n",
      "1350 1.0012825\n",
      "1360 0.9998238\n",
      "1370 1.0020075\n",
      "1380 1.0029495\n",
      "1390 1.0004305\n",
      "1400 0.99989355\n",
      "1410 1.0008296\n",
      "1420 1.0010519\n",
      "1430 1.0022472\n",
      "1440 1.0033784\n",
      "1450 1.0019456\n",
      "1460 1.0025129\n",
      "1470 1.002574\n",
      "1480 1.0012522\n",
      "1490 1.0011817\n",
      "1500 0.99944425\n",
      "1510 0.99891835\n",
      "1520 1.0001751\n",
      "1530 1.0027344\n",
      "1540 1.0033938\n",
      "1550 1.0019171\n",
      "1560 1.0011744\n",
      "1570 1.0003967\n",
      "1580 1.0027659\n",
      "1590 0.9999291\n",
      "1600 1.0005208\n",
      "1610 1.0026251\n",
      "1620 1.0032285\n",
      "1630 1.0005056\n",
      "1640 1.002442\n",
      "1650 1.0006998\n",
      "1660 1.000829\n",
      "1670 1.0033215\n",
      "1680 0.9976118\n",
      "1690 1.0019617\n",
      "1700 0.9996629\n",
      "1710 1.0023254\n",
      "1720 0.99914193\n",
      "1730 0.9993327\n",
      "1740 1.0014604\n",
      "1750 0.9987626\n",
      "1760 1.0002109\n",
      "1770 1.001534\n",
      "1780 1.0013592\n",
      "1790 1.0002289\n",
      "1800 1.0007538\n",
      "1810 0.99959165\n",
      "1820 1.0004005\n",
      "1830 1.000213\n",
      "1840 1.0022616\n",
      "1850 1.0006043\n",
      "1860 0.9996216\n",
      "1870 0.9995542\n",
      "1880 0.9995875\n",
      "1890 1.001167\n",
      "1900 1.0027753\n",
      "1910 1.0005828\n",
      "1920 1.0017366\n",
      "1930 1.0014733\n",
      "1940 1.001651\n",
      "1950 1.0009981\n",
      "1960 0.9997927\n",
      "1970 1.0021052\n",
      "1980 1.0014328\n",
      "1990 1.0011909\n",
      "2000 1.0014112\n",
      "2010 1.0031382\n",
      "2020 1.0005705\n",
      "2030 1.0020293\n",
      "2040 1.0007257\n",
      "2050 1.0013355\n",
      "2060 1.0001882\n",
      "2070 0.9999787\n",
      "2080 1.0015461\n",
      "2090 0.9996508\n",
      "2100 1.0011277\n",
      "2110 1.0002303\n",
      "2120 1.0006807\n",
      "2130 0.9999249\n",
      "2140 0.99964887\n",
      "2150 1.0008037\n",
      "2160 1.0002972\n",
      "2170 1.0015393\n",
      "2180 1.001539\n",
      "2190 1.0007577\n",
      "2200 1.0010384\n",
      "2210 0.99902236\n",
      "2220 1.0012254\n",
      "2230 1.0015427\n",
      "2240 0.99972445\n",
      "2250 1.0016437\n",
      "2260 0.9991051\n",
      "2270 0.9999633\n",
      "2280 1.0000087\n",
      "2290 0.9999172\n",
      "2300 0.99905205\n",
      "2310 0.99998355\n",
      "2320 1.000769\n",
      "2330 1.0003492\n",
      "2340 1.0010275\n",
      "2350 1.0003006\n",
      "2360 1.001523\n",
      "2370 1.0010306\n",
      "2380 0.99936676\n",
      "2390 1.0014064\n",
      "2400 1.0000527\n",
      "2410 1.0000105\n",
      "2420 1.000993\n",
      "2430 0.9999022\n",
      "2440 0.9994231\n",
      "2450 0.9994834\n",
      "2460 0.9994589\n",
      "2470 0.99959165\n",
      "2480 1.0003577\n",
      "2490 0.9986156\n",
      "2500 0.9990248\n",
      "2510 1.0002637\n",
      "2520 1.0026332\n",
      "2530 1.0008835\n",
      "2540 0.9995209\n",
      "2550 0.99919236\n",
      "2560 1.0000881\n",
      "2570 0.99938846\n",
      "2580 1.0016426\n",
      "2590 1.0006245\n",
      "2600 0.99991584\n",
      "2610 1.0008534\n",
      "2620 1.0002232\n",
      "2630 1.0004532\n",
      "2640 1.0000129\n",
      "2650 0.9997165\n",
      "2660 1.0006157\n",
      "2670 1.0010729\n",
      "2680 1.0007037\n",
      "2690 1.0013984\n",
      "2700 0.9988313\n",
      "2710 1.0002697\n",
      "2720 1.0005318\n",
      "2730 1.0012531\n",
      "2740 1.000132\n",
      "2750 0.9997403\n",
      "2760 1.0005306\n",
      "2770 1.001193\n",
      "2780 1.0001044\n",
      "2790 1.0008881\n",
      "2800 1.0004373\n",
      "2810 1.0001098\n",
      "2820 1.0021894\n",
      "2830 0.99819845\n",
      "2840 0.999954\n",
      "2850 0.99996066\n",
      "2860 0.99901915\n",
      "2870 1.0007252\n",
      "2880 1.0009162\n",
      "2890 1.0008438\n",
      "2900 0.9995243\n",
      "2910 1.0005658\n",
      "2920 0.99941117\n",
      "2930 1.0002674\n",
      "2940 1.0002114\n",
      "2950 0.9984615\n",
      "2960 0.99952394\n",
      "2970 0.99966085\n",
      "2980 1.0007372\n",
      "2990 1.0005518\n",
      "3000 1.0002377\n",
      "3010 1.000089\n",
      "3020 0.99942374\n",
      "3030 1.0006909\n",
      "3040 0.9983959\n",
      "3050 0.9999603\n",
      "3060 1.0013282\n",
      "3070 0.9980866\n",
      "3080 1.0003651\n",
      "3090 1.0004013\n",
      "3100 0.9996774\n",
      "3110 0.9997259\n",
      "3120 0.99907094\n",
      "3130 1.0004785\n",
      "3140 1.0000355\n",
      "3150 0.99904805\n",
      "3160 0.9994299\n",
      "3170 0.9996784\n",
      "3180 1.0005245\n",
      "3190 1.0001323\n",
      "3200 1.0005393\n",
      "3210 1.0009737\n",
      "3220 1.0007069\n",
      "3230 1.0001704\n",
      "3240 1.0001802\n",
      "3250 1.0002338\n",
      "3260 0.9989502\n",
      "3270 1.000258\n",
      "3280 0.99961454\n",
      "3290 1.0007597\n",
      "3300 0.9988016\n",
      "3310 0.99976563\n",
      "3320 1.0002447\n",
      "3330 0.9987344\n",
      "3340 1.0003456\n",
      "3350 1.0005035\n",
      "3360 0.99950814\n",
      "3370 0.999823\n",
      "3380 0.99957967\n",
      "3390 0.99872047\n",
      "3400 1.000726\n",
      "3410 1.0000744\n",
      "3420 0.99961305\n",
      "3430 1.0004817\n",
      "3440 0.9998022\n",
      "3450 1.0003104\n",
      "3460 1.0000495\n",
      "3470 1.0004325\n",
      "3480 0.9999095\n",
      "3490 0.9990309\n",
      "3500 1.0001075\n",
      "3510 0.99993205\n",
      "3520 0.99958855\n",
      "3530 0.9997097\n",
      "3540 1.0002759\n",
      "3550 1.0007614\n",
      "3560 0.9998096\n",
      "3570 0.99908936\n",
      "3580 0.9998766\n",
      "3590 1.0002197\n",
      "3600 0.9999407\n",
      "3610 1.0000061\n",
      "3620 0.9999447\n",
      "3630 0.99968123\n",
      "3640 0.99914515\n",
      "3650 1.0000342\n",
      "3660 1.0005606\n",
      "3670 0.9994915\n",
      "3680 0.9993853\n",
      "3690 0.9996202\n",
      "3700 0.9995497\n",
      "3710 1.0000437\n",
      "3720 1.0006529\n",
      "3730 1.0011979\n",
      "3740 0.9999485\n",
      "3750 0.9999733\n",
      "3760 1.000459\n",
      "3770 0.99939257\n",
      "3780 0.9995918\n",
      "3790 0.99998724\n",
      "3800 0.99947447\n",
      "3810 0.99923676\n",
      "3820 0.99998605\n",
      "3830 0.9997177\n",
      "3840 1.0000312\n",
      "3850 0.99960107\n",
      "3860 0.9986414\n",
      "3870 1.0000011\n",
      "3880 0.99957585\n",
      "3890 0.9990288\n",
      "3900 0.9999686\n",
      "3910 0.99923617\n",
      "3920 0.9997493\n",
      "3930 0.9993031\n",
      "3940 1.0001518\n",
      "3950 0.99978614\n",
      "3960 0.99852717\n",
      "3970 0.99912626\n",
      "3980 1.0009745\n",
      "3990 0.99999475\n",
      "4000 1.0003247\n",
      "4010 1.000271\n",
      "4020 1.0004239\n",
      "4030 0.99985075\n",
      "4040 0.9985374\n",
      "4050 1.0000556\n",
      "4060 0.99942\n",
      "4070 1.0001135\n",
      "4080 1.0006932\n",
      "4090 0.99884504\n",
      "4100 0.9996779\n",
      "4110 0.9997432\n",
      "4120 0.9996648\n",
      "4130 0.99991435\n",
      "4140 0.9998231\n",
      "4150 0.9993919\n",
      "4160 0.99934417\n",
      "4170 0.9987994\n",
      "4180 0.9999709\n",
      "4190 1.0001521\n",
      "4200 0.9987915\n",
      "4210 0.9995565\n",
      "4220 1.0001854\n",
      "4230 0.999758\n",
      "4240 0.99988467\n",
      "4250 0.9988624\n",
      "4260 1.0001589\n",
      "4270 0.9998961\n",
      "4280 0.99966455\n",
      "4290 1.0000141\n",
      "4300 0.9985763\n",
      "4310 0.99803436\n",
      "4320 0.99987185\n",
      "4330 0.99987817\n",
      "4340 0.9992076\n",
      "4350 0.99992263\n",
      "4360 0.99831545\n",
      "4370 1.0001432\n",
      "4380 0.9981202\n",
      "4390 0.9996825\n",
      "4400 0.9999327\n",
      "4410 0.9999807\n",
      "4420 0.99902266\n",
      "4430 1.0001005\n",
      "4440 1.00068\n",
      "4450 0.999308\n",
      "4460 0.99956125\n",
      "4470 0.999625\n",
      "4480 1.0004954\n",
      "4490 0.99974394\n",
      "4500 0.99969935\n",
      "4510 1.0006051\n",
      "4520 0.9989111\n",
      "4530 0.9999754\n",
      "4540 0.99931854\n",
      "4550 0.9990057\n",
      "4560 0.99976015\n",
      "4570 0.9990834\n",
      "4580 0.9997662\n",
      "4590 0.99997985\n",
      "4600 0.9987754\n",
      "4610 0.9983995\n",
      "4620 0.99896723\n",
      "4630 0.9991406\n",
      "4640 0.9981803\n",
      "4650 0.99967927\n",
      "4660 0.99900377\n",
      "4670 0.99885803\n",
      "4680 0.99938273\n",
      "4690 0.9986847\n",
      "4700 0.9992561\n",
      "4710 0.999643\n",
      "4720 1.000465\n",
      "4730 0.9988487\n",
      "4740 0.9994852\n",
      "4750 0.99952406\n",
      "4760 0.99786055\n",
      "4770 0.99831694\n",
      "4780 0.9983321\n",
      "4790 0.99930555\n",
      "4800 0.9988728\n",
      "4810 0.9993238\n",
      "4820 0.99775535\n",
      "4830 0.9991356\n",
      "4840 0.9981993\n",
      "4850 0.99928886\n",
      "4860 0.9996213\n",
      "4870 0.9999231\n",
      "4880 0.99870497\n",
      "4890 0.9985983\n",
      "4900 0.99909174\n",
      "4910 0.9985265\n",
      "4920 0.99920815\n",
      "4930 0.9995204\n",
      "4940 0.9958399\n",
      "4950 0.9984469\n",
      "4960 0.9991664\n",
      "4970 0.9987967\n",
      "4980 0.99866414\n",
      "4990 0.99802756\n",
      "5000 0.9987921\n",
      "5010 0.99929446\n",
      "5020 0.998552\n",
      "5030 0.9965379\n",
      "5040 0.99914855\n",
      "5050 0.99881244\n",
      "5060 0.99714166\n",
      "5070 1.0000819\n",
      "5080 0.99711585\n",
      "5090 0.99640226\n",
      "5100 0.99871904\n",
      "5110 0.9961929\n",
      "5120 0.9939325\n",
      "5130 0.9958418\n",
      "5140 0.99600554\n",
      "5150 0.9953579\n",
      "5160 0.9953758\n",
      "5170 0.9940069\n",
      "5180 0.9969924\n",
      "5190 0.99992263\n",
      "5200 0.99549586\n",
      "5210 0.99272037\n",
      "5220 0.9939054\n",
      "5230 0.9943028\n",
      "5240 0.99177575\n",
      "5250 0.9920938\n",
      "5260 0.9889905\n",
      "5270 0.9924775\n",
      "5280 0.990412\n",
      "5290 0.98905355\n",
      "5300 0.9886564\n",
      "5310 0.9931499\n",
      "5320 0.98635674\n",
      "5330 0.9807652\n",
      "5340 0.9796843\n",
      "5350 0.98767394\n",
      "5360 0.9858133\n",
      "5370 0.9730521\n",
      "5380 0.9797112\n",
      "5390 0.97404325\n",
      "5400 0.9683563\n",
      "5410 0.9609671\n",
      "5420 0.9546282\n",
      "5430 0.9645037\n",
      "5440 0.9582313\n",
      "5450 0.9438144\n",
      "5460 0.9348201\n",
      "5470 0.9324501\n",
      "5480 0.92506075\n",
      "5490 0.8925222\n",
      "5500 0.9091792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5510 0.8717929\n",
      "5520 0.8809567\n",
      "5530 0.86067826\n",
      "5540 0.8454764\n",
      "5550 0.79631287\n",
      "5560 0.80154556\n",
      "5570 0.81537473\n",
      "5580 0.80722255\n",
      "5590 0.7527994\n",
      "5600 0.77116257\n",
      "5610 0.7439624\n",
      "5620 0.78887963\n",
      "5630 0.75524384\n",
      "5640 0.7458747\n",
      "5650 0.74857676\n",
      "5660 0.720375\n",
      "5670 0.7247296\n",
      "5680 0.7409442\n",
      "5690 0.72958267\n",
      "5700 0.7505877\n",
      "5710 0.723272\n",
      "5720 0.6909953\n",
      "5730 0.70705795\n",
      "5740 0.712134\n",
      "5750 0.6491242\n",
      "5760 0.71918696\n",
      "5770 0.6584153\n",
      "5780 0.6844046\n",
      "5790 0.6233023\n",
      "5800 0.6403683\n",
      "5810 0.6317619\n",
      "5820 0.59011596\n",
      "5830 0.63512975\n",
      "5840 0.63101333\n",
      "5850 0.6128415\n",
      "5860 0.6224543\n",
      "5870 0.60973895\n",
      "5880 0.59630716\n",
      "5890 0.5388184\n",
      "5900 0.6227328\n",
      "5910 0.6085026\n",
      "5920 0.5371153\n",
      "5930 0.6521257\n",
      "5940 0.544988\n",
      "5950 0.5401281\n",
      "5960 0.58045447\n",
      "5970 0.6076745\n",
      "5980 0.5417662\n",
      "5990 0.54883504\n",
      "6000 0.60136104\n",
      "6010 0.53466535\n",
      "6020 0.53872156\n",
      "6030 0.5774787\n",
      "6040 0.5540005\n",
      "6050 0.56299174\n",
      "6060 0.5720539\n",
      "6070 0.60972035\n",
      "6080 0.60624593\n",
      "6090 0.5923791\n",
      "6100 0.59796286\n",
      "6110 0.53248954\n",
      "6120 0.59962523\n",
      "6130 0.5622322\n",
      "6140 0.5581155\n",
      "6150 0.54527384\n",
      "6160 0.5610752\n",
      "6170 0.5475939\n",
      "6180 0.5885416\n",
      "6190 0.5920162\n",
      "6200 0.59568095\n",
      "6210 0.5682518\n",
      "6220 0.5076421\n",
      "6230 0.5407991\n",
      "6240 0.63660437\n",
      "6250 0.48665464\n",
      "6260 0.54869217\n",
      "6270 0.6083493\n",
      "6280 0.6024264\n",
      "6290 0.5236825\n",
      "6300 0.6250394\n",
      "6310 0.593681\n",
      "6320 0.5634253\n",
      "6330 0.567812\n",
      "6340 0.5782341\n",
      "6350 0.6163046\n",
      "6360 0.5672461\n",
      "6370 0.56759346\n",
      "6380 0.5790609\n",
      "6390 0.55931765\n",
      "6400 0.5452673\n",
      "6410 0.5451378\n",
      "6420 0.5170727\n",
      "6430 0.6185095\n",
      "6440 0.5518024\n",
      "6450 0.54373544\n",
      "6460 0.5171627\n",
      "6470 0.51970273\n",
      "6480 0.5359297\n",
      "6490 0.5360074\n",
      "6500 0.6010019\n",
      "6510 0.57014054\n",
      "6520 0.54295003\n",
      "6530 0.5144132\n",
      "6540 0.5355497\n",
      "6550 0.60327846\n",
      "6560 0.5424337\n",
      "6570 0.54951334\n",
      "6580 0.51010543\n",
      "6590 0.54804033\n",
      "6600 0.50105464\n",
      "6610 0.57096076\n",
      "6620 0.5100829\n",
      "6630 0.52401817\n",
      "6640 0.52851236\n",
      "6650 0.57616556\n",
      "6660 0.5800779\n",
      "6670 0.49368086\n",
      "6680 0.5770965\n",
      "6690 0.585866\n",
      "6700 0.5596321\n",
      "6710 0.53866434\n",
      "6720 0.52610946\n",
      "6730 0.5752025\n",
      "6740 0.5264919\n",
      "6750 0.56369966\n",
      "6760 0.5191097\n",
      "6770 0.52680403\n",
      "6780 0.523046\n",
      "6790 0.5017552\n",
      "6800 0.6016058\n",
      "6810 0.52159655\n",
      "6820 0.47310102\n",
      "6830 0.5156678\n",
      "6840 0.59473556\n",
      "6850 0.51919734\n",
      "6860 0.50341284\n",
      "6870 0.5331394\n",
      "6880 0.56514055\n",
      "6890 0.53030455\n",
      "6900 0.5300906\n",
      "6910 0.5541248\n",
      "6920 0.496392\n",
      "6930 0.5383414\n",
      "6940 0.54273117\n",
      "6950 0.50307536\n",
      "6960 0.45582253\n",
      "6970 0.51645195\n",
      "6980 0.50583696\n",
      "6990 0.5096024\n",
      "7000 0.5098509\n",
      "7010 0.55271715\n",
      "7020 0.5350643\n",
      "7030 0.48927665\n",
      "7040 0.57606125\n",
      "7050 0.5408327\n",
      "7060 0.53938174\n",
      "7070 0.5003241\n",
      "7080 0.51059943\n",
      "7090 0.50964147\n",
      "7100 0.55661887\n",
      "7110 0.5004415\n",
      "7120 0.5767597\n",
      "7130 0.5390146\n",
      "7140 0.53179723\n",
      "7150 0.54249716\n",
      "7160 0.5730032\n",
      "7170 0.4808157\n",
      "7180 0.5537952\n",
      "7190 0.5570711\n",
      "7200 0.5373518\n",
      "7210 0.5659182\n",
      "7220 0.5679485\n",
      "7230 0.5361063\n",
      "7240 0.5371787\n",
      "7250 0.47195354\n",
      "7260 0.5214847\n",
      "7270 0.49620047\n",
      "7280 0.5286321\n",
      "7290 0.58817405\n",
      "7300 0.43693715\n",
      "7310 0.50467753\n",
      "7320 0.56598765\n",
      "7330 0.53849536\n",
      "7340 0.5602754\n",
      "7350 0.5303326\n",
      "7360 0.51762575\n",
      "7370 0.5551653\n",
      "7380 0.4941469\n",
      "7390 0.48029363\n",
      "7400 0.54065347\n",
      "7410 0.53257865\n",
      "7420 0.53658247\n",
      "7430 0.5802448\n",
      "7440 0.5542156\n",
      "7450 0.50760007\n",
      "7460 0.599362\n",
      "7470 0.49491632\n",
      "7480 0.60306364\n",
      "7490 0.5373457\n",
      "7500 0.5046094\n",
      "7510 0.5232203\n",
      "7520 0.4927545\n",
      "7530 0.6052195\n",
      "7540 0.52961946\n",
      "7550 0.52003443\n",
      "7560 0.5115705\n",
      "7570 0.5127699\n",
      "7580 0.5900277\n",
      "7590 0.5335537\n",
      "7600 0.53972524\n",
      "7610 0.5651189\n",
      "7620 0.5711072\n",
      "7630 0.5153741\n",
      "7640 0.55105066\n",
      "7650 0.52711576\n",
      "7660 0.5297558\n",
      "7670 0.50107706\n",
      "7680 0.5579579\n",
      "7690 0.589958\n",
      "7700 0.5002563\n",
      "7710 0.5891846\n",
      "7720 0.5476034\n",
      "7730 0.5788379\n",
      "7740 0.52899206\n",
      "7750 0.529239\n",
      "7760 0.50624555\n",
      "7770 0.53024375\n",
      "7780 0.51252794\n",
      "7790 0.5547858\n",
      "7800 0.5734485\n",
      "7810 0.49525332\n",
      "7820 0.51507175\n",
      "7830 0.5595654\n",
      "7840 0.59258366\n",
      "7850 0.5944935\n",
      "7860 0.5923771\n",
      "7870 0.5308493\n",
      "7880 0.56538224\n",
      "7890 0.5655136\n",
      "7900 0.55207914\n",
      "7910 0.55976015\n",
      "7920 0.52377564\n",
      "7930 0.46903655\n",
      "7940 0.5522441\n",
      "7950 0.4785438\n",
      "7960 0.5487248\n",
      "7970 0.49895453\n",
      "7980 0.52881485\n",
      "7990 0.44865897\n",
      "8000 0.48516983\n",
      "8010 0.5311814\n",
      "8020 0.50794643\n",
      "8030 0.57143193\n",
      "8040 0.5110414\n",
      "8050 0.5651544\n",
      "8060 0.4880894\n",
      "8070 0.47111636\n",
      "8080 0.48910844\n",
      "8090 0.5579907\n",
      "8100 0.52803695\n",
      "8110 0.5376108\n",
      "8120 0.5523167\n",
      "8130 0.5602973\n",
      "8140 0.5550939\n",
      "8150 0.57663256\n",
      "8160 0.52972\n",
      "8170 0.6018268\n",
      "8180 0.54312944\n",
      "8190 0.5647116\n",
      "8200 0.5239555\n",
      "8210 0.5280306\n",
      "8220 0.51597834\n",
      "8230 0.5481605\n",
      "8240 0.5483788\n",
      "8250 0.58317864\n",
      "8260 0.6254047\n",
      "8270 0.54978096\n",
      "8280 0.568545\n",
      "8290 0.55782026\n",
      "8300 0.57820123\n",
      "8310 0.54778594\n",
      "8320 0.5861601\n",
      "8330 0.5376311\n",
      "8340 0.5171631\n",
      "8350 0.52908015\n",
      "8360 0.55085576\n",
      "8370 0.5480873\n",
      "8380 0.54460686\n",
      "8390 0.55650234\n",
      "8400 0.5312055\n",
      "8410 0.58293974\n",
      "8420 0.55978584\n",
      "8430 0.48563755\n",
      "8440 0.59024656\n",
      "8450 0.5098854\n",
      "8460 0.4363801\n",
      "8470 0.5702163\n",
      "8480 0.5315642\n",
      "8490 0.5607037\n",
      "8500 0.50289994\n",
      "8510 0.4719388\n",
      "8520 0.49276993\n",
      "8530 0.48168424\n",
      "8540 0.6030657\n",
      "8550 0.5573909\n",
      "8560 0.61060643\n",
      "8570 0.549526\n",
      "8580 0.46292496\n",
      "8590 0.54727\n",
      "8600 0.525514\n",
      "8610 0.50964963\n",
      "8620 0.431071\n",
      "8630 0.48590696\n",
      "8640 0.5216667\n",
      "8650 0.5552629\n",
      "8660 0.5057769\n",
      "8670 0.5492131\n",
      "8680 0.48850036\n",
      "8690 0.54572684\n",
      "8700 0.68180853\n",
      "8710 0.5047414\n",
      "8720 0.5548492\n",
      "8730 0.5856729\n",
      "8740 0.5215733\n",
      "8750 0.45303333\n",
      "8760 0.55053025\n",
      "8770 0.55485684\n",
      "8780 0.5553118\n",
      "8790 0.47683406\n",
      "8800 0.4780741\n",
      "8810 0.5273141\n",
      "8820 0.4985853\n",
      "8830 0.59189105\n",
      "8840 0.45124656\n",
      "8850 0.48663387\n",
      "8860 0.5152137\n",
      "8870 0.5695838\n",
      "8880 0.5629999\n",
      "8890 0.5135898\n",
      "8900 0.49402505\n",
      "8910 0.53006387\n",
      "8920 0.5797443\n",
      "8930 0.4667534\n",
      "8940 0.45662144\n",
      "8950 0.47687206\n",
      "8960 0.50246084\n",
      "8970 0.529632\n",
      "8980 0.4841462\n",
      "8990 0.46841317\n",
      "9000 0.518632\n",
      "9010 0.51616144\n",
      "9020 0.511736\n",
      "9030 0.52368915\n",
      "9040 0.5153644\n",
      "9050 0.46938813\n",
      "9060 0.5004578\n",
      "9070 0.5683929\n",
      "9080 0.5335839\n",
      "9090 0.5149811\n",
      "9100 0.52163595\n",
      "9110 0.5108774\n",
      "9120 0.46311432\n",
      "9130 0.55710715\n",
      "9140 0.5246347\n",
      "9150 0.5119585\n",
      "9160 0.5901835\n",
      "9170 0.4063067\n",
      "9180 0.510797\n",
      "9190 0.55741066\n",
      "9200 0.47500542\n",
      "9210 0.49585193\n",
      "9220 0.52102625\n",
      "9230 0.51425457\n",
      "9240 0.5162251\n",
      "9250 0.5016642\n",
      "9260 0.55347943\n",
      "9270 0.54266095\n",
      "9280 0.5006687\n",
      "9290 0.504084\n",
      "9300 0.49281582\n",
      "9310 0.513232\n",
      "9320 0.5746089\n",
      "9330 0.50342315\n",
      "9340 0.53351164\n",
      "9350 0.5574581\n",
      "9360 0.60033333\n",
      "9370 0.59939736\n",
      "9380 0.57588565\n",
      "9390 0.5522865\n",
      "9400 0.5623133\n",
      "9410 0.5616015\n",
      "9420 0.49103856\n",
      "9430 0.5289025\n",
      "9440 0.5787298\n",
      "9450 0.55026114\n",
      "9460 0.5121807\n",
      "9470 0.5694773\n",
      "9480 0.51356435\n",
      "9490 0.5054844\n",
      "9500 0.57420164\n",
      "9510 0.56442744\n",
      "9520 0.49759856\n",
      "9530 0.5064522\n",
      "9540 0.5558042\n",
      "9550 0.56511337\n",
      "9560 0.5324555\n",
      "9570 0.5557169\n",
      "9580 0.5072213\n",
      "9590 0.55922174\n",
      "9600 0.5830295\n",
      "9610 0.5672958\n",
      "9620 0.4849543\n",
      "9630 0.5193484\n",
      "9640 0.4926158\n",
      "9650 0.4893589\n",
      "9660 0.5445007\n",
      "9670 0.4560262\n",
      "9680 0.5262048\n",
      "9690 0.48473158\n",
      "9700 0.5565734\n",
      "9710 0.56443536\n",
      "9720 0.4992195\n",
      "9730 0.506088\n",
      "9740 0.5274814\n",
      "9750 0.47170928\n",
      "9760 0.5316332\n",
      "9770 0.5304008\n",
      "9780 0.49137166\n",
      "9790 0.5038429\n",
      "9800 0.5279874\n",
      "9810 0.52921003\n",
      "9820 0.51771224\n",
      "9830 0.49613234\n",
      "9840 0.51886934\n",
      "9850 0.52967\n",
      "9860 0.49523953\n",
      "9870 0.49053633\n",
      "9880 0.55162287\n",
      "9890 0.52350897\n",
      "9900 0.5683275\n",
      "9910 0.5198083\n",
      "9920 0.5593615\n",
      "9930 0.5348484\n",
      "9940 0.5311615\n",
      "9950 0.60772395\n",
      "9960 0.5773488\n",
      "9970 0.50736743\n",
      "9980 0.5623587\n",
      "9990 0.49019665\n"
     ]
    }
   ],
   "source": [
    "# train network\n",
    "reset()\n",
    "inputs, outputs, weights, loss, train = build_network1()\n",
    "num_iter=10000\n",
    "batch_size=100\n",
    "# use a single fixed preamble\n",
    "preamble=gen_preamble()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(0,num_iter):\n",
    "    # generate data\n",
    "    channels = gen_channel(N=batch_size)\n",
    "    received = apply_channel(channels, preamble, snr=-1)\n",
    "    #channels = channels.reshape((batch_size,-1,1))\n",
    "    received = received.reshape((batch_size,-1,1))[:,:100,:]\n",
    "    \n",
    "    # train\n",
    "    sess.run(train, feed_dict={inputs[0]:preamble.reshape((1, 100, 1)),\n",
    "                     inputs[1]:received,\n",
    "                     inputs[2]:channels})\n",
    "    if i % 100 == 0:\n",
    "        l = sess.run(loss, feed_dict={inputs[0]:preamble.reshape((1, 100, 1)),\n",
    "                     inputs[1]:received,\n",
    "                     inputs[2]:channels})\n",
    "        print(i,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100) (30, 20) (30, 119)\n"
     ]
    }
   ],
   "source": [
    "preamble=gen_preamble()\n",
    "channels = gen_channel(N=30)\n",
    "received = apply_channel(channels, preamble, snr=-1)\n",
    "print(preamble.shape, channels.shape, received.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (1, 100, 1)\n",
      "1 (?, 100, 1)\n",
      "2 (?, 20)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print (i, inputs[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n"
     ]
    }
   ],
   "source": [
    "print (channels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(received.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
